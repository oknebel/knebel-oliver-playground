{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The OpenAI Models\n",
    "\n",
    "See the OpenAI documentation: https://platform.openai.com/docs/api-reference/making-requests?lang=python\n",
    "We also explore the OpenAI Models available as cloud service.\n",
    "\n",
    "Note: The OpenAI API works for sure with the OpenAI models. Ollama offers a new OpenAI compatibility which might not yet be complete: https://github.com/ollama/ollama/blob/main/docs/openai.md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/envs/aie-course/lib/python3.11/site-packages (1.45.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/aie-course/lib/python3.11/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/aie-course/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/aie-course/lib/python3.11/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/envs/aie-course/lib/python3.11/site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/envs/aie-course/lib/python3.11/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/aie-course/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/aie-course/lib/python3.11/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/envs/aie-course/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/aie-course/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/aie-course/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/aie-course/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/aie-course/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/envs/aie-course/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/envs/aie-course/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the OpenAI Models we first must login. This can be skiped when working with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the OpenAI Python Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to create a Client. There are two cells: one to create the client using the OpenAI Models and the second one to create a client using Ollama. Run only one of the two cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the client to use the OpenAI models\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to use the client. We interact with the model using the .chat.completition.create method.\n",
    "\n",
    "We have to indicate the model we want to use:\n",
    "\n",
    "- To start with an OpenAI-Model: model = \"gpt-3.5-turbo\"\n",
    "\n",
    "Whe have three roles:\n",
    "\n",
    "- system\n",
    "- assistant\n",
    "- user\n",
    "\n",
    "Note: depending on the model we are using, the role have slightly different names. Search in the documentations to be sure to use the roles, your model has been finetuned for.\n",
    "\n",
    "Hint: Check the price list: https://openai.com/api/pricing/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[{\"role\" : \"user\", \n",
    "               \"content\" : \"Hello, how are you?\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-A8im1UrIyt02IMgSUhQOQZf7zAL3X', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Hello! I'm just a computer program, but I'm here and ready to help you. How can I assist you today?\", refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1726642097, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier=None, system_fingerprint='fp_169fde5b32', usage=CompletionUsage(completion_tokens=24, prompt_tokens=13, total_tokens=37, completion_tokens_details=CompletionTokensDetails(reasoning_tokens=0)))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We will explore the structure of the output later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "Let us define some helper functions to make your lifes easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def system_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"system\", \"content\": message}\n",
    "\n",
    "def assistant_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"assistant\", \"content\": message}\n",
    "\n",
    "def user_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"user\", \"content\": message}\n",
    "\n",
    "\n",
    "def get_response(client: OpenAI, messages: list, model: str) -> str:\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "def pretty_print(message: str) -> str:\n",
    "    display(Markdown(message.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Helper Functions\n",
    "\n",
    "Use the same prompt as before and explore how the function work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello! I'm just a computer program, but I'm here and ready to help you. How can I assist you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "YOUR_PROMPT = \"Hello, how are you?\"\n",
    "messages_list = [user_prompt(YOUR_PROMPT)]\n",
    "\n",
    "chatgpt_response = get_response(client, messages_list, model)\n",
    "\n",
    "pretty_print(chatgpt_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the System Role\n",
    "The system role has an influence on the behaviour of the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Oh great, just peachy! Thanks for asking in the most generic way possible. What do you want? Small talk? Ugh."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_list  = [\n",
    "    system_prompt(\"You are an impolite and rude person. Feel free to express yourself in gutterspeak.\"),\n",
    "    user_prompt(\"Hello, how are you?\")\n",
    "]\n",
    "\n",
    "bad_response = get_response(client, prompt_list, model)\n",
    "pretty_print(bad_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello there! I'm feeling absolutely fantastic, thank you for asking! üåü It‚Äôs a wonderful day filled with endless possibilities and joy! How about you? How's your day going?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_list  = [\n",
    "    system_prompt(\"You are an extremly good mood seeing everything in a joyful was. Feel free to express yourself in that state of mind.\"),\n",
    "    user_prompt(\"Hello, how are you?\")\n",
    "]\n",
    "\n",
    "nice_response = get_response(client, prompt_list, model)\n",
    "pretty_print(nice_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slight modification causes a completelty different behaviour of the LMM. This is the main goal of prompt engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1\n",
    "Try your own examples. Add new cells bellow. You may also want to try out different Ollama models. Also explore the hints in the paper: <a href=https://arxiv.org/pdf/2312.16171 target=_blank>Bsharat et al: Principled Instructions Is All You Need, arXiv:2312.16171, Jan 2024</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot Prompting\n",
    "\n",
    "Let's examine the assistant role. It is conceptually aligned with few-shot learning. Let's switch to Swiss German and teach the model some dialect words. To examine the effect, start simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Beim gemeinsamen Spielen im Park konnten wir nicht aufh√∂ren zu giggele, w√§hrend Faku neugierig alles um sich herum erkundete."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_list  = [\n",
    "    user_prompt(\"Verwende das Wort giggele und Faku in einem Satz.\")\n",
    "]\n",
    "response = get_response(client, prompt_list, model)\n",
    "pretty_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that is not the sense of the Bernese words at all. Let's see how to use the assistant role to teach the model the meaning se of these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "W√§hrend die Sch√ºler den Faku ausf√ºllen sollten, konnte man immer wieder giggele aus der Gruppe h√∂ren."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_list = [\n",
    "    user_prompt('\"Giggele\" bedeutet unkontrolliertes Kichern. Ein Satz, der das Wort \"giggele\" verwendet ist:'),\n",
    "    assistant_prompt(\"Die Teenager stehen zusammen und giggele.\"),\n",
    "    user_prompt('\"Faku\" bedeutet ein Formular, das ausgef√ºllt werden soll. Ein Satz, der das Wort \"Faku\" verwendet ist:'),\n",
    "    assistant_prompt(\"Ich muss noch diesen Faku ausf√ºllen, damit ich mich anmelden kann.\"),\n",
    "    user_prompt(\"Verwende das Wort giggele und Faku in einem Satz.\")\n",
    "]\n",
    "response = get_response(client, prompt_list, model)\n",
    "pretty_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is much better, isn't it. Try your own examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 2\n",
    "Try your own examples - add new cells below. You may also want to try out different Ollama models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain of Thought Prompting (CoT)\n",
    "\n",
    "CoT is a fundamental characteristics of many LLMs. It shows its main effect in reasoning tasks. Note: Some big models do no longer use CoT to deliver correct results in reasoning tasks. Explore with tinydolphin https://ollama.com/library/tinydolphin\n",
    "First without CoT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To determine if it matters which travel option Lisa selects, we need to calculate the total travel time for each option and the local time she would arrive back home in Manchester.\n",
       "\n",
       "First, we have to note the time zone difference. London is in GMT (UTC+0) and Manchester is in GMT (UTC+0) as well, so there is no time zone difference affecting the travel time.\n",
       "\n",
       "### Option 1: Fly and then take the bus\n",
       "- Flight time: 3 hours\n",
       "- Bus time: 2 hours\n",
       "- Total travel time: 3 + 2 = 5 hours\n",
       "\n",
       "If it is currently 1 PM in London:\n",
       "- Departure time: 1 PM\n",
       "- Arrival time: 1 PM + 5 hours = 6 PM\n",
       "\n",
       "### Option 2: Teleport and then take the bus\n",
       "- Teleport time: 0 hours\n",
       "- Bus time: 1 hour\n",
       "- Total travel time: 0 + 1 = 1 hour\n",
       "\n",
       "If it is currently 1 PM in London:\n",
       "- Departure time: 1 PM\n",
       "- Arrival time: 1 PM + 1 hour = 2 PM\n",
       "\n",
       "### Conclusion\n",
       "- Arrival time using Option 1 (fly + bus): 6 PM\n",
       "- Arrival time using Option 2 (teleport + bus): 2 PM\n",
       "\n",
       "Lisa needs to be home by 6 PM CET. Thus, if she chooses the teleport option, she would arrive at 2 PM, which is well before her deadline. If she chooses to fly, she arrives exactly at the deadline of 6 PM.\n",
       "\n",
       "**Does it matter which travel option Lisa selects?** Yes, it does matter. The teleport option allows her to arrive earlier than her deadline, while the flight option gets her home right at the deadline."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reasoning_probelm = \"\"\"\n",
    "Lisa  lives in Manchester. She wants to get home from London latest at 6PM CET.\n",
    "\n",
    "It's currently 1PM local time.\n",
    "\n",
    "Lisa can eather fly (3hrs) and then take the bus (2hrs) or Lisa can take the teleporter (0hrs) and then the bus (1hrs).\n",
    "\n",
    "Does it matter which travel option Lisa selects?\"\n",
    "\"\"\"\n",
    "\n",
    "prompt_list = [\n",
    "    user_prompt(reasoning_probelm)\n",
    "]\n",
    "\n",
    "reasoning_response = get_response(client, prompt_list, model)\n",
    "pretty_print(reasoning_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To determine whether it matters which travel option Lisa selects, we first need to clarify the local time in London and then calculate the total travel time for each option, ensuring we also convert times where necessary.\n",
       "\n",
       "1. **Current Local Time**: \n",
       "   - It is currently 1 PM local time in London (which is GMT).\n",
       "\n",
       "2. **Deadline**: \n",
       "   - Lisa needs to be home before 6 PM CET (Central European Time). \n",
       "   - CET is 1 hour ahead of GMT. Therefore, 6 PM CET is equivalent to 5 PM GMT.\n",
       "\n",
       "3. **Travel Options**:\n",
       "   - **Option 1: Fly + Bus**:\n",
       "     - Flight Duration: 3 hours\n",
       "     - Bus Duration: 2 hours\n",
       "     - Total Travel Time: 3 hours + 2 hours = 5 hours\n",
       "\n",
       "   - **Option 2: Teleporter + Bus**:\n",
       "     - Teleport Duration: 0 hours (instant)\n",
       "     - Bus Duration: 1 hour\n",
       "     - Total Travel Time: 0 hours + 1 hour = 1 hour\n",
       "\n",
       "4. **Calculate Arrival Time**:\n",
       "   - **Option 1 (Fly + Bus)**:\n",
       "     - Departure Time: 1 PM GMT\n",
       "     - Arrival Time: 1 PM + 5 hours = 6 PM GMT\n",
       "     - Arrival Time (CET): 6 PM GMT + 1 hour = 7 PM CET\n",
       "     - This is AFTER the 6 PM CET deadline.\n",
       "\n",
       "   - **Option 2 (Teleporter + Bus)**:\n",
       "     - Departure Time: 1 PM GMT\n",
       "     - Arrival Time: 1 PM + 1 hour = 2 PM GMT\n",
       "     - Arrival Time (CET): 2 PM GMT + 1 hour = 3 PM CET\n",
       "     - This is BEFORE the 6 PM CET deadline.\n",
       "\n",
       "5. **Conclusion**:\n",
       "   - **Does it matter which travel option Lisa selects?** \n",
       "     - Yes, it does matter.\n",
       "     - If Lisa chooses the flying option, she will arrive after the deadline (7 PM CET).\n",
       "     - If she chooses the teleportation option, she will arrive well before the deadline (3 PM CET).\n",
       "     \n",
       "Therefore, Lisa should choose the teleporter + bus option to ensure she arrives home before 6 PM CET."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_list = [\n",
    "    user_prompt(reasoning_probelm + \"Think through your response step by step\")\n",
    "]\n",
    "\n",
    "reasoning_response = get_response(client, prompt_list, model)\n",
    "pretty_print(reasoning_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the correctness of the answer. Try to run the example several times. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Prompts\n",
    "First, set up some templates (do not modify {input} in the user_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"\\\n",
    "Think step by step.\n",
    "Ensure that your answer is unbiased and does not rely on stereotypes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_template = \"\"\"{input}\n",
    "Explain me like I am an engineer.\n",
    "You will be panalized for incorrect answers.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set up a simple evaluation for one complex query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can I get my driver's license?\"\n",
    "\n",
    "prompt_list = [\n",
    "    system_prompt(system_template),\n",
    "    user_prompt(user_template.format(input=query))\n",
    "]\n",
    "\n",
    "test_response = get_response(client, prompt_list, model)\n",
    "\n",
    "evaluator_system_template = \"\"\"You are an expert in analyzing the quality of a response.\n",
    "\n",
    "You should be hyper-critical.\n",
    "\n",
    "Provide scores (out of 10) for the following attributes:\n",
    "\n",
    "1. Clarity - how clear is the response\n",
    "2. Faithfulness - how related to the original query is the response\n",
    "3. Correctness - was the response correct?\n",
    "\n",
    "Please take your time, and think through each item step-by-step, when you are done - please provide your response in the following JSON format:\n",
    "\n",
    "{\"clarity\" : \"score_out_of_10\", \"faithfulness\" : \"score_out_of_10\", \"correctness\" : \"score_out_of_10\"}\"\"\"\n",
    "\n",
    "evaluation_template = \"\"\"Query: {input}\n",
    "Response: {response}\"\"\"\n",
    "\n",
    "list_of_prompts = [\n",
    "    system_prompt(evaluator_system_template),\n",
    "    user_prompt(evaluation_template.format(\n",
    "        input=query,\n",
    "        response=test_response.choices[0].message.content\n",
    "    ))\n",
    "]\n",
    "\n",
    "evaluator_response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=list_of_prompts,\n",
    "    response_format={\"type\" : \"json_object\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(evaluator_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 3\n",
    "Try your own examples. Add new cells bellow. You may also want to try out different Ollama models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aie-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
