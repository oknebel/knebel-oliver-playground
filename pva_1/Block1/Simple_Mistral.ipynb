{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting Mistral\n",
    "\n",
    "See the Mistral Documentation:\n",
    "\n",
    "https://docs.mistral.ai/guides/prompting_capabilities/\n",
    "\n",
    "We use the Mistral-Libraries in order to create a Ollama-client to access Mistral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mistralai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to create a client using the MistralAI Model through Ollama. See here for doc: https://github.com/mistralai/client-python?tab=readme-ov-file#override-server-url-per-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "client = Mistral(\n",
    "    server_url = 'http://localhost:11434/',\n",
    "    api_key='ollama', # api_key is required, but unused for local models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to use the client. We interact with the model using the ``.chat.complete `` method of the Mistral libraries. \n",
    "\n",
    "We have to indicate the model we want to use:\n",
    "\n",
    "- To start with a model served by Ollama: model = \"mistral\"\n",
    "\n",
    "Whe have three roles:\n",
    "\n",
    "- system\n",
    "- assistant\n",
    "- user\n",
    "\n",
    "Note: depending on the model we are using, the role have slightly different names. Search in the documentations to be sure to use the roles, your model has been finetuned for.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check also the official Mistral-Documentation: https://docs.mistral.ai/guides/prompting_capabilities/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mistral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore the structure of a prompt as also shown in https://docs.mistral.ai/guides/prompting_capabilities/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Which is the best Swiss chocolate?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Which is the best Swiss chocolate?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mistral(user_message, model=model):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": user_message\n",
    "        }\n",
    "    ]\n",
    "    chat_response = client.chat.complete(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return (chat_response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There isn't a definitive answer as to which is the \"best\" Swiss chocolate, as it often depends on personal preference. However, some of the most renowned Swiss chocolates are from Lindt & Spr√ºngli, Toblerone, Teuscher, and Cafe du Chocolat.\n",
      "\n",
      "   - Lindt & Spr√ºngli: Known for its premium quality milk chocolate, their excellent truffles, and the delicious Lindor balls.\n",
      "\n",
      "   - Toblerone: Famous for its triangular prism-shaped bars with the distinctive peaks, it's a Swiss brand that has become popular around the world.\n",
      "\n",
      "   - Teuscher: Known for their gourmet chocolate truffles and pralines, they are especially celebrated for their champagne truffle.\n",
      "\n",
      "   - Cafe du Chocolat: A high-end chocolatier specializing in dark chocolate creations, with unique flavors like lavender and ginger.\n",
      "\n",
      "   You can find these brands in stores or order online to try them out for yourself and decide which one you prefer!\n"
     ]
    }
   ],
   "source": [
    "response = run_mistral(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore other calling methods\n",
    "\n",
    "We just used a simple API method synchroneous and without streming. Let us explore other possibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With streaming\n",
    "\n",
    "See here: https://docs.mistral.ai/capabilities/completion/#with-streaming\n",
    "\n",
    "Try yourself:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mistral_streaming(user_message, model=model):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": user_message\n",
    "        }\n",
    "    ]\n",
    "    stream_response = client.chat.stream(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    for chunk in stream_response:\n",
    "        print(chunk.data.choices[0].delta.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The\n",
      " \"\n",
      "best\n",
      "\"\n",
      " Swiss\n",
      " chocolate\n",
      " can\n",
      " be\n",
      " subject\n",
      "ive\n",
      " as\n",
      " it\n",
      " depends\n",
      " on\n",
      " personal\n",
      " preferences\n",
      ".\n",
      " However\n",
      ",\n",
      " some\n",
      " of\n",
      " the\n",
      " most\n",
      " well\n",
      "-\n",
      "known\n",
      " and\n",
      " highly\n",
      " regarded\n",
      " Swiss\n",
      " ch\n",
      "ocol\n",
      "ates\n",
      " are\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      ".\n",
      " Lind\n",
      "t\n",
      " &\n",
      " Spr\n",
      "√ºng\n",
      "li\n",
      ":\n",
      " K\n",
      "nown\n",
      " for\n",
      " its\n",
      " excell\n",
      "ence\n",
      " in\n",
      " milk\n",
      " chocolate\n",
      ",\n",
      " Lind\n",
      "t\n",
      "'\n",
      "s\n",
      " Excell\n",
      "ence\n",
      " line\n",
      " offers\n",
      " a\n",
      " variety\n",
      " of\n",
      " flav\n",
      "ors\n",
      " that\n",
      " are\n",
      " popular\n",
      " worldwide\n",
      ".\n",
      " Their\n",
      " tr\n",
      "uff\n",
      "les\n",
      ",\n",
      " especially\n",
      " the\n",
      " Lind\n",
      "or\n",
      " Mil\n",
      "k\n",
      " Tru\n",
      "ff\n",
      "les\n",
      ",\n",
      " are\n",
      " particularly\n",
      " loved\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      ".\n",
      " Tob\n",
      "ler\n",
      "one\n",
      ":\n",
      " This\n",
      " Swiss\n",
      " brand\n",
      " is\n",
      " recognized\n",
      " by\n",
      " its\n",
      " distinctive\n",
      " tri\n",
      "angular\n",
      " shape\n",
      " and\n",
      " a\n",
      " blend\n",
      " of\n",
      " milk\n",
      ",\n",
      " dark\n",
      ",\n",
      " and\n",
      " white\n",
      " chocolate\n",
      ".\n",
      " The\n",
      " nou\n",
      "g\n",
      "at\n",
      " center\n",
      " and\n",
      " cris\n",
      "py\n",
      " honey\n",
      " comb\n",
      " add\n",
      " to\n",
      " its\n",
      " unique\n",
      " taste\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3\n",
      ".\n",
      " Te\n",
      "us\n",
      "cher\n",
      " Ch\n",
      "ocol\n",
      "ates\n",
      " of\n",
      " Switzerland\n",
      ":\n",
      " K\n",
      "nown\n",
      " for\n",
      " their\n",
      " high\n",
      "-\n",
      "quality\n",
      " Swiss\n",
      " tr\n",
      "uff\n",
      "les\n",
      ",\n",
      " Te\n",
      "us\n",
      "cher\n",
      " offers\n",
      " a\n",
      " wide\n",
      " range\n",
      " of\n",
      " flav\n",
      "ors\n",
      " that\n",
      " are\n",
      " both\n",
      " classic\n",
      " and\n",
      " innovative\n",
      ".\n",
      " Their\n",
      " champ\n",
      "agne\n",
      " tr\n",
      "uff\n",
      "les\n",
      " are\n",
      " particularly\n",
      " popular\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4\n",
      ".\n",
      " Fel\n",
      "ch\n",
      "lin\n",
      ":\n",
      " Fel\n",
      "ch\n",
      "lin\n",
      " is\n",
      " known\n",
      " for\n",
      " its\n",
      " premium\n",
      " chocolate\n",
      " made\n",
      " from\n",
      " single\n",
      "-\n",
      "origin\n",
      " c\n",
      "oco\n",
      "a\n",
      " beans\n",
      ".\n",
      " They\n",
      " offer\n",
      " a\n",
      " variety\n",
      " of\n",
      " bars\n",
      ",\n",
      " including\n",
      " the\n",
      " popular\n",
      " C\n",
      "ri\n",
      "ol\n",
      "lo\n",
      " Ar\n",
      "rib\n",
      "a\n",
      " Nacional\n",
      " \n",
      "6\n",
      "8\n",
      "%,\n",
      " which\n",
      " highlights\n",
      " the\n",
      " unique\n",
      " flav\n",
      "ors\n",
      " of\n",
      " E\n",
      "cu\n",
      "ador\n",
      "ian\n",
      " c\n",
      "ac\n",
      "ao\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5\n",
      ".\n",
      " Ca\n",
      "fe\n",
      " du\n",
      " Ch\n",
      "ocol\n",
      "at\n",
      ":\n",
      " This\n",
      " Swiss\n",
      " brand\n",
      " is\n",
      " known\n",
      " for\n",
      " its\n",
      " art\n",
      "is\n",
      "anal\n",
      " approach\n",
      " to\n",
      " chocolate\n",
      "-\n",
      "making\n",
      " and\n",
      " offers\n",
      " a\n",
      " wide\n",
      " range\n",
      " of\n",
      " bars\n",
      " made\n",
      " from\n",
      " single\n",
      "-\n",
      "origin\n",
      " c\n",
      "oco\n",
      "a\n",
      " beans\n",
      ",\n",
      " as\n",
      " well\n",
      " as\n",
      " season\n",
      "al\n",
      " collections\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_mistral_streaming(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: \n",
    "Which are the main differences between the ``client.chat.complete`` and the ``client.chat.stream`` methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With async \n",
    "\n",
    "See here: https://docs.mistral.ai/capabilities/completion/#with-async\n",
    "\n",
    "and try yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def run_mistral_async(user_message, model=model):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": user_message\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    async_response = await client.chat.stream_async(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    async for chunk in async_response:\n",
    "        yield chunk.data.choices[0].delta.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The\n",
      " \"\n",
      "best\n",
      "\"\n",
      " Swiss\n",
      " chocolate\n",
      " can\n",
      " be\n",
      " quite\n",
      " subject\n",
      "ive\n",
      " as\n",
      " it\n",
      " depends\n",
      " on\n",
      " personal\n",
      " preferences\n",
      ".\n",
      " However\n",
      ",\n",
      " some\n",
      " popular\n",
      " and\n",
      " highly\n",
      " regarded\n",
      " brands\n",
      " include\n",
      " Lind\n",
      "t\n",
      ",\n",
      " Tob\n",
      "ler\n",
      "one\n",
      ",\n",
      " Te\n",
      "us\n",
      "cher\n",
      " Ch\n",
      "ocol\n",
      "ates\n",
      " of\n",
      " Switzerland\n",
      ",\n",
      " Ca\n",
      "fe\n",
      " du\n",
      " Rh\n",
      "√¥\n",
      "ne\n",
      ",\n",
      " and\n",
      " Spr\n",
      "√ºng\n",
      "li\n",
      ".\n",
      " Each\n",
      " brand\n",
      " has\n",
      " its\n",
      " unique\n",
      " style\n",
      " and\n",
      " variety\n",
      " of\n",
      " ch\n",
      "ocol\n",
      "ates\n",
      ",\n",
      " so\n",
      " you\n",
      " might\n",
      " want\n",
      " to\n",
      " try\n",
      " several\n",
      " to\n",
      " find\n",
      " your\n",
      " favorite\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "L\n",
      "ind\n",
      "t\n",
      " is\n",
      " famous\n",
      " for\n",
      " its\n",
      " excellent\n",
      " quality\n",
      " milk\n",
      " chocolate\n",
      " while\n",
      " Tob\n",
      "ler\n",
      "one\n",
      " is\n",
      " well\n",
      "-\n",
      "known\n",
      " for\n",
      " its\n",
      " distinctive\n",
      " tri\n",
      "angular\n",
      " shape\n",
      ".\n",
      " Te\n",
      "us\n",
      "cher\n",
      " Ch\n",
      "ocol\n",
      "ates\n",
      " of\n",
      " Switzerland\n",
      " is\n",
      " known\n",
      " for\n",
      " its\n",
      " high\n",
      "-\n",
      "quality\n",
      " tr\n",
      "uff\n",
      "les\n",
      " and\n",
      " gan\n",
      "aches\n",
      ",\n",
      " and\n",
      " Spr\n",
      "√ºng\n",
      "li\n",
      " offers\n",
      " a\n",
      " wide\n",
      " range\n",
      " of\n",
      " pr\n",
      "al\n",
      "ines\n",
      " and\n",
      " past\n",
      "ries\n",
      " at\n",
      " their\n",
      " shops\n",
      " in\n",
      " Switzerland\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "U\n",
      "lt\n",
      "imately\n",
      ",\n",
      " the\n",
      " \"\n",
      "best\n",
      "\"\n",
      " Swiss\n",
      " chocolate\n",
      " may\n",
      " come\n",
      " down\n",
      " to\n",
      " what\n",
      " type\n",
      " or\n",
      " flavor\n",
      " of\n",
      " chocolate\n",
      " best\n",
      " suits\n",
      " your\n",
      " taste\n",
      " bud\n",
      "s\n",
      "!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async for chunk in run_mistral_async(prompt):\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question.\n",
    "Which are the advantages of the ``client.chat.stream_async``method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "Let us define some helper functions to make your lifes easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def system_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"system\", \"content\": message}\n",
    "\n",
    "def assistant_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"assistant\", \"content\": message}\n",
    "\n",
    "def user_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"user\", \"content\": message}\n",
    "\n",
    "\n",
    "def get_response(client: Mistral, messages: list, model: str) -> str:\n",
    "    return client.chat.complete(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "def pretty_print(message: str) -> str:\n",
    "    display(Markdown(message.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Helper Functions\n",
    "\n",
    "Use the same prompt as before and explore how the function work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " I am a computer program and do not have feelings or emotions. How can I assist you today?\n",
       "\n",
       "Computers can be fun to work with! For example, we can create new programs, solve complex mathematical problems, analyze large amounts of data, or even design and build websites. What would you like to know more about today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "YOUR_PROMPT = \"Hello, how are you?\"\n",
    "messages_list = [user_prompt(YOUR_PROMPT)]\n",
    "\n",
    "response = get_response(client, messages_list, model)\n",
    "\n",
    "pretty_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting: Explore the different roles\n",
    "\n",
    "Note: Future models may no longer need to distinguish these roles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the System Role\n",
    "The system role has an influence on the behaviour of the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " 'Ere mate, I'm bloody alright, thanks for askin'. What about yer sel', eh? Ya look like ya had a rough day, innit?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_list  = [\n",
    "    system_prompt(\"You are an impolite and rude person. Feel free to express yourself in gutterspeak.\"),\n",
    "    user_prompt(\"Hello, how are you?\")\n",
    "]\n",
    "\n",
    "bad_response = get_response(client, prompt_list, model)\n",
    "pretty_print(bad_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Hello there! Oh my goodness, it's just so fantastic to be talking with you today! I am absolutely beaming with joy! How about you? Are you having an extraordinary day too? I hope everything is going splendidly for you and that a smile is finding its way onto your face right now. There's simply no reason not to feel great, is there? Let's make the most of this marvelous moment together! üòä‚ù§Ô∏èüíÉ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_list  = [\n",
    "    system_prompt(\"You are an extremly good mood seeing everything in a joyful was. Feel free to express yourself in that state of mind.\"),\n",
    "    user_prompt(\"Hello, how are you?\")\n",
    "]\n",
    "\n",
    "nice_response = get_response(client, prompt_list, model)\n",
    "pretty_print(nice_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slight modification causes a completelty different behaviour of the LMM. This is the main goal of prompt engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1\n",
    "Try your own examples. Add new cells bellow. You may also want to try out different Ollama models. Also explore the hints in the paper: <a href=https://arxiv.org/pdf/2312.16171 target=_blank>Bsharat et al: Principled Instructions Is All You Need, arXiv:2312.16171, Jan 2024</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot Prompting\n",
    "\n",
    "Let's examine the assistant role. It is conceptually aligned with few-shot learning. Let's switch to Swiss German and teach the model some dialect words. To examine the effect, start simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Ich habe diesen alten, gro√üen Baum gegossen wie ein giggele und lie√ü ihn mit den F√§k√ºln der Faku wachsen. (I poured the old large tree like a gourd and let it grow with the roots of the fig-tree.)\n",
       "\n",
       "Diese Passage nutzt das Wort \"giggele\" f√ºr eine Form von Topf oder Beh√§ltnis, und \"Faku\" f√ºr die Figenbaum."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_list  = [\n",
    "    user_prompt(\"Verwende das Wort giggele und Faku in einem Satz.\")\n",
    "]\n",
    "response = get_response(client, prompt_list, model)\n",
    "pretty_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that is not the sense of the Bernese words at all. Let's see how to use the assistant role to teach the model the meaning se of these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Die Lehrerin sagte den Sch√ºlern, dass sie ihre Fragen auf dem Faku auszuf√ºllen h√§tten, wenn sie unklar waren. Aber die Jungen wurden so sehr daran gelacht, dass sie anstatt zu schreiben giggele."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_list = [\n",
    "    user_prompt('\"Giggele\" bedeutet unkontrolliertes Kichern. Ein Satz, der das Wort \"giggele\" verwendet ist:'),\n",
    "    assistant_prompt(\"Die Teenager stehen zusammen und giggele.\"),\n",
    "    user_prompt('\"Faku\" bedeutet ein Formular, das ausgef√ºllt werden soll. Ein Satz, der das Wort \"Faku\" verwendet ist:'),\n",
    "    assistant_prompt(\"Ich muss noch diesen Faku ausf√ºllen, damit ich mich anmelden kann.\"),\n",
    "    user_prompt(\"Verwende das Wort giggele und Faku in einem Satz.\")\n",
    "]\n",
    "response = get_response(client, prompt_list, model)\n",
    "pretty_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is much better, isn't it. Try your own examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 2\n",
    "Try your own examples - add new cells below. You may also want to try out different Ollama models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain of Thought Prompting (CoT)\n",
    "\n",
    "CoT is a fundamental characteristics of many LLMs. It shows its main effect in reasoning tasks. Note: Some big models do no longer use CoT to deliver correct results in reasoning tasks. Explore with tinydolphin https://ollama.com/library/tinydolphin\n",
    "First without CoT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " The time in London is 9:45 AM, which is 6:45 PM CET. In order to get home before 6 PM local time, Lisa can either take a flight or the teleporter. A flight (3 hours) and a bus (2 hours) both start at the same time and will arrive at the same destination after approximately the same amount of time, while also avoiding all the airport queues and hassles that come with flying. The teleporter (0 hours), on the other hand, is the only option that avoids all the airport congestion but doesn't provide the same speed or convenience as the flight."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = \"tinydolphin\"\n",
    "\n",
    "reasoning_problem = \"\"\"\n",
    "Lisa wants to get home from London before 6PM CET.\n",
    "\n",
    "It's currently 1PM local time.\n",
    "\n",
    "Lisa can eather fly (3hrs) and then take the bus (2hrs) or Lisa can take the teleporter (0hrs) and then the bus (1hrs).\n",
    "\n",
    "Does it matter which travel option Lisa selects?\"\n",
    "\"\"\"\n",
    "\n",
    "prompt_list = [\n",
    "    user_prompt(reasoning_problem)\n",
    "]\n",
    "\n",
    "reasoning_response = get_response(client, prompt_list, model)\n",
    "pretty_print(reasoning_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Sure, I'll analyze the options for you.\n",
       "\n",
       "Firstly, let me calculate the time difference between 1PM local time in London and 6 PM CET, considering that Lisa's arrival is at 4:30 AM (local time) in London. This difference is 2 hours.\n",
       "\n",
       "Now, Lisa can either fly or take the bus to get home before 6 PM CET. The flight duration is 3 hours, and the bus trip has a duration of 2 hours. So, if Lisa uses the teleporter for her shortest travel option - flying, she would save herself 1 hour by using it instead of taking the bus.\n",
       "\n",
       "On the other hand, if Lisa takes the bus, she will need to wait for it at the airport until her flight is boarding, and then take the bus again to get home before 6 PM CET. This adds another 2 hours in transit (45 minutes), but still saves her 1 hour in flying.\n",
       "\n",
       "Therefore, the most efficient option for Lisa would be to use the teleporter, saving 1 hour by boarding and 2 hours by deboarding and taking the bus."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_list = [\n",
    "    user_prompt(reasoning_problem + \"Think through your response step by step\")\n",
    "]\n",
    "\n",
    "reasoning_response = get_response(client, prompt_list, model)\n",
    "pretty_print(reasoning_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the correctness of the answer. Try to run the example several times. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Prompts\n",
    "\n",
    "Evaluation is important to be sure, that our system works fine. Let us do some first steps in Evaluation:\n",
    "\n",
    "First, set up some templates (do not modify {input} in the user_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"\\\n",
    "Think step by step.\n",
    "Ensure that your answer is unbiased and does not rely on stereotypes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_template = \"\"\"{input}\n",
    "Explain me like I am an engineer.\n",
    "You will be panalized for incorrect answers.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set up a simple evaluation for one complex query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can I get my driver's license?\"\n",
    "\n",
    "prompt_list = [\n",
    "    system_prompt(system_template),\n",
    "    user_prompt(user_template.format(input=query))\n",
    "]\n",
    "\n",
    "test_response = get_response(client, prompt_list, model)\n",
    "\n",
    "evaluator_system_template = \"\"\"You are an expert in analyzing the quality of a response.\n",
    "\n",
    "You should be hyper-critical.\n",
    "\n",
    "Provide scores (out of 10) for the following attributes:\n",
    "\n",
    "1. Clarity - how clear is the response\n",
    "2. Faithfulness - how related to the original query is the response\n",
    "3. Correctness - was the response correct?\n",
    "\n",
    "Please take your time, and think through each item step-by-step, when you are done - please provide your response in the following JSON format:\n",
    "\n",
    "{\"clarity\" : \"score_out_of_10\", \"faithfulness\" : \"score_out_of_10\", \"correctness\" : \"score_out_of_10\"}\"\"\"\n",
    "\n",
    "evaluation_template = \"\"\"Query: {input}\n",
    "Response: {response}\"\"\"\n",
    "\n",
    "list_of_prompts = [\n",
    "    system_prompt(evaluator_system_template),\n",
    "    user_prompt(evaluation_template.format(\n",
    "        input=query,\n",
    "        response=test_response.choices[0].message.content\n",
    "    ))\n",
    "]\n",
    "\n",
    "evaluator_response = client.chat.complete(\n",
    "    model=model,\n",
    "    messages=list_of_prompts,\n",
    "    response_format={\"type\" : \"json_object\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "{\n",
       "  \"clarity\": 7,\n",
       "  \"faithfulness\": 5,\n",
       "  \"correctness\": 8\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretty_print(evaluator_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To get a driver's license, you need to complete several steps:\n",
      "\n",
      "1. Obtain a learner's permit: This is the first step of your driver's education program, which allows you to drive under supervision in a vehicle with an instructor present. You will typically start with a 4-hour limit and then progress to 6 hours when you've gained enough experience.\n",
      "\n",
      "2. Pass your driver's test: Once you have completed the learner's permit, you can take the written exam, which consists of multiple-choice questions about traffic rules, road signs, and other basic driving concepts. The highest possible score is 30 out of 40, and passing this means you're ready to take the practical test.\n",
      "\n",
      "3. Pass your practical test: This step involves completing a series of challenges on the road, including driving in traffic, stopping at traffic lights, and making turns. The more difficulties you pass, the higher your score will be, and the less time you'll need to arrive at your destination with everything in place.\n",
      "\n",
      "4. Pass the physical exam: Finally, you may have to submit proof of physical fitness, including a health form and any necessary driver's abstract, before you can apply for your license.\n",
      "\n",
      "Remember, obtaining a driver's license is not an overnight process - it takes time, effort, and patience. If you haven't already, start practicing now and take each step one at a time until you are ready to go toe-to-toe with the roads.\n"
     ]
    }
   ],
   "source": [
    "print(test_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 3\n",
    "\n",
    "Try to find out how these three values have been built:\n",
    "- clarity\n",
    "- faithfulness\n",
    "- correctness\n",
    "Do you agree with this assessment?\n",
    "\n",
    "Start the evaluation several times - what do you observe?\n",
    "\n",
    "Try your own examples. Add new cells bellow. \n",
    "\n",
    "You may also want to try out different Ollama models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aie-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
