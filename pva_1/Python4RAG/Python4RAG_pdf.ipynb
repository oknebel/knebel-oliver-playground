{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG ror PDF written purely in Python\n",
    "\n",
    "First install a library to read pdf-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the vector store\n",
    "\n",
    "First we build the vector store based on our main document. Here we downloaded some paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_document_path = \"./data/2205.13147v4.pdf\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py4ragTools.database import Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructor here\n"
     ]
    }
   ],
   "source": [
    "# embedding_model = 'all-minilm'  # 384\n",
    "embedding_model='nomic-embed-text'  # 768\n",
    "db = Database(embedding_model=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from py4ragTools.text_tools import TextHelper, CharacterTextSplitter\n",
    "th = TextHelper()\n",
    "\n",
    "# load the document\n",
    "doc = th.load(the_document_path)\n",
    "\n",
    "# split the text into chunks\n",
    "splitter = CharacterTextSplitter(chunk_size=800,chunk_overlap=200)\n",
    "chunks = splitter.split(doc)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212 entries in db\n"
     ]
    }
   ],
   "source": [
    "# now we build the 'vector store' converting each chunk into a vector\n",
    "count = 0\n",
    "for chunk in chunks:\n",
    "    # Bei Texten der Länge 0 bleibt ollama stehen\n",
    "    # Sehr kurze Texte sind unbrauchbar für's anschliessende Generieren\n",
    "    if len(chunk) > 10:\n",
    "        # print(count)\n",
    "        # print(entry)\n",
    "        db.add(chunk)\n",
    "        count += 1\n",
    "print(f'{count} entries in db')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "We have built our vector store let us retrieve a context for a user-query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a query regarding today's topics on the newspaper's front page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  observ: sentance 'it is often the case...' available in context, but not in response\n",
    "USER_PROMPT_0 = \"\"\"\n",
    "Are there computational and statistical constraints?\n",
    "\"\"\"\n",
    "\n",
    "#  observ: context has complete sub points response only returns first\n",
    "USER_PROMPT_1 = \"\"\" \\\n",
    "What does the Checklist say, if you are using existing assets?\n",
    "\"\"\"\n",
    "\n",
    "# audience prompt pattern\n",
    "#  observ: response sounds well, though hard to say if it fully makes sense\n",
    "USER_PROMPT_2 = \"\"\" \\\n",
    "What is Matryoshka Representation Learning? Explain to me like I’m 11 years old.\n",
    "\"\"\"\n",
    "\n",
    "# persona  pattern\n",
    "#  observ: response sounds well, though hard to say if it fully makes sense\n",
    "USER_PROMPT_3 = \"\"\" \\\n",
    "What is Matryoshka Representation Learning? Act as a oriental story teller.\n",
    "\"\"\"\n",
    "\n",
    "# response template pattern\n",
    "#  observ: works well\n",
    "USER_PROMPT_4 = \"\"\" \\\n",
    "What is Matryoshka Representation Learning?\n",
    "Format your answer as follows:\n",
    "#Question: [The question being asked by User]\n",
    "## Summary \n",
    "[a one paragraph summary of the answer]\n",
    "## Details \n",
    "[a detailed answer]\n",
    "\"\"\"\n",
    "\n",
    "# question refinement pattern\n",
    "# observ: this, does not semm make much sense in this szenarion because the llm needs to know the whole document to suggest more precise questions\n",
    "USER_PROMPT_5 = \"\"\" \\\n",
    "Is Matryoshka Representation usefull at all?\n",
    "Suggest a better version of the question and ask me if I would like to use it instead\n",
    "\"\"\"\n",
    "\n",
    "user_query = USER_PROMPT_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = db.query_database(user_query, 0.5 )\n",
    "len(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7258670687494336,\n",
       "  'oshka Representation Learning (MRL) to induce flexibility in the learned\\nrepresentation. MRL learns representations of varying capacities within the same high-dimensional\\nvector through explicit optimization of O(log(d)) lower-dimensional vectors in a nested fashion,\\nhence the name Matryoshka. MRL can be adapted to any existing representation pipeline and\\nis easily extended to many standard tasks in computer vision and natural language processing.\\nFigure 1 illustrates the core idea of Matryoshka Representation Learning (MRL) and the adaptive\\ndeployment settings of the learned Matryoshka Representations.\\nAdaptive Retrieval\\nShortlisting\\nRe-ranking\\nAdaptive Classification\\nTraining\\nInference\\n<latexit sha1_base64=\"eh9hk+peBkdsPY6v+r4rONmxYLY=\">A\\nB7nicbVBNSwMxEJ2tX7V+VT16CRbBU9kVoR6LXjxWsB/QLiWb'),\n",
       " (0.7117174144365266,\n",
       "  ', and signif-\\nicantly for lower ones. This demonstrates that training to learn Matryoshka Representations\\nis feasible and extendable even for extremely large scale datasets.\\nWe also demonstrate that\\nMatryoshka Representations are learned at interpolated dimensions for both ALIGN and JFT-\\nViT, as shown in Table 5, despite not being trained explicitly at these dimensions. Lastly, Table 6\\nshows that MRL training leads to a increase in the cosine similarity span between positive and\\nrandom image-text pairs.\\nWe also evaluated the capability of Matryoshka Representations to extend to other natural language\\nprocessing via masked language modeling (MLM) with BERT [19], whose results are tabulated\\nin Table 7. Without any hyper-parameter tuning, we observed Matryoshka Representations to be\\nwithin 0.'),\n",
       " (0.7112284653105804,\n",
       "  'ib+53USE1x4KZdxYpiks0NBIrCJcFYB7nPFqBFjSwhV3GbFdEgUocYWVbQluPNfXiTN06rVN3bs3LtMq+jAIdwBVw4RxqcA1aAFB\\nc/wCm/oEb2gd/QxG1C+c4B/AH6/AGZEJHn</latexit>\\nFigure\\n1:\\nMatryoshka Representation Learning\\nis\\nadaptable to any representation learning setup and begets\\na Matryoshka Representation z\\nby optimizing the orig-\\ninal\\nloss\\nL(.)\\nat\\nO(log(d))\\nchosen\\nrepresentation\\nsizes.\\nMatryoshka Representation can be utilized effectively for adap-\\ntive deployment across environments and downstream tasks.\\nThe first m-dimensions, m ∈[d], of\\nthe Matryoshka Representation is\\nan information-rich low-dimensional\\nvector, at no additional training cost,\\nthat is as accurate as an indepen-\\ndently trained m-dimensional repre-\\nsentation.\\nThe information within\\nthe Matryoshka Representation in-\\ncreases with the dimensionality '),\n",
       " (0.6761588430445213,\n",
       "  'erse set of ap-\\nplications along with an extensive evaluation of the learned multifidelity representations. Further,\\nwe showcase the downstream applications of the learned Matryoshka Representations for flexible\\nlarge-scale deployment through (a) Adaptive Classification (AC) and (b) Adaptive Retrieval (AR).\\n4.1\\nRepresentation Learning\\nWe adapt Matryoshka Representation Learning (MRL) to various representation learning setups\\n(a) Supervised learning for vision: ResNet50 [29] on ImageNet-1K [76] and ViT-B/16 [22] on\\nJFT-300M [85], (b) Contrastive learning for vision + language: ALIGN model with ViT-B/16 vision\\nencoder and BERT language encoder on ALIGN data [46] and (c) Masked language modelling:\\nBERT [19] on English Wikipedia and BooksCorpus [102]. Please refer to Appendices B and C for\\ndet'),\n",
       " (0.663921575366701,\n",
       "  ' weight-tying across all the linear classifiers, i.e., by defining W(m) = W1:m\\nfor a set of common weights W ∈RL×d. This would reduce the memory cost due to the linear\\nclassifiers by almost half, which would be crucial in cases of extremely large output spaces [89, 99].\\nThis variant is called Efficient Matryoshka Representation Learning (MRL–E). Refer to Alg 1\\nand Alg 2 in Appendix A for the building blocks of Matryoshka Representation Learning (MRL).\\nAdaptation to Learning Frameworks.\\nMRL can be adapted seamlessly to most representation\\nlearning frameworks at web-scale with minimal modifications (Section 4.1). For example, MRL’s\\nadaptation to masked language modelling reduces to MRL–E due to the weight-tying between the\\ninput embedding matrix and the linear classifier. For contrastive lea'),\n",
       " (0.6632648038140058,\n",
       "  'ons (Section 4.1). For example, MRL’s\\nadaptation to masked language modelling reduces to MRL–E due to the weight-tying between the\\ninput embedding matrix and the linear classifier. For contrastive learning, both in context of vision &\\nvision + language, MRL is applied to both the embeddings that are being contrasted with each other.\\nThe presence of normalization on the representation needs to be handled independently for each of\\nthe nesting dimension for best results (see Appendix C for more details).\\n4\\nApplications\\nIn this section, we discuss Matryoshka Representation Learning (MRL) for a diverse set of ap-\\nplications along with an extensive evaluation of the learned multifidelity representations. Further,\\nwe showcase the downstream applications of the learned Matryoshka Representations f'),\n",
       " (0.6623609437997384,\n",
       "  'Matryoshka Representation Learning\\nAditya Kusupati∗†⋄, Gantavya Bhatt∗†, Aniket Rege∗†,\\nMatthew Wallingford†, Aditya Sinha⋄, Vivek Ramanujan†, William Howard-Snyder†,\\nKaifeng Chen⋄, Sham Kakade‡, Prateek Jain⋄and Ali Farhadi†\\n†University of Washington, ⋄Google Research, ‡Harvard University\\n{kusupati,ali}@cs.washington.edu, prajain@google.com\\nAbstract\\nLearned representations are a central component in modern ML systems, serv-\\ning a multitude of downstream tasks. When training such representations, it\\nis often the case that computational and statistical constraints for each down-\\nstream task are unknown. In this context, rigid fixed-capacity representations\\ncan be either over or under-accommodating to the task at hand. This leads us\\nto ask: can we design a flexible representation that can ad'),\n",
       " (0.653138277656782,\n",
       "  'guage applications are built [40] on large language models [8] that are pretrained [68, 75]\\nin a un/self-supervised fashion with masked language modelling [19] or autoregressive training [70].\\nMatryoshka Representation Learning (MRL) is complementary to all these setups and can be\\nadapted with minimal overhead (Section 3). MRL equips representations with multifidelity at no\\nadditional cost which enables adaptive deployment based on the data and task (Section 4).\\nEfficient Classification and Retrieval.\\nEfficiency in classification and retrieval during inference\\ncan be studied with respect to the high yet constant deep featurization costs or the search cost which\\nscales with the size of the label space and data. Efficient neural networks address the first issue\\nthrough a variety of algorithm'),\n",
       " (0.6519220027657986,\n",
       "  '36th Conference on Neural Information Processing Systems (NeurIPS 2022).\\narXiv:2205.13147v4  [cs.LG]  8 Feb 2024\\ndue to training/maintenance overhead, numerous expensive forward passes through all of the data,\\nstorage and memory cost for multiple copies of encoded data, expensive on-the-fly feature selection\\nor a significant drop in accuracy. By encoding coarse-to-fine-grained representations, which are as\\naccurate as the independently trained counterparts, we learn with minimal overhead a representation\\nthat can be deployed adaptively at no additional cost during inference.\\nWe introduce\\nMatryoshka Representation Learning (MRL) to induce flexibility in the learned\\nrepresentation. MRL learns representations of varying capacities within the same high-dimensional\\nvector through explicit optim'),\n",
       " (0.6489538759183751,\n",
       "  'P7tBRJKQCk04VqDnFi7KZaEU7HpW6iaIzJEPdpx1CBQ6rcdJ/DPeNEsBeJM0TGk7UnxspDpUahb6ZzNKqWS8T/M6ie6duSkTcaKpINDvYRDHcGsDBgwSYnmI0MwkcxkhWSAJSbaVFYyJaDZL/8lzaMqcqro5rhcu\\n8jrKIJdsAcqAIFTUANXoA4agIAH8ARewKv1aD1b9b7dLRg5Tvb4Besj2/eCZSw</latexit>\\n<latexit sha1_base64=\"OPHM4ACsGr0VI7qMpDgoN+t2ICI=\">AB9XicbVDLSgMx\\nFL3xWeur6tJNsAh1U2ZE0GXRjQsXFewD2rFk0kwbmskMSUapQ/DjQtF3Pov7vwbM+0stPVA4HDOvdyT48eCa+M432hpeWV1b2wUdzc2t7ZLe3tN3WUKMoaNBKRavtEM8Elaxh\\nuBGvHipHQF6zlj64yv/XAlOaRvDPjmHkhGUgecEqMle67ITHDQJERvqk8nfRKZafqTIEXiZuTMuSo90pf3X5Ek5BJQwXRuM6sfFSogyngk2K3USzmNARGbCOpZKETHvpNPUEH1\\nulj4NI2ScNnq/N1ISaj0OfTuZpdTzXib+53USE1x4KZdxYpiks0NBIrCJcFYB7nPFqBFjSwhV3GbFdEgUocYWVbQluPNfXiTN06rVN3bs3LtMq+jAIdwBVw4RxqcA1aAFB\\nc/wCm/oEb2gd/QxG1C+c4B/AH6/AGZEJHn</latexit>\\nFigure\\n1:\\nMatryoshka Representation Learning\\nis\\nadapta'),\n",
       " (0.6466075279377438,\n",
       "  'scuss the design choices in Section 4 for each of the representation learning settings.\\nFor the ease of exposition, we present the formulation for fully supervised representation learning\\nvia multi-class classification. Matryoshka Representation Learning modifies the typical setting\\nto become a multi-scale representation learning problem on the same task. For example, we train\\nResNet50 [29] on ImageNet-1K [76] which embeds a 224 × 224 pixel image into a d = 2048\\nrepresentation vector and then passed through a linear classifier to make a prediction, ˆy among the\\nL = 1000 labels. For MRL, we choose M = {8, 16, . . . , 1024, 2048} as the nesting dimensions.\\nSuppose we are given a labelled dataset D = {(x1, y1), . . . , (xN, yN)} where xi ∈X is an input\\npoint and yi ∈[L] is the label of xi for'),\n",
       " (0.6380859825287875,\n",
       "  ' unknown. In this context, rigid fixed-capacity representations\\ncan be either over or under-accommodating to the task at hand. This leads us\\nto ask: can we design a flexible representation that can adapt to multiple down-\\nstream tasks with varying computational resources? Our main contribution is\\nMatryoshka Representation Learning (MRL) which encodes information at\\ndifferent granularities and allows a single embedding to adapt to the computational\\nconstraints of downstream tasks. MRL minimally modifies existing representation\\nlearning pipelines and imposes no additional cost during inference and deployment.\\nMRL learns coarse-to-fine representations that are at least as accurate and rich as\\nindependently trained low-dimensional representations. The flexibility within the\\nlearned Matryoshka '),\n",
       " (0.6372650033849211,\n",
       "  'leviates this extra compute with a\\nminimal drop in accuracy.\\nD.2\\nJFT, ALIGN and BERT\\nWe examine the k-NN classification accuracy of learned Matryoshka Representations via\\nALIGN–MRL and JFT-ViT–MRL in Table 4.\\nFor ALIGN [46], we observed that learning\\nMatryoshka Representations via ALIGN–MRL improved classification accuracy at nearly all\\ndimensions when compared to ALIGN. We observed a similar trend when training ViT-B/16 [22]\\nfor JFT-300M [85] classification, where learning Matryoshka Representations via MRL and\\nMRL–E on top of JFT-ViT improved classification accuracy for nearly all dimensions, and signif-\\nicantly for lower ones. This demonstrates that training to learn Matryoshka Representations\\nis feasible and extendable even for extremely large scale datasets.\\nWe also demonstrate that\\nM'),\n",
       " (0.6343959313043339,\n",
       "  'f general purpose representations for computer vision [4, 98]. These representations\\nare typically learned through supervised and un/self-supervised learning paradigms. Supervised\\npretraining [29, 51, 82] casts representation learning as a multi-class/label classification problem,\\nwhile un/self-supervised learning learns representation via proxy tasks like instance classification [97]\\nand reconstruction [31, 63]. Recent advances [12, 30] in contrastive learning [27] enabled learning\\nfrom web-scale data [21] that powers large-capacity cross-modal models [18, 46, 71, 101]. Similarly,\\nnatural language applications are built [40] on large language models [8] that are pretrained [68, 75]\\nin a un/self-supervised fashion with masked language modelling [19] or autoregressive training [70].\\nMatryos'),\n",
       " (0.6298945437873916,\n",
       "  'no additional training cost,\\nthat is as accurate as an indepen-\\ndently trained m-dimensional repre-\\nsentation.\\nThe information within\\nthe Matryoshka Representation in-\\ncreases with the dimensionality creat-\\ning a coarse-to-fine grained represen-\\ntation, all without significant training\\nor additional deployment overhead.\\nMRL equips the representation vector\\nwith the desired flexibility and multi-\\nfidelity that can ensure a near-optimal\\naccuracy-vs-compute trade-off. With\\nthese advantages, MRL enables adap-\\ntive deployment based on accuracy\\nand compute constraints.\\nThe Matryoshka Representations improve efficiency for large-scale classification and retrieval\\nwithout any significant loss of accuracy. While there are potentially several applications of coarse-to-\\nfine Matryoshka Representation'),\n",
       " (0.62895464280292,\n",
       "  ' are as accu-\\nrate as independently trained counterparts without the multiple expensive forward passes.\\nMatryoshka Representations provide an intermediate abstraction between high-dimensional vec-\\ntors and their efficient ANNS indices through the adaptive embeddings nested within the original\\nrepresentation vector (Section 4). All other aforementioned efficiency techniques are complementary\\nand can be readily applied to the learned Matryoshka Representations obtained from MRL.\\nSeveral works in efficient neural network literature [9, 93, 100] aim at packing neural networks of\\nvarying capacity within the same larger network. However, the weights for each progressively smaller\\nnetwork can be different and often require distinct forward passes to isolate the final representations.\\nThis is detr'),\n",
       " (0.6227559666648677,\n",
       "  'e whether MRL\\nis able to learn Matryoshka Representations at dimensions in between the representation size\\nfor which it was trained, we also tabulate the performance of MRL at interpolated Ds ∈\\n{12, 24, 48, 96, 192, 384, 768, 1536} as MRL–Interpolated and MRL–E–Interpolated (see Table 8).\\nWe observed that performance scaled nearly monotonically between the original representation\\n23\\nTable 6: Cosine similarity between embeddings\\nAvg. Cosine Similarity\\nALIGN\\nALIGN-MRL\\nPositive Text to Image\\n0.27\\n0.49\\nRandom Text to Image\\n8e-3\\n-4e-03\\nRandom Image to Image\\n0.10\\n0.08\\nRandom Text to Text\\n0.22\\n0.07\\nTable 7: Masked Language Modelling (MLM) accuracy(%) of FF and MRL models on the validation\\nset.\\nRep. Size\\nBERT-FF\\nBERT-MRL\\n12\\n60.12\\n59.92\\n24\\n62.49\\n62.05\\n48\\n63.85\\n63.40\\n96\\n64.32\\n64.15\\n192\\n64.70\\n64.58\\n3'),\n",
       " (0.6216871852251074,\n",
       "  '\\nuse nested dropout in the context of autoencoders to learn nested representations. MRL differentiates\\nitself in formulation by optimizing only for O(log(d)) nesting dimensions instead of O(d). Despite\\nthis, MRL diffuses information to intermediate dimensions interpolating between the optimized\\nMatryoshka Representation sizes accurately (Figure 5); making web-scale feasible.\\n3\\nMatryoshka Representation Learning\\nFor d ∈N, consider a set M ⊂[d] of representation sizes. For a datapoint x in the input do-\\nmain X, our goal is to learn a d-dimensional representation vector z ∈Rd. For every m ∈M,\\n3\\nMatryoshka Representation Learning (MRL) enables each of the first m dimensions of the em-\\nbedding vector, z1:m ∈Rm to be independently capable of being a transferable and general purpose\\nrepresentatio'),\n",
       " (0.6139419411463226,\n",
       "  'o obtain flexible representa-\\ntions (Matryoshka Representations) for adaptive deployment (Section 3).\\n2. Up to 14× faster yet accurate large-scale classification and retrieval using MRL (Section 4).\\n3. Seamless adaptation of MRL across modalities (vision - ResNet & ViT, vision + language -\\nALIGN, language - BERT) and to web-scale data (ImageNet-1K/4K, JFT-300M and ALIGN data).\\n4. Further analysis of MRL’s representations in the context of other downstream tasks (Section 5).\\n2\\n2\\nRelated Work\\nRepresentation Learning.\\nLarge-scale datasets like ImageNet [16, 76] and JFT [85] enabled\\nthe learning of general purpose representations for computer vision [4, 98]. These representations\\nare typically learned through supervised and un/self-supervised learning paradigms. Supervised\\npretraining [29, 51,'),\n",
       " (0.6114371017148251,\n",
       "  'atryoshka Representation Learning (MRL) enables each of the first m dimensions of the em-\\nbedding vector, z1:m ∈Rm to be independently capable of being a transferable and general purpose\\nrepresentation of the datapoint x. We obtain z using a deep neural network F( · ; θF ): X →Rd\\nparameterized by learnable weights θF , i.e., z := F(x; θF ). The multi-granularity is captured through\\nthe set of the chosen dimensions M, that contains less than log(d) elements, i.e., |M| ≤⌊log(d)⌋.\\nThe usual set M consists of consistent halving until the representation size hits a low information\\nbottleneck. We discuss the design choices in Section 4 for each of the representation learning settings.\\nFor the ease of exposition, we present the formulation for fully supervised representation learning\\nvia multi-cl'),\n",
       " (0.6098845080065193,\n",
       "  '2\\n60.48\\n61.71\\n28.51\\n28.45\\n28.85\\n3.00\\n3.55\\n3.59\\n21.70\\n20.38\\n21.77\\n32\\n74.68\\n74.80\\n75.26\\n62.24\\n62.23\\n63.05\\n31.28\\n30.79\\n31.47\\n2.60\\n3.65\\n3.57\\n22.03\\n21.87\\n22.48\\n64\\n75.45\\n75.48\\n76.17\\n63.51\\n63.15\\n63.99\\n32.96\\n32.13\\n33.39\\n2.87\\n3.99\\n3.76\\n22.13\\n22.56\\n23.43\\n128\\n75.47\\n76.05\\n76.46\\n63.67\\n63.52\\n64.69\\n33.93\\n33.48\\n34.54\\n2.81\\n3.71\\n3.73\\n22.73\\n22.73\\n23.70\\n256\\n75.78\\n76.31\\n76.66\\n64.13\\n63.80\\n64.71\\n34.80\\n33.91\\n34.85\\n2.77\\n3.65\\n3.60\\n22.63\\n22.88\\n23.59\\n512\\n76.30\\n76.48\\n76.82\\n64.11\\n64.09\\n64.78\\n35.53\\n34.20\\n34.97\\n2.37\\n3.57\\n3.59\\n23.41\\n22.89\\n23.67\\n1024\\n76.74\\n76.60\\n76.93\\n64.43\\n64.20\\n64.95\\n36.06\\n34.22\\n34.99\\n2.53\\n3.56\\n3.68\\n23.44\\n22.98\\n23.72\\n2048\\n77.10\\n76.65\\n76.95\\n64.69\\n64.17\\n64.93\\n37.10\\n34.29\\n35.07\\n2.93\\n3.49\\n3.59\\n24.05\\n23.01\\n23.70\\nMatryoshka Representation Learning-2048 dimensional model. This also showed that some in-\\nstances '),\n",
       " (0.6076745304562865,\n",
       "  ' equipped with retrieval capabilities that can bring forward every\\ninstance [7]. Approximate Nearest Neighbor Search (ANNS) [42] makes it feasible with efficient\\nindexing [14] and traversal [5, 6] to present the users with the most similar documents/images from\\nthe database for a requested query. Widely adopted HNSW [62] (O(d log(N))) is as accurate as\\nexact retrieval (O(dN)) at the cost of a graph-based index overhead for RAM and disk [44].\\nMRL tackles the linear dependence on embedding size,\\nd,\\nby learning multifidelity\\nMatryoshka Representations.\\nLower-dimensional Matryoshka Representations are as accu-\\nrate as independently trained counterparts without the multiple expensive forward passes.\\nMatryoshka Representations provide an intermediate abstraction between high-dimensional vec-\\ntor'),\n",
       " (0.6034099366909469,\n",
       "  'rd)” had a clear visual distinction between the object and background and\\nthus predictions using 8 dimensions also led to a good inter-class separability within the superclass.\\n5.1\\nAblations\\nTable 26 in Appendix K presents that Matryoshka Representations can be enabled within off-the-\\nshelf pretrained models with inexpensive partial finetuning thus paving a way for ubiquitous adoption\\nof MRL. At the same time, Table 27 in Appendix C indicates that with optimal weighting of the\\nnested losses we could improve accuracy of lower-dimensions representations without accuracy\\nloss. Tables 28 and 29 in Appendix C ablate over the choice of initial granularity and spacing of the\\ngranularites. Table 28 reaffirms the design choice to shun extremely low dimensions that have poor\\nclassification accuracy '),\n",
       " (0.6024789031615334,\n",
       "  'ght decay of 1e-4.\\nOur code (Appendix A) makes minimal modifications to the training pipeline provided by FFCV to\\nlearn Matryoshka Representations.\\nWe trained ViT-B/16 models for JFT-300M on a 8x8 cloud TPU pod [49] using Tensorflow [1] with a\\nbatchsize of 128 and trained for 300K steps. Similarly, ALIGN models were trained using Tensorflow\\non 8x8 cloud TPU pod for 1M steps with a batchsize of 64 per TPU. Both these models were trained\\nwith adafactor optimizer [81] with a linear learning rate decay starting at 1e-3.\\nLastly, we trained a BERT-Base model on English Wikipedia and BookCorpus. We trained our models\\nin Tensorflow using a 4x4 cloud TPU pod with a total batchsize of 1024. We used AdamW [61]\\noptimizer with a linear learning rate decay starting at 1e-4 and trained for 450K steps.\\nIn'),\n",
       " (0.6000199673818964,\n",
       "  'ired semantic information for coarser classification that could be leveraged for adaptive routing\\nfor retrieval and classification. Mutifidelity of Matryoshka Representation naturally captures the\\nunderlying hierarchy of the class labels with one single model. Lastly, Figure 11 showcases the\\naccuracy trends per superclass with MRL. The utility of additional dimensions in distinguishing\\na class from others within the same superclass is evident for “garment” which has up to 11%\\nimprovement for 8 →16 dimensional representation transition. We also observed that superclasses\\nsuch as “oscine (songbird)” had a clear visual distinction between the object and background and\\nthus predictions using 8 dimensions also led to a good inter-class separability within the superclass.\\n5.1\\nAblations\\nTable 26 '),\n",
       " (0.5954570726124385,\n",
       "  'ation (MRL–AC)\\n. . . . . . . . . . . . . . . . . . . . . . . . .\\n21\\nD.2\\nJFT, ALIGN and BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n22\\nE\\nImage Retrieval\\n22\\nF\\nAdaptive Retrieval\\n24\\nG Few-shot and Sample Efficiency\\n25\\nH Robustness Experiments\\n27\\nI\\nIn Practice Costs\\n27\\nJ\\nAnalysis of Model Disagreement\\n29\\nK Ablation Studies\\n32\\nK.1\\nMRL Training Paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n32\\nK.2\\nRetrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n33\\n18\\nA\\nCode for Matryoshka Representation Learning\\n(MRL)\\nWe use Alg 1 and 2 provided below to train supervised ResNet50–MRL models on ImageNet-1K.\\nWe provide this code as a template to extend MRL to any domain.\\nAlgorithm 1 Pytorch code for Matryoshka Cross-Entropy '),\n",
       " (0.5945083003065906,\n",
       "  'daptive Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n4.3\\nRetrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n4.3.1\\nAdaptive Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n7\\n5\\nFurther Analysis and Ablations\\n8\\n5.1\\nAblations\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n10\\n6\\nDiscussion and Conclusions\\n10\\nA Code for Matryoshka Representation Learning\\n(MRL)\\n19\\nB\\nDatasets\\n20\\nC Matryoshka Representation Learning Model Training\\n20\\nD Classification Results\\n21\\nD.1 Adaptive Classification (MRL–AC)\\n. . . . . . . . . . . . . . . . . . . . . . . . .\\n21\\nD.2\\nJFT, ALIGN and BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n22\\nE\\nImage Retrieval\\n22\\nF\\nAdaptive Retrieval'),\n",
       " (0.5942351905020298,\n",
       "  'anguage\\nprocessing via masked language modeling (MLM) with BERT [19], whose results are tabulated\\nin Table 7. Without any hyper-parameter tuning, we observed Matryoshka Representations to be\\nwithin 0.5% of FF representations for BERT MLM validation accuracy. This is a promising initial\\nresult that could help with large-scale adaptive document retrieval using BERT–MRL.\\nE\\nImage Retrieval\\nWe evaluated the strength of Matryoshka Representations via image retrieval on ImageNet-1K (the\\ntraining distribution), as well as on out-of-domain datasets ImageNetV2 and ImageNet-4K for all\\n22\\nTable 4: ViT-B/16 and ViT-B/16-MRL top-1 and top-5 k-NN accuracy (%) for ALIGN and JFT. Top-1\\nentries where MRL–E and MRL outperform baselines are bolded for both ALIGN and JFT-ViT.\\nRep. Size\\nALIGN\\nALIGN-MRL\\nJFT-ViT\\n'),\n",
       " (0.593793304256026,\n",
       "  'entation to enable dataset and representation aware retrieval. (4) Finally, the\\njoint optimization of multi-objective MRL combined with end-to-end learnable search data-structure\\nto have data-driven adaptive large-scale retrieval for web-scale search applications.\\nIn conclusion, we presented\\nMatryoshka Representation Learning (MRL), a flexible represen-\\ntation learning approach that encodes information at multiple granularities in a single embedding\\nvector. This enables the MRL to adapt to a downstream task’s statistical complexity as well as\\nthe available compute resources. We demonstrate that MRL can be used for large-scale adaptive\\nclassification as well as adaptive retrieval. On standard benchmarks, MRL matches the accuracy of\\nthe fixed-feature baseline despite using 14× smaller repres'),\n",
       " (0.5926139938669125,\n",
       "  ',\\n(1)\\nwhere L: RL × [L] →R+ is the multi-class softmax cross-entropy loss function. This is a standard\\noptimization problem that can be solved using sub-gradient descent methods. We set all the impor-\\ntance scales, cm = 1 for all m ∈M; see Section 5 for ablations. Lastly, despite only optimizing\\nfor O(log(d)) nested dimensions, MRL results in accurate representations, that interpolate, for\\ndimensions that fall between the chosen granularity of the representations (Section 4.2).\\nWe call this formulation as Matryoshka Representation Learning (MRL). A natural way to make\\nthis efficient is through weight-tying across all the linear classifiers, i.e., by defining W(m) = W1:m\\nfor a set of common weights W ∈RL×d. This would reduce the memory cost due to the linear\\nclassifiers by almost half, whic'),\n",
       " (0.5921911320371547,\n",
       "  'e distribution, without sacrificing accuracy on other classes (Table 16 in\\nAppendix G). Additionally we find the accuracy between low-dimensional and high-dimensional\\nrepresentations is marginal for pretrain classes. We hypothesize that the higher-dimensional represen-\\ntations are required to differentiate the classes when few training examples of each are known. This\\nresults provides further evidence that different tasks require varying capacity based on their difficulty.\\nDisagreement across Dimensions.\\nThe information packing in Matryoshka Representations\\noften results in gradual increase of accuracy with increase in capacity. However, we observed that\\n8\\n(a)\\n(b)\\n(c)\\nFigure 9: Grad-CAM [80] progression of predictions in MRL model across 8, 16, 32 and 2048\\ndimensions. (a) 8-dimensional rep'),\n",
       " (0.5888916040078592,\n",
       "  '.98\\n23.72\\n2048\\n77.10\\n76.65\\n76.95\\n64.69\\n64.17\\n64.93\\n37.10\\n34.29\\n35.07\\n2.93\\n3.49\\n3.59\\n24.05\\n23.01\\n23.70\\nMatryoshka Representation Learning-2048 dimensional model. This also showed that some in-\\nstances and classes could benefit from lower-dimensional representations.\\nDiscussion of Oracle Accuracy\\nBased on our observed model disagreements for different rep-\\nresentation sizes d, we defined an optimal oracle accuracy [58] for MRL. We labeled an image as\\ncorrectly predicted if classification using any representation size was correct. The percentage of\\ntotal samples of ImageNet-1K that were firstly correctly predicted using each representation size d is\\nshown in Table 22. This defined an upper bound on the performance of MRL models, as 18.46%\\nof the ImageNet-1K validation set were incorrectly pre'),\n",
       " (0.5874099420719332,\n",
       "  'nts and screenshots, if\\napplicable? [N/A]\\n(b) Did you describe any potential participant risks, with links to Institutional Review\\nBoard (IRB) approvals, if applicable? [N/A]\\n(c) Did you include the estimated hourly wage paid to participants and the total amount\\nspent on participant compensation? [N/A]\\n17\\nContents\\n1\\nIntroduction\\n1\\n2\\nRelated Work\\n3\\n3\\nMatryoshka Representation Learning\\n3\\n4\\nApplications\\n4\\n4.1\\nRepresentation Learning\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n4\\n4.2\\nClassification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n5\\n4.2.1\\nAdaptive Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n4.3\\nRetrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n4.3.1\\nAdaptive Retrieva'),\n",
       " (0.5872387923549818,\n",
       "  'dim. Similarly, Figure 3\\nshowcases the comparison of learned representation quality through 1-NN accuracy on ImageNet-1K\\n(trainset with 1.3M samples as the database and validation set with 50K samples as the queries).\\nMatryoshka Representations are up to 2% more accurate than their fixed-feature counterparts for\\nthe lower-dimensions while being as accurate elsewhere. 1-NN accuracy is an excellent proxy, at no\\nadditional training cost, to gauge the utility of learned representations in the downstream tasks.\\nWe also evaluate the quality of the representations from training ViT-B/16 on JFT-300M alongside the\\nViT-B/16 vision encoder of the ALIGN model – two web-scale setups. Due to the expensive nature of\\nthese experiments, we only train the highest capacity fixed feature model and choose rand'),\n",
       " (0.5830865194510487,\n",
       "  'ectation using the distribution of representation\\nsizes. As shown in Table 3 and Figure 6, we observed that in expectation, we only needed a ∼37\\nsized representation to achieve 76.3% classification accuracy on ImageNet-1K, which was roughly\\n14× smaller than the FF–512 baseline. Even if we computed the expectation as a weighted average\\nover the cumulative sum of representation sizes {8, 24, 56, . . .}, due to the nature of multiple linear\\nheads for MRL, we ended up with an expected size of 62 that still provided a roughly 8.2× efficient\\nrepresentation than the FF–512 baseline. However, MRL–E alleviates this extra compute with a\\nminimal drop in accuracy.\\nD.2\\nJFT, ALIGN and BERT\\nWe examine the k-NN classification accuracy of learned Matryoshka Representations via\\nALIGN–MRL and JFT-ViT–MRL in '),\n",
       " (0.5825781833524961,\n",
       "  ' retrieval (Section 4.3.1). Finally, as MRL explicitly\\nlearns coarse-to-fine representation vectors, intuitively it should share more semantic information\\namong its various dimensions (Figure 5). This is reflected in up to 2% accuracy gains in long-tail\\ncontinual learning settings while being as robust as the original embeddings. Furthermore, due to its\\ncoarse-to-fine grained nature, MRL can also be used as method to analyze hardness of classification\\namong instances and information bottlenecks.\\nWe make the following key contributions:\\n1. We introduce\\nMatryoshka Representation Learning (MRL) to obtain flexible representa-\\ntions (Matryoshka Representations) for adaptive deployment (Section 3).\\n2. Up to 14× faster yet accurate large-scale classification and retrieval using MRL (Section 4).\\n3'),\n",
       " (0.577511501482278,\n",
       "  'shows that MRL also\\nimproves the cosine similarity span between positive and random image-text pairs.\\nFew-shot and Long-tail Learning.\\nWe exhaustively evaluated few-shot learning on MRL models\\nusing nearest class mean [79]. Table 15 in Appendix G shows that that representations learned\\nthrough MRL perform comparably to FF representations across varying shots and number of classes.\\nMatryoshka Representations realize a unique pattern while evaluating on FLUID [92], a long-tail\\nsequential learning framework. We observed that MRL provides up to 2% accuracy higher on novel\\nclasses in the tail of the distribution, without sacrificing accuracy on other classes (Table 16 in\\nAppendix G). Additionally we find the accuracy between low-dimensional and high-dimensional\\nrepresentations is marginal for p'),\n",
       " (0.5770915507592963,\n",
       "  '8\\nFF\\n86.40\\n37.09\\n71.74\\n10.77\\n37.04\\n52.67\\nMRL\\n85.60\\n36.83\\n70.34\\n12.88\\n37.46\\n52.18\\nMRL–E\\n83.01\\n29.99\\n65.37\\n7.60\\n31.97\\n47.16\\nTable 17: Top-1 classification accuracy (%) on out-of-domain datasets (ImageNet-V2/R/A/Sketch) to\\nexamine robustness of Matryoshka Representation Learning. Note that these results are without\\nany fine tuning on these datasets.\\nImageNet-V1\\nImageNet-V2\\nImageNet-R\\nImageNet-A\\nImageNet-Sketch\\nRep. Size\\nFF\\nMRL–E\\nMRL\\nFF\\nMRL–E\\nMRL\\nFF\\nMRL–E\\nMRL\\nFF\\nMRL–E\\nMRL\\nFF\\nMRL–E\\nMRL\\n8\\n65.86\\n56.92\\n67.46\\n54.05\\n47.40\\n55.59\\n24.60\\n22.98\\n23.57\\n2.92\\n3.63\\n3.39\\n17.73\\n15.07\\n17.98\\n16\\n73.10\\n72.38\\n73.80\\n60.52\\n60.48\\n61.71\\n28.51\\n28.45\\n28.85\\n3.00\\n3.55\\n3.59\\n21.70\\n20.38\\n21.77\\n32\\n74.68\\n74.80\\n75.26\\n62.24\\n62.23\\n63.05\\n31.28\\n30.79\\n31.47\\n2.60\\n3.65\\n3.57\\n22.03\\n21.87\\n22.48\\n64\\n75.45\\n75.48\\n76.17\\n63.51\\n63.15\\n63.99\\n32.96\\n'),\n",
       " (0.5722447951471032,\n",
       "  'all 1,000 ImageNet-1K classes.\\nObjectNet [2] contains 50K images across 313 object classes, each containing ∼160 images each.\\nC\\nMatryoshka Representation Learning Model Training\\nWe trained all ResNet50–MRL models using the efficient dataloaders of FFCV [56]. We utilized the\\nrn50_40_epochs.yaml configuration file of FFCV to train all MRL models defined below:\\n• MRL: ResNet50 model with the fc layer replaced by MRL_Linear_Layer(efficient=False)\\n• MRL–E: ResNet50 model with the fc layer replaced by MRL_Linear_Layer(efficient=True)\\n• FF–k: ResNet50 model with the fc layer replaced by torch.nn.Linear(k, num_classes),\\nwhere k ∈[8, 16, 32, 64, 128, 256, 512, 1024, 2048]. We will henceforth refer to these models as\\nsimply FF, with the k value denoting representation size.\\nWe trained all ResNet50 m'),\n",
       " (0.570924660253639,\n",
       "  'ecognition, pages\\n11162–11173, 2021.\\n[19] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional\\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n[20] T. G. Dietterich and G. Bakiri. Solving multiclass learning problems via error-correcting\\noutput codes. Journal of artificial intelligence research, 2:263–286, 1994.\\n[21] S. K. Divvala, A. Farhadi, and C. Guestrin. Learning everything about anything: Webly-\\nsupervised visual concept learning. In Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition, pages 3270–3277, 2014.\\n[22] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. De-\\nhghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transfor'),\n",
       " (0.5690873722612929,\n",
       "  'yers can be found\\nin the ResNet50 architecture [29]. All models were finetuned with the FFCV pipeline, with same\\ntraining configuration as in the end-to-end training aside from changing lr = 0.1 and epochs = 10. We\\nobserved that finetuning the linear layer alone was insufficient to learn Matryoshka Representations\\nat lower dimensionalities. Adding more and more non-linear conv+ReLU layers steadily improved\\nclassification accuracy of d = 8 from 5% to 60% after finetuning, which was only 6% less than\\ntraining MRL end-to-end for 40 epochs. This difference was successively less pronounced as we\\nincreased dimensionality past d = 64, to within 1.5% for all larger dimensionalities. The full results\\nof this ablation can be seen in Table 26.\\nRelative Importance.\\nWe performed an ablation of MRL over'),\n",
       " (0.5677658481985594,\n",
       "  'clear\\ntrend. When we repeated this experiment with independently trained FF models, we noticed that 950\\nclasses did not show a clear trend. This motivated us to leverage the disagreement as well as gradual\\nimprovement of accuracy at different representation sizes by training Matryoshka Representations.\\nFigure 12 showcases the progression of relative per-class accuracy distribution compared to the\\n29\\nTable 16: Accuracy (%) categories indicates whether classes were present during ImageNet pretraining\\nand head/tail indicates classes that have greater/less than 50 examples in the streaming test set. We\\nobserved that MRL performed better than the baseline on novel tail classes by ∼2% on average.\\nRep. Size\\nMethod\\nPretrain\\n- Head (>50)\\nNovel\\n- Head (>50)\\nPretrain\\n- Tail (<50)\\nNovel\\n- Tail (<50)\\nM'),\n",
       " (0.5623130098077551,\n",
       "  's.\\nTable 25 quantifies the performance with different representation size.\\nK\\nAblation Studies\\nK.1\\nMRL Training Paradigm\\nMatryoshka Representations via Finetuning.\\nTo observe if nesting can be induced in models that\\nwere not explicitly trained with nesting from scratch, we loaded a pretrained FF-2048 ResNet50 model\\nand initialized a new MRL layer, as defined in Algorithm 2, Appendix C. We then unfroze different\\nlayers of the backbone to observe how much non-linearity in the form of unfrozen conv layers needed\\nto be present to enforce nesting into a pretrained FF model. A description of these layers can be found\\nin the ResNet50 architecture [29]. All models were finetuned with the FFCV pipeline, with same\\ntraining configuration as in the end-to-end training aside from changing lr = 0.1 and e'),\n",
       " (0.5599891311422666,\n",
       "  'asun, A. Torralba, and S. Fidler. Aligning\\nbooks and movies: Towards story-like visual explanations by watching movies and reading\\nbooks. In Proceedings of the IEEE international conference on computer vision, pages 19–27,\\n2015.\\n16\\nChecklist\\n1. For all authors...\\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s\\ncontributions and scope? [Yes]\\n(b) Did you describe the limitations of your work? [Yes] See Section 6\\n(c) Did you discuss any potential negative societal impacts of your work? [N/A] Our work\\ndoes not have any additional negative societal impact on top of the existing impact of\\nrepresentation learning. However, a study on the trade-off between representation size\\nand the tendency to encode biases is an interesting future direction along the '),\n",
       " (0.5599012070213202,\n",
       "  '68\\n70.14\\n69.54\\n69.01\\n68.41\\n2048\\n70.98\\n65.20\\n63.57\\n62.56\\n61.60\\n70.18\\n69.52\\n68.98\\n68.35\\n1024\\n2048\\n1312\\n70.97\\n65.20\\n63.57\\n62.56\\n61.60\\n70.18\\n69.52\\n68.98\\n68.35\\nThese results provide further evidence that different tasks require varying capacity based on their\\ndifficulty.\\nH\\nRobustness Experiments\\nWe evaluated the robustness of MRL models on out-of-domain datasets (ImageNetV2/R/A/Sketch)\\nand compared them to the FF baseline. Each of these datasets is described in Appendix B. The\\nresults in Table 17 demonstrate that learning Matryoshka Representations does not hurt out-of-\\ndomain generalization relative to FF models, and Matryoshka Representations in fact improve\\nthe performance on ImageNet-A. For a ALIGN–MRL model, we examine the the robustness via\\nzero-shot retrieval on out-of-domain datasets, i'),\n",
       " (0.5553750035422427,\n",
       "  'ties without the\\nadditional expense of multiple model forward passes for the web-scale databases. FF models\\nalso generate independent databases which become prohibitively expense to store and switch in\\nbetween. Matryoshka Representations enable adaptive retrieval (AR) which alleviates the need\\nto use full-capacity representations, d = 2048, for all data and downstream tasks. Lastly, all the\\nvector compression techniques [60, 45] used as part of the ANNS pipelines are complimentary to\\nMatryoshka Representations and can further improve the efficiency-vs-accuracy trade-off.\\n4.3.1\\nAdaptive Retrieval\\nWe benchmark MRL in the adaptive retrieval setting (AR) [50]. For a given query image, we obtained\\na shortlist, K = 200, of images from the database using a lower-dimensional representation, e.g.\\nD'),\n",
       " (0.5542394041982246,\n",
       "  ', J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,\\nA. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International\\njournal of computer vision, 115(3):211–252, 2015.\\n[77] R. Salakhutdinov and G. Hinton. Learning a nonlinear embedding by preserving class neigh-\\nbourhood structure. In Artificial Intelligence and Statistics, pages 412–419. PMLR, 2007.\\n[78] R. Salakhutdinov and G. Hinton. Semantic hashing. International Journal of Approximate\\nReasoning, 50(7):969–978, 2009.\\n[79] J. S. Sánchez, F. Pla, and F. J. Ferri. On the use of neighbourhood-based non-parametric\\nclassifiers. Pattern Recognition Letters, 18(11-13):1179–1186, 1997.\\n[80] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-cam:\\nVisual explanation'),\n",
       " (0.5536231537554214,\n",
       "  'ar(k, num_classes),\\nwhere k ∈[8, 16, 32, 64, 128, 256, 512, 1024, 2048]. We will henceforth refer to these models as\\nsimply FF, with the k value denoting representation size.\\nWe trained all ResNet50 models with a learning rate of 0.475 with a cyclic learning rate schedule [83].\\nThis was after appropriate scaling (0.25×) of the learning rate specified in the configuration file to\\naccommodate for 2xA100 NVIDIA GPUs available for training, compared to the 8xA100 GPUs\\nutilized in the FFCV benchmarks. We trained with a batch size of 256 per GPU, momentum [86] of\\n0.9, and an SGD optimizer with a weight decay of 1e-4.\\nOur code (Appendix A) makes minimal modifications to the training pipeline provided by FFCV to\\nlearn Matryoshka Representations.\\nWe trained ViT-B/16 models for JFT-300M on a 8x8 clo'),\n",
       " (0.5527970542179229,\n",
       "  'sentation size for both top-1 and mAP@10, and especially\\nat low representation size (Ds ≤32). MRL–E loses out to FF significantly only at Ds = 8. This\\nindicates that training ResNet50 models via the MRL training paradigm improves retrieval at low\\nrepresentation size over models explicitly trained at those representation size (FF-8...2048).\\nWe carried out all retrieval experiments at Ds ∈{8, 16, 32, 64, 128, 256, 512, 1024, 2048}, as\\nthese were the representation sizes which were a part of the nesting_list at which losses\\nwere added during training, as seen in Algorithm 1, Appendix A. To examine whether MRL\\nis able to learn Matryoshka Representations at dimensions in between the representation size\\nfor which it was trained, we also tabulate the performance of MRL at interpolated Ds ∈\\n{12, 2'),\n",
       " (0.5527825989485938,\n",
       "  'ce of MRL model on 31-way classification (1 extra class is for reject token) on\\nImageNet-1K superclasses.\\nRep. Size\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\nMRL\\n85.57\\n88.67\\n89.48\\n89.82\\n89.97\\n90.11\\n90.18\\n90.22\\n90.21\\nMatryoshka Representations at Arbitrary Granularities.\\nTo train MRL, we used nested di-\\nmensions at logarithmic granularities M = {8, 16, . . . , 1024, 2048} as detailed in Section 3. We\\nmade this choice for two empirically-driven reasons: a) The accuracy improvement with increasing\\nrepresentation size was more logarithmic than linear (as shown by FF models in Figure 2). This indi-\\ncated that optimizing for granularities increasing in a non-logarithmic fashion would be sub-optimal\\nboth for maximum performance and expected efficiency; b) If we have m arbitrary granularities,\\nthe expected'),\n",
       " (0.5476693297077694,\n",
       "  'at optimizing for granularities increasing in a non-logarithmic fashion would be sub-optimal\\nboth for maximum performance and expected efficiency; b) If we have m arbitrary granularities,\\nthe expected cost of the linear classifier to train MRL scales as O(L ∗(m2)) while logarithmic\\ngranularities result in O(L ∗2log(d)) space and compute costs.\\nTo demonstrate this effect, we learned Matryoshka Representations with uniform (MRL-Uniform)\\nnesting dimensions m\\n∈\\nM\\n=\\n{8, 212, 416, 620, 824, 1028, 1232, 1436, 1640, 1844, 2048}.\\nWe\\nevaluated\\nthis\\nmodel\\nat\\nthe\\nstandard\\n(MRL-log)\\ndimensions\\nm\\n∈\\nM\\n=\\n{8, 16, 32, 64, 128, 256, 512, 1024, 2048} for ease of comparison to reported numbers using 1-NN ac-\\ncuracy (%). As shown in Table 29, we observed that while performance interpolated, MRL-Uniform\\nsuffered'),\n",
       " (0.5462262093895836,\n",
       "  'able 7: Masked Language Modelling (MLM) accuracy(%) of FF and MRL models on the validation\\nset.\\nRep. Size\\nBERT-FF\\nBERT-MRL\\n12\\n60.12\\n59.92\\n24\\n62.49\\n62.05\\n48\\n63.85\\n63.40\\n96\\n64.32\\n64.15\\n192\\n64.70\\n64.58\\n384\\n65.03\\n64.81\\n768\\n65.54\\n65.00\\nsize and the interpolated representation size as we increase Ds, which demonstrates that MRL is\\nable to learn Matryoshka Representations at nearly all representation size m ∈[8, 2048] despite\\noptimizing only for |M| nested representation sizes.\\nWe examined the robustness of MRL for retrieval on out-of-domain datasets ImageNetV2 and\\nImageNet-4K, as shown in Table 9 and Table 10 respectively. On ImageNetV2, we observed that MRL\\noutperformed FF at all Ds on top-1 Accuracy and mAP@10, and MRL–E outperformed FF at all\\nDs except Ds = 8. This demonstrates the robustness'),\n",
       " (0.5455449716057305,\n",
       "  'nd deployment.\\nMRL learns coarse-to-fine representations that are at least as accurate and rich as\\nindependently trained low-dimensional representations. The flexibility within the\\nlearned Matryoshka Representations offer: (a) up to 14× smaller embedding\\nsize for ImageNet-1K classification at the same level of accuracy; (b) up to 14×\\nreal-world speed-ups for large-scale retrieval on ImageNet-1K and 4K; and (c) up\\nto 2% accuracy improvements for long-tail few-shot classification, all while being\\nas robust as the original representations. Finally, we show that MRL extends seam-\\nlessly to web-scale datasets (ImageNet, JFT) across various modalities – vision\\n(ViT, ResNet), vision + language (ALIGN) and language (BERT). MRL code and\\npretrained models are open-sourced at https://github.com/RAIVN'),\n",
       " (0.5443439496433947,\n",
       "  'd Table 10 respectively. On ImageNetV2, we observed that MRL\\noutperformed FF at all Ds on top-1 Accuracy and mAP@10, and MRL–E outperformed FF at all\\nDs except Ds = 8. This demonstrates the robustness of the learned Matryoshka Representations\\nfor out-of-domain image retrieval.\\nF\\nAdaptive Retrieval\\nThe time complexity of retrieving a shortlist of k-NN often scales as O(d), where d =Ds, for a\\nfixed k and N. We thus will have a theoretical 256× higher cost for Ds = 2048 over Ds = 8. We\\ndiscuss search complexity in more detail in Appendix I. In an attempt to replicate performance at\\nhigher Ds while using less FLOPs, we perform adaptive retrieval via retrieving a k-NN shortlist with\\nrepresentation size Ds, and then re-ranking the shortlist with representations of size Dr. Adaptive\\nretrieval for'),\n",
       " (0.5424137626472727,\n",
       "  ' improve efficiency for large-scale classification and retrieval\\nwithout any significant loss of accuracy. While there are potentially several applications of coarse-to-\\nfine Matryoshka Representations, in this work we focus on two key building blocks of real-world\\nML systems: large-scale classification and retrieval. For classification, we use adaptive cascades with\\nthe variable-size representations from a model trained with MRL, significantly reducing the average\\ndimension of embeddings needed to achieve a particular accuracy. For example, on ImageNet-1K,\\nMRL + adaptive classification results in up to a 14× smaller representation size at the same accuracy\\nas baselines (Section 4.2.1). Similarly, we use MRL in an adaptive retrieval system. Given a query,\\nwe shortlist retrieval candidates '),\n",
       " (0.5375119381113274,\n",
       "  't). Every combination of Ds & Dr falls above the Pareto\\nline (orange dots) of single-shot retrieval with a fixed representation size while having configurations\\nthat are as accurate while being up to 14× faster in real-world deployment. Funnel retrieval is almost\\nas accurate as the baseline while alleviating some of the parameter choices of Adaptive Retrieval.\\npoint, further strengthening the use-case for Matryoshka Representation Learning and adaptive\\nretrieval.\\nEven with adaptive retrieval, it is hard to determine the choice of Ds & Dr. In order to alleviate this\\nissue to an extent, we propose Funnel Retrieval, a consistent cascade for adaptive retrieval. Funnel\\nthins out the initial shortlist by a repeated re-ranking and shortlisting with a series of increasing\\ncapacity representations.'),\n",
       " (0.5368681211927807,\n",
       "  'ion size for MRL &\\nFF models showing the capture of underlying\\nhierarchy through tight information bottlenecks.\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\nRepresentation Size\\n65\\n70\\n75\\n80\\n85\\n90\\n95\\nTop-1 Accuracy (%)\\nmeasuring device\\nbuilding\\ngarment\\ntool\\nnourishment\\nprotective covering\\nvessel\\noscine\\nFigure 11:\\nDiverse per-superclass accuracy\\ntrends across representation sizes for ResNet50-\\nMRL on ImageNet-1K.\\n9\\noccurs with both MRL and FF models; MRL is more accurate across dimensions. This shows that\\ntight information bottlenecks while not highly accurate for fine-grained classification, do capture\\nrequired semantic information for coarser classification that could be leveraged for adaptive routing\\nfor retrieval and classification. Mutifidelity of Matryoshka Representation naturally captures the\\nund'),\n",
       " (0.5353374734080837,\n",
       "  'Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig.\\nScaling up visual and vision-language representation learning with noisy text supervision. In\\nInternational Conference on Machine Learning, pages 4904–4916. PMLR, 2021.\\n[47] J. Johnson, M. Douze, and H. Jégou. Billion-scale similarity search with GPUs. IEEE\\nTransactions on Big Data, 7(3):535–547, 2019.\\n[48] W. B. Johnson. Extensions of lipschitz mappings into a hilbert space. Contemp. Math., 26:\\n189–206, 1984.\\n[49] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, S. Bates, S. Bhatia,\\nN. Boden, A. Borchers, et al. In-datacenter performance analysis of a tensor processing unit.\\nIn Proceedings of the 44th annual international symposium on computer architecture, pages\\n1–12, 2017.\\n[50] T.'),\n",
       " (0.5348532318390784,\n",
       "  ' accuracy with increase in capacity. However, we observed that\\n8\\n(a)\\n(b)\\n(c)\\nFigure 9: Grad-CAM [80] progression of predictions in MRL model across 8, 16, 32 and 2048\\ndimensions. (a) 8-dimensional representation confuses due to presence of other relevant objects (with\\na larger field of view) in the scene and predicts “shower cap” ; (b) 8-dim model confuses within\\nthe same super-class of “boa” ; (c) 8 and 16-dim models incorrectly focus on the eyes of the doll\\n(\"sunglasses\") and not the \"sweatshirt\" which is correctly in focus at higher dimensions; MRL fails\\ngracefully in these scenarios and shows potential use cases of disagreement across dimensions.\\nthis trend was not ubiquitous and certain instances and classes were more accurate when evaluated\\nwith lower-dimensions (Figure 12 in Appendi'),\n",
       " (0.5335190015996832,\n",
       "  'Conference on Machine Learning, pages 5389–5400. PMLR,\\n2019.\\n[73] O. Rippel, M. Gelbart, and R. Adams. Learning ordered representations with nested dropout.\\nIn International Conference on Machine Learning, pages 1746–1754. PMLR, 2014.\\n[74] J. Rissanen. Modeling by shortest data description. Automatica, 14(5):465–471, 1978.\\n[75] S. Ruder, M. E. Peters, S. Swayamdipta, and T. Wolf. Transfer learning in natural language\\nprocessing. In Proceedings of the 2019 conference of the North American chapter of the\\nassociation for computational linguistics: Tutorials, pages 15–18, 2019.\\n[76] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,\\nA. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International\\njournal of computer vision, 115'),\n",
       " (0.5329396411522742,\n",
       "  'ows potential use cases of disagreement across dimensions.\\nthis trend was not ubiquitous and certain instances and classes were more accurate when evaluated\\nwith lower-dimensions (Figure 12 in Appendix J). With perfect routing of instances to appropriate\\ndimension, MRL can gain up to 4.6% classification accuracy. At the same time, the low-dimensional\\nmodels are less accurate either due to confusion within the same superclass [24] of the ImageNet\\nhierarchy or presence of multiple objects of interest. Figure 9 showcases 2 such examples for 8-\\ndimensional representation. These results along with Appendix J put forward the potential for MRL\\nto be a systematic framework for analyzing the utility and efficiency of information bottlenecks.\\nSuperclass Accuracy.\\nAs the information bottleneck become'),\n",
       " (0.5297972762941769,\n",
       "  'ch\\nusing hierarchical navigable small world graphs. IEEE transactions on pattern analysis and\\nmachine intelligence, 42(4):824–836, 2018.\\n[63] J. Masci, U. Meier, D. Cire¸san, and J. Schmidhuber. Stacked convolutional auto-encoders for\\nhierarchical feature extraction. In International conference on artificial neural networks, pages\\n52–59. Springer, 2011.\\n[64] P. Mitra, C. Murthy, and S. K. Pal. Unsupervised feature selection using feature similarity.\\nIEEE transactions on pattern analysis and machine intelligence, 24(3):301–312, 2002.\\n[65] V. Nanda, T. Speicher, J. P. Dickerson, S. Feizi, K. P. Gummadi, and A. Weller. Diffused\\nredundancy in pre-trained representations. arXiv preprint arXiv:2306.00183, 2023.\\n[66] P. Nayak. Understanding searches better than ever before. Google AI Blog, 2019. '),\n",
       " (0.5294138108662926,\n",
       "  'ny\\nrepresentation size. The remaining 81.54% constitutes the oracle accuracy.\\nRep. Size\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\nAlways\\nWrong\\nCorrectly\\nPredicted\\n67.46\\n8.78\\n2.58\\n1.35\\n0.64\\n0.31\\n0.20\\n0.12\\n0.06\\n18.46\\nof disagreement arising when the models got confused within the same superclass. For example,\\nImageNet-1K has multiple \"snake\" classes, and models often confuse a snake image for an incorrect\\nspecies of snake.\\nSuperclass Performance\\nWe created a 30 superclass subset of the validation set based on wordnet\\nhierarchy (Table 24) to quantify the performance of MRL model on ImageNet-1K superclasses.\\nTable 25 quantifies the performance with different representation size.\\nK\\nAblation Studies\\nK.1\\nMRL Training Paradigm\\nMatryoshka Representations via Finetuning.\\nTo observe if nesting can be induced '),\n",
       " (0.5281898438317135,\n",
       "  ' Ds = 64 on ImageNet-1K and ImageNet-4K, at 32× less\\nMFLOPs. This demonstrates the value of intelligent routing techniques which utilize appropriately\\nsized Matryoshka Representations for retrieval.\\n24\\nTable 8: Retrieve a shortlist of 200-NN with Ds sized representations on ImageNet-1K via exact\\nsearch with L2 distance metric. Top-1 and mAP@10 entries (%) where MRL–E and MRL outperform\\nFF at their respective representation sizes are bolded.\\nModel\\nDs\\nMFlops\\nTop-1\\nTop-5\\nTop-10\\nmAP@10\\nmAP@25\\nmAP@50\\nmAP@100\\nP@10\\nP@25\\nP@50\\nP@100\\nFF\\n8\\n10\\n58.93\\n75.76\\n80.25\\n53.42\\n52.29\\n51.84\\n51.57\\n59.32\\n59.28\\n59.25\\n59.21\\n16\\n20\\n66.77\\n80.88\\n84.40\\n61.63\\n60.51\\n59.98\\n59.62\\n66.76\\n66.58\\n66.43\\n66.27\\n32\\n41\\n68.84\\n82.58\\n86.14\\n63.35\\n62.08\\n61.36\\n60.76\\n68.43\\n68.13\\n67.83\\n67.48\\n64\\n82\\n69.41\\n83.56\\n87.33\\n63.26\\n61.64\\n60.63\\n59.67\\n68.4'),\n",
       " (0.5270041001366375,\n",
       "  'ines significantly, which indicates that\\npretrained models lack the multifidelity of Matryoshka Representations and are incapable of fitting\\nan accurate linear classifier at low representation sizes.\\nWe compared the performance of MRL models at various representation sizes via 1-nearest neighbors\\n(1-NN) image classification accuracy on ImageNet-1K in Table 2 and Figure 3. We provide detailed\\ninformation regarding the k-NN search pipeline in Appendix E. We compared against a baseline\\nof attempting to enforce nesting to a FF-2048 model by 1) Random Feature Selection (Rand. FS):\\nconsidering the first m dimensions of FF-2048 for NN lookup, and 2) FF+SVD: performing SVD\\non the FF-2048 representations at the specified representation size, 3) FF+JL: performing random\\nprojection according to the J'),\n",
       " (0.5269248286908924,\n",
       "  're 5: Despite optimizing MRL only for\\nO(log(d)) dimensions for ResNet50 and ViT-\\nB/16 models; the accuracy in the intermediate\\ndimensions shows interpolating behaviour.\\n4.2.1\\nAdaptive Classification\\nThe flexibility and coarse-to-fine granularity within Matryoshka Representations allows model\\ncascades [90] for Adaptive Classification (AC) [28]. Unlike standard model cascades [95], MRL does\\nnot require multiple expensive neural network forward passes. To perform AC with an MRL trained\\nmodel, we learn thresholds on the maximum softmax probability [33] for each nested classifier on\\na holdout validation set. We then use these thresholds to decide when to transition to the higher\\ndimensional representation (e.g 8 →16 →32) of the MRL model. Appendix D.1 discusses the\\nimplementation and learning o'),\n",
       " (0.5209211382600961,\n",
       "  'Gummadi, and A. Weller. Diffused\\nredundancy in pre-trained representations. arXiv preprint arXiv:2306.00183, 2023.\\n[66] P. Nayak. Understanding searches better than ever before. Google AI Blog, 2019. URL https:\\n//blog.google/products/search/search-language-understanding-bert/.\\n[67] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\\nN. Gimelshein, L. Antiga, et al.\\nPytorch: An imperative style, high-performance deep\\nlearning library. Advances in neural information processing systems, 32, 2019.\\n[68] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer.\\nDeep contextualized word representations. In Proceedings of the 2018 Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language\\nTe'),\n",
       " (0.5208230806293211,\n",
       "  's, 27, 2014.\\n[99] H.-F. Yu, K. Zhong, J. Zhang, W.-C. Chang, and I. S. Dhillon. Pecos: Prediction for enormous\\nand correlated output spaces. Journal of Machine Learning Research, 23(98):1–32, 2022.\\n[100] J. Yu, L. Yang, N. Xu, J. Yang, and T. Huang. Slimmable neural networks. arXiv preprint\\narXiv:1812.08928, 2018.\\n[101] R. Zellers, J. Lu, X. Lu, Y. Yu, Y. Zhao, M. Salehi, A. Kusupati, J. Hessel, A. Farhadi, and\\nY. Choi. Merlot reserve: Neural script knowledge through vision and language and sound.\\narXiv preprint arXiv:2201.02639, 2022.\\n[102] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning\\nbooks and movies: Towards story-like visual explanations by watching movies and reading\\nbooks. In Proceedings of the IEEE international conference on compute'),\n",
       " (0.5193596850758098,\n",
       "  '-accuracy\\ntrade-off\\nfor\\nadaptive\\nretrieval\\nusing\\nMatryoshka Representations compared to single-shot using fixed features with ResNet50\\non ImageNet-1K. We observed that all AR settings lied above the Pareto frontier of single-shot\\nretrieval with varying representation sizes. In particular for ImageNet-1K, we show that the AR\\nmodel with Ds = 16 & Dr = 2048 is as accurate as single-shot retrieval with d = 2048 while being\\n∼128× more efficient in theory and ∼14× faster in practice (compared using HNSW on the same\\nhardware). We show similar trends with ImageNet-4K, but note that we require Ds = 64 given\\nthe increased difficulty of the dataset. This results in ∼32× and ∼6× theoretical and in-practice\\nspeedups respectively. Lastly, while K = 200 works well for our adaptive retrieval experiments, '),\n",
       " (0.5177714300798575,\n",
       "  'ithin the same larger network. However, the weights for each progressively smaller\\nnetwork can be different and often require distinct forward passes to isolate the final representations.\\nThis is detrimental for adaptive inference due to the need for re-encoding the entire retrieval database\\nwith expensive sub-net forward passes of varying capacities. Several works [23, 26, 65, 59] investigate\\nthe notions of intrinsic dimensionality and redundancy of representations and objective spaces pointing\\nto minimum description length [74]. Finally, ordered representations proposed by Rippel et al. [73]\\nuse nested dropout in the context of autoencoders to learn nested representations. MRL differentiates\\nitself in formulation by optimizing only for O(log(d)) nesting dimensions instead of O(d). Despit'),\n",
       " (0.517359076452567,\n",
       "  ') utilization\\nof the representation for downstream applications [50, 89]. Compute costs for the latter part of the\\npipeline scale with the embedding dimensionality as well as the data size (N) and label space (L).\\nAt web-scale [15, 85] this utilization cost overshadows the feature computation cost. The rigidity in\\nthese representations forces the use of high-dimensional embedding vectors across multiple tasks\\ndespite the varying resource and accuracy constraints that require flexibility.\\nHuman perception of the natural world has a naturally coarse-to-fine granularity [28, 32]. However,\\nperhaps due to the inductive bias of gradient-based training [84], deep learning models tend to diffuse\\n“information” across the entire representation vector. The desired elasticity is usually enabled in the'),\n",
       " (0.5171991943357835,\n",
       "  't MRL can be used for large-scale adaptive\\nclassification as well as adaptive retrieval. On standard benchmarks, MRL matches the accuracy of\\nthe fixed-feature baseline despite using 14× smaller representation size on average. Furthermore, the\\nMatryoshka Representation based adaptive shortlisting and re-ranking system ensures comparable\\nmAP@10 to the baseline while being 128× cheaper in FLOPs and 14× faster in wall-clock time.\\nFinally, most of the efficiency techniques for model inference and vector search are complementary\\nto MRL\\nfurther assisting in deployment at the compute-extreme environments.\\nAcknowledgments\\nWe are grateful to Srinadh Bhojanapalli, Lovish Madaan, Raghav Somani, Ludwig Schmidt, and\\nVenkata Sailesh Sanampudi for helpful discussions and feedback. Aditya Kusupati also tha'),\n",
       " (0.5131716307952339,\n",
       "  '5736\\n0.6060\\n1.2781\\n2.7047\\nHNSW32\\n0.1193\\n0.1455\\n0.1833\\n0.2145\\n0.2333\\n0.2670\\nobservation on the expected dimensionality for 76.30% top-1 classification accuracy being just\\nd ∼37. We leave the design and learning of a more optimal policy for future work.\\nGrad-CAM Examples\\nWe analyzed the nature of model disagreement across representation\\nsizes with MRL models with the help of Grad-CAM visualization [80]. We observed there were\\ncertain classes in ImageNet-1K such as \"tools\", \"vegetables\" and \"meat cutting knife\" which were\\noccasionally located around multiple objects and a cluttered environment. In such scenarios, we\\nobserved that smaller representation size models would often get confused due to other objects and fail\\nto extract the object of interest which generated the correct label. We als'),\n",
       " (0.5128708405238176,\n",
       "  ' on top of the existing impact of\\nrepresentation learning. However, a study on the trade-off between representation size\\nand the tendency to encode biases is an interesting future direction along the lines of\\nexisting literature [36, 37]. A part of this is already presented in Section 5.\\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to\\nthem? [Yes]\\n2. If you are including theoretical results...\\n(a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n(b) Did you include complete proofs of all theoretical results? [N/A]\\n3. If you ran experiments...\\n(a) Did you include the code, data, and instructions needed to reproduce the main ex-\\nperimental results (either in the supplemental material or as a URL)? [Yes] See sup-\\nplemental mater'),\n",
       " (0.511742456781981,\n",
       "  '2020.\\n[70] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understand-\\ning by generative pre-training. OpenAI Blog, 2018. URL https://openai.com/blog/\\nlanguage-unsupervised/.\\n14\\n[71] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\\nP. Mishkin, J. Clark, et al. Learning transferable visual models from natural language su-\\npervision. In International Conference on Machine Learning, pages 8748–8763. PMLR,\\n2021.\\n[72] B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to\\nimagenet? In International Conference on Machine Learning, pages 5389–5400. PMLR,\\n2019.\\n[73] O. Rippel, M. Gelbart, and R. Adams. Learning ordered representations with nested dropout.\\nIn International Conference on Machine Lear'),\n",
       " (0.5114873377601222,\n",
       "  'rXiv preprint arXiv:1911.05248, 2019.\\n[37] S. Hooker, N. Moorosi, G. Clark, S. Bengio, and E. Denton. Characterising bias in compressed\\nmodels. arXiv preprint arXiv:2010.03058, 2020.\\n[38] H. Hotelling. Analysis of a complex of statistical variables into principal components. Journal\\nof educational psychology, 24(6):417, 1933.\\n[39] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and\\nH. Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications.\\narXiv preprint arXiv:1704.04861, 2017.\\n[40] J. Howard and S. Ruder. Universal language model fine-tuning for text classification. arXiv\\npreprint arXiv:1801.06146, 2018.\\n[41] H. Hu, D. Dey, M. Hebert, and J. A. Bagnell. Learning anytime predictions in neural networks\\nvia adaptive loss bal'),\n",
       " (0.5114715787295717,\n",
       "  'rs.\\nWe also found that for both MRL and FF, as the shot number decreased, the required representa-\\ntion size to reach optimal accuracy decreased (Table 15). For example, we observed that 1-shot\\nperformance at 32 representation size had equal accuracy to 2048 representation size.\\nFLUID.\\nFor the long-tailed setting we evaluated MRL on the FLUID benchmark [92] which\\ncontains a mixture of pretrain and new classes. Table 16 shows the evaluation of the learned\\nrepresentation on FLUID. We observed that MRL provided up to 2% higher accuracy on novel\\nclasses in the tail of the distribution, without sacrificing accuracy on other classes. Additionally we\\nfound the accuracy between low-dimensional and high-dimensional representations was marginal for\\npretrain classes. For example, the 64-dimensional M'),\n",
       " (0.5106968707378711,\n",
       "  '= {12, 24, 48, 96, 192, 384, 768} as\\nthe explicitly optimized nested dimensions respectively. Lastly, we extensively compare the MRL\\nand MRL–E models to independently trained low-dimensional (fixed feature) representations (FF),\\ndimensionality reduction (SVD), sub-net method (slimmable networks [100]) and randomly selected\\nfeatures of the highest capacity FF model.\\nIn section 4.2, we evaluate the quality and capacity of the learned representations through linear\\nclassification/probe (LP) and 1-nearest neighbour (1-NN) accuracy. Experiments show that MRL\\nmodels remove the dependence on |M| resource-intensive independently trained models for the\\ncoarse-to-fine representations while being as accurate. Lastly, we show that despite optimizing only\\nfor |M| dimensions, MRL models diffuse the info'),\n",
       " (0.5083666138788462,\n",
       "  'ing 1.8B image-text pairs.\\nImageNet Robustness Datasets\\nWe experimented on the following datasets to examine the robust-\\nness of MRL models:\\nImageNetV2 [72] is a collection of 10K images sampled a decade after the original construction of\\nImageNet [16]. ImageNetV2 contains 10 examples each from the 1,000 classes of ImageNet-1K.\\nImageNet-A [35] contains 7.5K real-world adversarially filtered images from 200 ImageNet-\\n1K classes.\\nImageNet-R [34] contains 30K artistic image renditions for 200 of the original ImageNet-1K classes.\\nImageNet-Sketch [94] contains 50K sketches, evenly distributed over all 1,000 ImageNet-1K classes.\\nObjectNet [2] contains 50K images across 313 object classes, each containing ∼160 images each.\\nC\\nMatryoshka Representation Learning Model Training\\nWe trained all ResNet5'),\n",
       " (0.5070474858605135,\n",
       "  'ViT-B/16 vision\\nencoder and BERT language encoder on ALIGN data [46] and (c) Masked language modelling:\\nBERT [19] on English Wikipedia and BooksCorpus [102]. Please refer to Appendices B and C for\\ndetails regarding the model architectures, datasets and training specifics.\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\nRepresentation Size\\n40\\n50\\n60\\n70\\n80\\nTop-1 Accuracy (%)\\nMRL\\nMRL-E\\nFF\\nSVD\\nSlim. Net\\nRand. LP\\nFigure 2: ImageNet-1K linear classification ac-\\ncuracy of ResNet50 models. MRL is as accurate\\nas the independently trained FF models for every\\nrepresentation size.\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\nRepresentation Size\\n40\\n50\\n60\\n70\\n1-NN Accuracy (%)\\nMRL\\nMRL-E\\nFF\\nSVD\\nSlim. Net\\nRand. FS\\nFigure 3:\\nImageNet-1K 1-NN accuracy of\\nResNet50 models measuring the representation\\nquality for downstream task. MRL ou'),\n",
       " (0.506942638074451,\n",
       "  'V. Vanhoucke,\\nV. Vasudevan, F. Viégas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and\\nX. Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL\\nhttps://www.tensorflow.org/. Software available from tensorflow.org.\\n[2] A. Barbu, D. Mayo, J. Alverio, W. Luo, C. Wang, D. Gutfreund, J. Tenenbaum, and B. Katz.\\nObjectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition\\nmodels. Advances in neural information processing systems, 32, 2019.\\n[3] S. Bengio, J. Weston, and D. Grangier. Label embedding trees for large multi-class tasks.\\nAdvances in Neural Information Processing Systems, 23, 2010.\\n[4] Y. Bengio. Deep learning of representations for unsupervised and transfer learning. In\\nProceedings of ICML workshop on unsupervised'),\n",
       " (0.5061362743844308,\n",
       "  'n artificial intelligence and statistics, pages 297–304. JMLR Workshop and Conference\\nProceedings, 2010.\\n[28] M. G. Harris and C. D. Giachritsis. Coarse-grained information dominates fine-grained\\ninformation in judgments of time-to-contact from retinal flow. Vision research, 40(6):601–611,\\n2000.\\n[29] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In\\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 770–\\n778, 2016.\\n[30] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual\\nrepresentation learning. In Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, pages 9729–9738, 2020.\\n[31] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick. Masked autoencoders'),\n",
       " (0.5051052204649187,\n",
       "  'le datasets (ImageNet, JFT) across various modalities – vision\\n(ViT, ResNet), vision + language (ALIGN) and language (BERT). MRL code and\\npretrained models are open-sourced at https://github.com/RAIVNLab/MRL.\\n1\\nIntroduction\\nLearned representations [57] are fundamental building blocks of real-world ML systems [66, 91].\\nTrained once and frozen, d-dimensional representations encode rich information and can be used\\nto perform multiple downstream tasks [4]. The deployment of deep representations has two steps:\\n(1) an expensive yet constant-cost forward pass to compute the representation [29] and (2) utilization\\nof the representation for downstream applications [50, 89]. Compute costs for the latter part of the\\npipeline scale with the embedding dimensionality as well as the data size (N) and lab'),\n",
       " (0.5042899607847322,\n",
       "  '. Yang, and S. Kumar. Pre-training tasks for embedding-\\nbased large-scale retrieval. arXiv preprint arXiv:2002.03932, 2020.\\n[11] W.-C. Chang, D. Jiang, H.-F. Yu, C. H. Teo, J. Zhang, K. Zhong, K. Kolluri, Q. Hu,\\nN. Shandilya, V. Ievgrafov, et al. Extreme multi-label learning for semantic matching in\\nproduct search. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discov-\\nery & Data Mining, pages 2643–2651, 2021.\\n[12] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning\\nof visual representations. In International conference on machine learning, pages 1597–1607.\\nPMLR, 2020.\\n[13] Y. Chen, Z. Liu, H. Xu, T. Darrell, and X. Wang. Meta-baseline: exploring simple meta-\\nlearning for few-shot learning. In Proceedings of the IEEE/CVF Internationa'),\n",
       " (0.503084682879535,\n",
       "  'e, and L. Zettlemoyer.\\nDeep contextualized word representations. In Proceedings of the 2018 Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long Papers), pages 2227–2237, New Orleans, Louisiana, June\\n2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1202. URL https:\\n//aclanthology.org/N18-1202.\\n[69] Y. Prabhu, A. Kusupati, N. Gupta, and M. Varma. Extreme regression for dynamic search\\nadvertising. In Proceedings of the 13th International Conference on Web Search and Data\\nMining, pages 456–464, 2020.\\n[70] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understand-\\ning by generative pre-training. OpenAI Blog, 2018. URL https://openai.com/blog/\\nlanguage-unsupervise'),\n",
       " (0.5024545628468082,\n",
       "  ' results in Section 5.1 reveal interesting weaknesses of MRL that would be logical directions\\nfor future work. (1) Optimizing the weightings of the nested losses to obtain a Pareto optimal\\naccuracy-vs-efficiency trade-off – a potential solution could emerge from adaptive loss balancing\\naspects of anytime neural networks [41]. (2) Using different losses at various fidelities aimed at\\nsolving a specific aspect of adaptive deployment – e.g. high recall for 8-dimension and robustness\\nfor 2048-dimension. (3) Learning a search data-structure, like differentiable k-d tree, on top of\\nMatryoshka Representation to enable dataset and representation aware retrieval. (4) Finally, the\\njoint optimization of multi-objective MRL combined with end-to-end learnable search data-structure\\nto have data-driven a'),\n",
       " (0.5011721572030657,\n",
       "  'rmance of ResNet50 representations on ImageNet-1K across\\ndimensionalities for MRL, MRL–E, FF, slimmable networks along with post-hoc compression\\nof vectors using SVD and random feature selection. Matryoshka Representations are often the\\nmost accurate while being up to 3% better than the FF baselines. Similar to classification, post-hoc\\ncompression and slimmable network baselines suffer from significant drop-off in retrieval mAP@10\\nwith ≤256 dimensions. Appendix E discusses the mAP@10 of the same models on ImageNet-4K.\\nMRL models are capable of performing accurate retrieval at various granularities without the\\nadditional expense of multiple model forward passes for the web-scale databases. FF models\\nalso generate independent databases which become prohibitively expense to store and switch i'),\n",
       " (0.5006452472724213,\n",
       "  'lg 1 and 2 provided below to train supervised ResNet50–MRL models on ImageNet-1K.\\nWe provide this code as a template to extend MRL to any domain.\\nAlgorithm 1 Pytorch code for Matryoshka Cross-Entropy Loss\\nclass Matryoshka_CE_Loss(nn.Module):\\ndef __init__(self, relative_importance, **kwargs):\\nsuper(Matryoshka_CE_Loss, self).__init__()\\nself.criterion = nn.CrossEntropyLoss(**kwargs)\\nself.relative_importance = relative_importance # usually set\\nto all ones\\ndef forward(self, output, target):\\nloss=0\\nfor i in range(len(output)):\\nloss+= self.relative_importance[i] * self.criterion(output[\\ni], target)\\nreturn loss\\nAlgorithm 2 Pytorch code for MRL Linear Layer\\nclass MRL_Linear_Layer(nn.Module):\\ndef __init__(self, nesting_list: List, num_classes=1000, efficient=\\nFalse, **kwargs):\\nsuper(MRL_Linear_Layer')]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation\n",
    "Now that we have built our \"vector database\" and retrieved our context, we do the inference part:\n",
    "\n",
    "First, we augment the prompt using the context we retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py4ragTools.prompt_tools import PromptTools\n",
    "\n",
    "pt = PromptTools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test some other system templates\n",
    "\n",
    "SYS_PROMPT_1 = \"\"\" \\\n",
    "Context:\n",
    "{context}\n",
    "Based on the context provided above, answer the following question. Do not use any external knowledge.\n",
    "If the answer is not present in the context, respond with \"I don't know.\"\n",
    "If the context is empty or irrelevant to the question, respond with \"No relevant information found in the context.\"\n",
    "\"\"\"\n",
    "\n",
    "SYS_PROMPT_2 = \"\"\" \\\n",
    "Context:\n",
    "{context}\n",
    "Based on the context provided above, answer the following question. Do not use any external knowledge.\n",
    "Ensure your response cites the relevant sentence numbers from the context for each fact you provide.\"\n",
    "\"\"\"\n",
    "\n",
    "SYS_PROMPT_3 = \"\"\" \\\n",
    "Context:\n",
    "{context}\n",
    "Based on the context provided above, answer the question. Do only use information from the provided context in your answer\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "pt.SYSTEM_TEMPLATE = SYS_PROMPT_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oshka Representation Learning (MRL) to induce flexibility in the learned\n",
      "representation. MRL learns representations of varying capacities within the same high-dimensional\n",
      "vector through explicit optimization of O(log(d)) lower-dimensional vectors in a nested fashion,\n",
      "hence the name Matryoshka. MRL can be adapted to any existing representation pipeline and\n",
      "is easily extended to many standard tasks in computer vision and natural language processing.\n",
      "Figure 1 illustrates the core idea of Matryoshka Representation Learning (MRL) and the adaptive\n",
      "deployment settings of the learned Matryoshka Representations.\n",
      "Adaptive Retrieval\n",
      "Shortlisting\n",
      "Re-ranking\n",
      "Adaptive Classification\n",
      "Training\n",
      "Inference\n",
      "<latexit sha1_base64=\"eh9hk+peBkdsPY6v+r4rONmxYLY=\">A\n",
      "B7nicbVBNSwMxEJ2tX7V+VT16CRbBU9kVoR6LXjxWsB/QLiWb\\n, and signif-\n",
      "icantly for lower ones. This demonstrates that training to learn Matryoshka Representations\n",
      "is feasible and extendable even for extremely large scale datasets.\n",
      "We also demonstrate that\n",
      "Matryoshka Representations are learned at interpolated dimensions for both ALIGN and JFT-\n",
      "ViT, as shown in Table 5, despite not being trained explicitly at these dimensions. Lastly, Table 6\n",
      "shows that MRL training leads to a increase in the cosine similarity span between positive and\n",
      "random image-text pairs.\n",
      "We also evaluated the capability of Matryoshka Representations to extend to other natural language\n",
      "processing via masked language modeling (MLM) with BERT [19], whose results are tabulated\n",
      "in Table 7. Without any hyper-parameter tuning, we observed Matryoshka Representations to be\n",
      "within 0.\\nib+53USE1x4KZdxYpiks0NBIrCJcFYB7nPFqBFjSwhV3GbFdEgUocYWVbQluPNfXiTN06rVN3bs3LtMq+jAIdwBVw4RxqcA1aAFB\n",
      "c/wCm/oEb2gd/QxG1C+c4B/AH6/AGZEJHn</latexit>\n",
      "Figure\n",
      "1:\n",
      "Matryoshka Representation Learning\n",
      "is\n",
      "adaptable to any representation learning setup and begets\n",
      "a Matryoshka Representation z\n",
      "by optimizing the orig-\n",
      "inal\n",
      "loss\n",
      "L(.)\n",
      "at\n",
      "O(log(d))\n",
      "chosen\n",
      "representation\n",
      "sizes.\n",
      "Matryoshka Representation can be utilized effectively for adap-\n",
      "tive deployment across environments and downstream tasks.\n",
      "The first m-dimensions, m ∈[d], of\n",
      "the Matryoshka Representation is\n",
      "an information-rich low-dimensional\n",
      "vector, at no additional training cost,\n",
      "that is as accurate as an indepen-\n",
      "dently trained m-dimensional repre-\n",
      "sentation.\n",
      "The information within\n",
      "the Matryoshka Representation in-\n",
      "creases with the dimensionality \\nerse set of ap-\n",
      "plications along with an extensive evaluation of the learned multifidelity representations. Further,\n",
      "we showcase the downstream applications of the learned Matryoshka Representations for flexible\n",
      "large-scale deployment through (a) Adaptive Classification (AC) and (b) Adaptive Retrieval (AR).\n",
      "4.1\n",
      "Representation Learning\n",
      "We adapt Matryoshka Representation Learning (MRL) to various representation learning setups\n",
      "(a) Supervised learning for vision: ResNet50 [29] on ImageNet-1K [76] and ViT-B/16 [22] on\n",
      "JFT-300M [85], (b) Contrastive learning for vision + language: ALIGN model with ViT-B/16 vision\n",
      "encoder and BERT language encoder on ALIGN data [46] and (c) Masked language modelling:\n",
      "BERT [19] on English Wikipedia and BooksCorpus [102]. Please refer to Appendices B and C for\n",
      "det\\n weight-tying across all the linear classifiers, i.e., by defining W(m) = W1:m\n",
      "for a set of common weights W ∈RL×d. This would reduce the memory cost due to the linear\n",
      "classifiers by almost half, which would be crucial in cases of extremely large output spaces [89, 99].\n",
      "This variant is called Efficient Matryoshka Representation Learning (MRL–E). Refer to Alg 1\n",
      "and Alg 2 in Appendix A for the building blocks of Matryoshka Representation Learning (MRL).\n",
      "Adaptation to Learning Frameworks.\n",
      "MRL can be adapted seamlessly to most representation\n",
      "learning frameworks at web-scale with minimal modifications (Section 4.1). For example, MRL’s\n",
      "adaptation to masked language modelling reduces to MRL–E due to the weight-tying between the\n",
      "input embedding matrix and the linear classifier. For contrastive lea\\nons (Section 4.1). For example, MRL’s\n",
      "adaptation to masked language modelling reduces to MRL–E due to the weight-tying between the\n",
      "input embedding matrix and the linear classifier. For contrastive learning, both in context of vision &\n",
      "vision + language, MRL is applied to both the embeddings that are being contrasted with each other.\n",
      "The presence of normalization on the representation needs to be handled independently for each of\n",
      "the nesting dimension for best results (see Appendix C for more details).\n",
      "4\n",
      "Applications\n",
      "In this section, we discuss Matryoshka Representation Learning (MRL) for a diverse set of ap-\n",
      "plications along with an extensive evaluation of the learned multifidelity representations. Further,\n",
      "we showcase the downstream applications of the learned Matryoshka Representations f\\nMatryoshka Representation Learning\n",
      "Aditya Kusupati∗†⋄, Gantavya Bhatt∗†, Aniket Rege∗†,\n",
      "Matthew Wallingford†, Aditya Sinha⋄, Vivek Ramanujan†, William Howard-Snyder†,\n",
      "Kaifeng Chen⋄, Sham Kakade‡, Prateek Jain⋄and Ali Farhadi†\n",
      "†University of Washington, ⋄Google Research, ‡Harvard University\n",
      "{kusupati,ali}@cs.washington.edu, prajain@google.com\n",
      "Abstract\n",
      "Learned representations are a central component in modern ML systems, serv-\n",
      "ing a multitude of downstream tasks. When training such representations, it\n",
      "is often the case that computational and statistical constraints for each down-\n",
      "stream task are unknown. In this context, rigid fixed-capacity representations\n",
      "can be either over or under-accommodating to the task at hand. This leads us\n",
      "to ask: can we design a flexible representation that can ad\\nguage applications are built [40] on large language models [8] that are pretrained [68, 75]\n",
      "in a un/self-supervised fashion with masked language modelling [19] or autoregressive training [70].\n",
      "Matryoshka Representation Learning (MRL) is complementary to all these setups and can be\n",
      "adapted with minimal overhead (Section 3). MRL equips representations with multifidelity at no\n",
      "additional cost which enables adaptive deployment based on the data and task (Section 4).\n",
      "Efficient Classification and Retrieval.\n",
      "Efficiency in classification and retrieval during inference\n",
      "can be studied with respect to the high yet constant deep featurization costs or the search cost which\n",
      "scales with the size of the label space and data. Efficient neural networks address the first issue\n",
      "through a variety of algorithm\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n",
      "arXiv:2205.13147v4  [cs.LG]  8 Feb 2024\n",
      "due to training/maintenance overhead, numerous expensive forward passes through all of the data,\n",
      "storage and memory cost for multiple copies of encoded data, expensive on-the-fly feature selection\n",
      "or a significant drop in accuracy. By encoding coarse-to-fine-grained representations, which are as\n",
      "accurate as the independently trained counterparts, we learn with minimal overhead a representation\n",
      "that can be deployed adaptively at no additional cost during inference.\n",
      "We introduce\n",
      "Matryoshka Representation Learning (MRL) to induce flexibility in the learned\n",
      "representation. MRL learns representations of varying capacities within the same high-dimensional\n",
      "vector through explicit optim\\nP7tBRJKQCk04VqDnFi7KZaEU7HpW6iaIzJEPdpx1CBQ6rcdJ/DPeNEsBeJM0TGk7UnxspDpUahb6ZzNKqWS8T/M6ie6duSkTcaKpINDvYRDHcGsDBgwSYnmI0MwkcxkhWSAJSbaVFYyJaDZL/8lzaMqcqro5rhcu\n",
      "8jrKIJdsAcqAIFTUANXoA4agIAH8ARewKv1aD1b9b7dLRg5Tvb4Besj2/eCZSw</latexit>\n",
      "<latexit sha1_base64=\"OPHM4ACsGr0VI7qMpDgoN+t2ICI=\">AB9XicbVDLSgMx\n",
      "FL3xWeur6tJNsAh1U2ZE0GXRjQsXFewD2rFk0kwbmskMSUapQ/DjQtF3Pov7vwbM+0stPVA4HDOvdyT48eCa+M432hpeWV1b2wUdzc2t7ZLe3tN3WUKMoaNBKRavtEM8Elaxh\n",
      "uBGvHipHQF6zlj64yv/XAlOaRvDPjmHkhGUgecEqMle67ITHDQJERvqk8nfRKZafqTIEXiZuTMuSo90pf3X5Ek5BJQwXRuM6sfFSogyngk2K3USzmNARGbCOpZKETHvpNPUEH1\n",
      "ulj4NI2ScNnq/N1ISaj0OfTuZpdTzXib+53USE1x4KZdxYpiks0NBIrCJcFYB7nPFqBFjSwhV3GbFdEgUocYWVbQluPNfXiTN06rVN3bs3LtMq+jAIdwBVw4RxqcA1aAFB\n",
      "c/wCm/oEb2gd/QxG1C+c4B/AH6/AGZEJHn</latexit>\n",
      "Figure\n",
      "1:\n",
      "Matryoshka Representation Learning\n",
      "is\n",
      "adapta\\nscuss the design choices in Section 4 for each of the representation learning settings.\n",
      "For the ease of exposition, we present the formulation for fully supervised representation learning\n",
      "via multi-class classification. Matryoshka Representation Learning modifies the typical setting\n",
      "to become a multi-scale representation learning problem on the same task. For example, we train\n",
      "ResNet50 [29] on ImageNet-1K [76] which embeds a 224 × 224 pixel image into a d = 2048\n",
      "representation vector and then passed through a linear classifier to make a prediction, ˆy among the\n",
      "L = 1000 labels. For MRL, we choose M = {8, 16, . . . , 1024, 2048} as the nesting dimensions.\n",
      "Suppose we are given a labelled dataset D = {(x1, y1), . . . , (xN, yN)} where xi ∈X is an input\n",
      "point and yi ∈[L] is the label of xi for\\n unknown. In this context, rigid fixed-capacity representations\n",
      "can be either over or under-accommodating to the task at hand. This leads us\n",
      "to ask: can we design a flexible representation that can adapt to multiple down-\n",
      "stream tasks with varying computational resources? Our main contribution is\n",
      "Matryoshka Representation Learning (MRL) which encodes information at\n",
      "different granularities and allows a single embedding to adapt to the computational\n",
      "constraints of downstream tasks. MRL minimally modifies existing representation\n",
      "learning pipelines and imposes no additional cost during inference and deployment.\n",
      "MRL learns coarse-to-fine representations that are at least as accurate and rich as\n",
      "independently trained low-dimensional representations. The flexibility within the\n",
      "learned Matryoshka \\nleviates this extra compute with a\n",
      "minimal drop in accuracy.\n",
      "D.2\n",
      "JFT, ALIGN and BERT\n",
      "We examine the k-NN classification accuracy of learned Matryoshka Representations via\n",
      "ALIGN–MRL and JFT-ViT–MRL in Table 4.\n",
      "For ALIGN [46], we observed that learning\n",
      "Matryoshka Representations via ALIGN–MRL improved classification accuracy at nearly all\n",
      "dimensions when compared to ALIGN. We observed a similar trend when training ViT-B/16 [22]\n",
      "for JFT-300M [85] classification, where learning Matryoshka Representations via MRL and\n",
      "MRL–E on top of JFT-ViT improved classification accuracy for nearly all dimensions, and signif-\n",
      "icantly for lower ones. This demonstrates that training to learn Matryoshka Representations\n",
      "is feasible and extendable even for extremely large scale datasets.\n",
      "We also demonstrate that\n",
      "M\\nf general purpose representations for computer vision [4, 98]. These representations\n",
      "are typically learned through supervised and un/self-supervised learning paradigms. Supervised\n",
      "pretraining [29, 51, 82] casts representation learning as a multi-class/label classification problem,\n",
      "while un/self-supervised learning learns representation via proxy tasks like instance classification [97]\n",
      "and reconstruction [31, 63]. Recent advances [12, 30] in contrastive learning [27] enabled learning\n",
      "from web-scale data [21] that powers large-capacity cross-modal models [18, 46, 71, 101]. Similarly,\n",
      "natural language applications are built [40] on large language models [8] that are pretrained [68, 75]\n",
      "in a un/self-supervised fashion with masked language modelling [19] or autoregressive training [70].\n",
      "Matryos\\nno additional training cost,\n",
      "that is as accurate as an indepen-\n",
      "dently trained m-dimensional repre-\n",
      "sentation.\n",
      "The information within\n",
      "the Matryoshka Representation in-\n",
      "creases with the dimensionality creat-\n",
      "ing a coarse-to-fine grained represen-\n",
      "tation, all without significant training\n",
      "or additional deployment overhead.\n",
      "MRL equips the representation vector\n",
      "with the desired flexibility and multi-\n",
      "fidelity that can ensure a near-optimal\n",
      "accuracy-vs-compute trade-off. With\n",
      "these advantages, MRL enables adap-\n",
      "tive deployment based on accuracy\n",
      "and compute constraints.\n",
      "The Matryoshka Representations improve efficiency for large-scale classification and retrieval\n",
      "without any significant loss of accuracy. While there are potentially several applications of coarse-to-\n",
      "fine Matryoshka Representation\\n are as accu-\n",
      "rate as independently trained counterparts without the multiple expensive forward passes.\n",
      "Matryoshka Representations provide an intermediate abstraction between high-dimensional vec-\n",
      "tors and their efficient ANNS indices through the adaptive embeddings nested within the original\n",
      "representation vector (Section 4). All other aforementioned efficiency techniques are complementary\n",
      "and can be readily applied to the learned Matryoshka Representations obtained from MRL.\n",
      "Several works in efficient neural network literature [9, 93, 100] aim at packing neural networks of\n",
      "varying capacity within the same larger network. However, the weights for each progressively smaller\n",
      "network can be different and often require distinct forward passes to isolate the final representations.\n",
      "This is detr\\ne whether MRL\n",
      "is able to learn Matryoshka Representations at dimensions in between the representation size\n",
      "for which it was trained, we also tabulate the performance of MRL at interpolated Ds ∈\n",
      "{12, 24, 48, 96, 192, 384, 768, 1536} as MRL–Interpolated and MRL–E–Interpolated (see Table 8).\n",
      "We observed that performance scaled nearly monotonically between the original representation\n",
      "23\n",
      "Table 6: Cosine similarity between embeddings\n",
      "Avg. Cosine Similarity\n",
      "ALIGN\n",
      "ALIGN-MRL\n",
      "Positive Text to Image\n",
      "0.27\n",
      "0.49\n",
      "Random Text to Image\n",
      "8e-3\n",
      "-4e-03\n",
      "Random Image to Image\n",
      "0.10\n",
      "0.08\n",
      "Random Text to Text\n",
      "0.22\n",
      "0.07\n",
      "Table 7: Masked Language Modelling (MLM) accuracy(%) of FF and MRL models on the validation\n",
      "set.\n",
      "Rep. Size\n",
      "BERT-FF\n",
      "BERT-MRL\n",
      "12\n",
      "60.12\n",
      "59.92\n",
      "24\n",
      "62.49\n",
      "62.05\n",
      "48\n",
      "63.85\n",
      "63.40\n",
      "96\n",
      "64.32\n",
      "64.15\n",
      "192\n",
      "64.70\n",
      "64.58\n",
      "3\\n\n",
      "use nested dropout in the context of autoencoders to learn nested representations. MRL differentiates\n",
      "itself in formulation by optimizing only for O(log(d)) nesting dimensions instead of O(d). Despite\n",
      "this, MRL diffuses information to intermediate dimensions interpolating between the optimized\n",
      "Matryoshka Representation sizes accurately (Figure 5); making web-scale feasible.\n",
      "3\n",
      "Matryoshka Representation Learning\n",
      "For d ∈N, consider a set M ⊂[d] of representation sizes. For a datapoint x in the input do-\n",
      "main X, our goal is to learn a d-dimensional representation vector z ∈Rd. For every m ∈M,\n",
      "3\n",
      "Matryoshka Representation Learning (MRL) enables each of the first m dimensions of the em-\n",
      "bedding vector, z1:m ∈Rm to be independently capable of being a transferable and general purpose\n",
      "representatio\\no obtain flexible representa-\n",
      "tions (Matryoshka Representations) for adaptive deployment (Section 3).\n",
      "2. Up to 14× faster yet accurate large-scale classification and retrieval using MRL (Section 4).\n",
      "3. Seamless adaptation of MRL across modalities (vision - ResNet & ViT, vision + language -\n",
      "ALIGN, language - BERT) and to web-scale data (ImageNet-1K/4K, JFT-300M and ALIGN data).\n",
      "4. Further analysis of MRL’s representations in the context of other downstream tasks (Section 5).\n",
      "2\n",
      "2\n",
      "Related Work\n",
      "Representation Learning.\n",
      "Large-scale datasets like ImageNet [16, 76] and JFT [85] enabled\n",
      "the learning of general purpose representations for computer vision [4, 98]. These representations\n",
      "are typically learned through supervised and un/self-supervised learning paradigms. Supervised\n",
      "pretraining [29, 51,\\natryoshka Representation Learning (MRL) enables each of the first m dimensions of the em-\n",
      "bedding vector, z1:m ∈Rm to be independently capable of being a transferable and general purpose\n",
      "representation of the datapoint x. We obtain z using a deep neural network F( · ; θF ): X →Rd\n",
      "parameterized by learnable weights θF , i.e., z := F(x; θF ). The multi-granularity is captured through\n",
      "the set of the chosen dimensions M, that contains less than log(d) elements, i.e., |M| ≤⌊log(d)⌋.\n",
      "The usual set M consists of consistent halving until the representation size hits a low information\n",
      "bottleneck. We discuss the design choices in Section 4 for each of the representation learning settings.\n",
      "For the ease of exposition, we present the formulation for fully supervised representation learning\n",
      "via multi-cl\\n2\n",
      "60.48\n",
      "61.71\n",
      "28.51\n",
      "28.45\n",
      "28.85\n",
      "3.00\n",
      "3.55\n",
      "3.59\n",
      "21.70\n",
      "20.38\n",
      "21.77\n",
      "32\n",
      "74.68\n",
      "74.80\n",
      "75.26\n",
      "62.24\n",
      "62.23\n",
      "63.05\n",
      "31.28\n",
      "30.79\n",
      "31.47\n",
      "2.60\n",
      "3.65\n",
      "3.57\n",
      "22.03\n",
      "21.87\n",
      "22.48\n",
      "64\n",
      "75.45\n",
      "75.48\n",
      "76.17\n",
      "63.51\n",
      "63.15\n",
      "63.99\n",
      "32.96\n",
      "32.13\n",
      "33.39\n",
      "2.87\n",
      "3.99\n",
      "3.76\n",
      "22.13\n",
      "22.56\n",
      "23.43\n",
      "128\n",
      "75.47\n",
      "76.05\n",
      "76.46\n",
      "63.67\n",
      "63.52\n",
      "64.69\n",
      "33.93\n",
      "33.48\n",
      "34.54\n",
      "2.81\n",
      "3.71\n",
      "3.73\n",
      "22.73\n",
      "22.73\n",
      "23.70\n",
      "256\n",
      "75.78\n",
      "76.31\n",
      "76.66\n",
      "64.13\n",
      "63.80\n",
      "64.71\n",
      "34.80\n",
      "33.91\n",
      "34.85\n",
      "2.77\n",
      "3.65\n",
      "3.60\n",
      "22.63\n",
      "22.88\n",
      "23.59\n",
      "512\n",
      "76.30\n",
      "76.48\n",
      "76.82\n",
      "64.11\n",
      "64.09\n",
      "64.78\n",
      "35.53\n",
      "34.20\n",
      "34.97\n",
      "2.37\n",
      "3.57\n",
      "3.59\n",
      "23.41\n",
      "22.89\n",
      "23.67\n",
      "1024\n",
      "76.74\n",
      "76.60\n",
      "76.93\n",
      "64.43\n",
      "64.20\n",
      "64.95\n",
      "36.06\n",
      "34.22\n",
      "34.99\n",
      "2.53\n",
      "3.56\n",
      "3.68\n",
      "23.44\n",
      "22.98\n",
      "23.72\n",
      "2048\n",
      "77.10\n",
      "76.65\n",
      "76.95\n",
      "64.69\n",
      "64.17\n",
      "64.93\n",
      "37.10\n",
      "34.29\n",
      "35.07\n",
      "2.93\n",
      "3.49\n",
      "3.59\n",
      "24.05\n",
      "23.01\n",
      "23.70\n",
      "Matryoshka Representation Learning-2048 dimensional model. This also showed that some in-\n",
      "stances \\n equipped with retrieval capabilities that can bring forward every\n",
      "instance [7]. Approximate Nearest Neighbor Search (ANNS) [42] makes it feasible with efficient\n",
      "indexing [14] and traversal [5, 6] to present the users with the most similar documents/images from\n",
      "the database for a requested query. Widely adopted HNSW [62] (O(d log(N))) is as accurate as\n",
      "exact retrieval (O(dN)) at the cost of a graph-based index overhead for RAM and disk [44].\n",
      "MRL tackles the linear dependence on embedding size,\n",
      "d,\n",
      "by learning multifidelity\n",
      "Matryoshka Representations.\n",
      "Lower-dimensional Matryoshka Representations are as accu-\n",
      "rate as independently trained counterparts without the multiple expensive forward passes.\n",
      "Matryoshka Representations provide an intermediate abstraction between high-dimensional vec-\n",
      "tor\\nrd)” had a clear visual distinction between the object and background and\n",
      "thus predictions using 8 dimensions also led to a good inter-class separability within the superclass.\n",
      "5.1\n",
      "Ablations\n",
      "Table 26 in Appendix K presents that Matryoshka Representations can be enabled within off-the-\n",
      "shelf pretrained models with inexpensive partial finetuning thus paving a way for ubiquitous adoption\n",
      "of MRL. At the same time, Table 27 in Appendix C indicates that with optimal weighting of the\n",
      "nested losses we could improve accuracy of lower-dimensions representations without accuracy\n",
      "loss. Tables 28 and 29 in Appendix C ablate over the choice of initial granularity and spacing of the\n",
      "granularites. Table 28 reaffirms the design choice to shun extremely low dimensions that have poor\n",
      "classification accuracy \\nght decay of 1e-4.\n",
      "Our code (Appendix A) makes minimal modifications to the training pipeline provided by FFCV to\n",
      "learn Matryoshka Representations.\n",
      "We trained ViT-B/16 models for JFT-300M on a 8x8 cloud TPU pod [49] using Tensorflow [1] with a\n",
      "batchsize of 128 and trained for 300K steps. Similarly, ALIGN models were trained using Tensorflow\n",
      "on 8x8 cloud TPU pod for 1M steps with a batchsize of 64 per TPU. Both these models were trained\n",
      "with adafactor optimizer [81] with a linear learning rate decay starting at 1e-3.\n",
      "Lastly, we trained a BERT-Base model on English Wikipedia and BookCorpus. We trained our models\n",
      "in Tensorflow using a 4x4 cloud TPU pod with a total batchsize of 1024. We used AdamW [61]\n",
      "optimizer with a linear learning rate decay starting at 1e-4 and trained for 450K steps.\n",
      "In\\nired semantic information for coarser classification that could be leveraged for adaptive routing\n",
      "for retrieval and classification. Mutifidelity of Matryoshka Representation naturally captures the\n",
      "underlying hierarchy of the class labels with one single model. Lastly, Figure 11 showcases the\n",
      "accuracy trends per superclass with MRL. The utility of additional dimensions in distinguishing\n",
      "a class from others within the same superclass is evident for “garment” which has up to 11%\n",
      "improvement for 8 →16 dimensional representation transition. We also observed that superclasses\n",
      "such as “oscine (songbird)” had a clear visual distinction between the object and background and\n",
      "thus predictions using 8 dimensions also led to a good inter-class separability within the superclass.\n",
      "5.1\n",
      "Ablations\n",
      "Table 26 \\nation (MRL–AC)\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "21\n",
      "D.2\n",
      "JFT, ALIGN and BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "22\n",
      "E\n",
      "Image Retrieval\n",
      "22\n",
      "F\n",
      "Adaptive Retrieval\n",
      "24\n",
      "G Few-shot and Sample Efficiency\n",
      "25\n",
      "H Robustness Experiments\n",
      "27\n",
      "I\n",
      "In Practice Costs\n",
      "27\n",
      "J\n",
      "Analysis of Model Disagreement\n",
      "29\n",
      "K Ablation Studies\n",
      "32\n",
      "K.1\n",
      "MRL Training Paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "32\n",
      "K.2\n",
      "Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "33\n",
      "18\n",
      "A\n",
      "Code for Matryoshka Representation Learning\n",
      "(MRL)\n",
      "We use Alg 1 and 2 provided below to train supervised ResNet50–MRL models on ImageNet-1K.\n",
      "We provide this code as a template to extend MRL to any domain.\n",
      "Algorithm 1 Pytorch code for Matryoshka Cross-Entropy \\ndaptive Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "6\n",
      "4.3\n",
      "Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "6\n",
      "4.3.1\n",
      "Adaptive Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "7\n",
      "5\n",
      "Further Analysis and Ablations\n",
      "8\n",
      "5.1\n",
      "Ablations\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "10\n",
      "6\n",
      "Discussion and Conclusions\n",
      "10\n",
      "A Code for Matryoshka Representation Learning\n",
      "(MRL)\n",
      "19\n",
      "B\n",
      "Datasets\n",
      "20\n",
      "C Matryoshka Representation Learning Model Training\n",
      "20\n",
      "D Classification Results\n",
      "21\n",
      "D.1 Adaptive Classification (MRL–AC)\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "21\n",
      "D.2\n",
      "JFT, ALIGN and BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "22\n",
      "E\n",
      "Image Retrieval\n",
      "22\n",
      "F\n",
      "Adaptive Retrieval\\nanguage\n",
      "processing via masked language modeling (MLM) with BERT [19], whose results are tabulated\n",
      "in Table 7. Without any hyper-parameter tuning, we observed Matryoshka Representations to be\n",
      "within 0.5% of FF representations for BERT MLM validation accuracy. This is a promising initial\n",
      "result that could help with large-scale adaptive document retrieval using BERT–MRL.\n",
      "E\n",
      "Image Retrieval\n",
      "We evaluated the strength of Matryoshka Representations via image retrieval on ImageNet-1K (the\n",
      "training distribution), as well as on out-of-domain datasets ImageNetV2 and ImageNet-4K for all\n",
      "22\n",
      "Table 4: ViT-B/16 and ViT-B/16-MRL top-1 and top-5 k-NN accuracy (%) for ALIGN and JFT. Top-1\n",
      "entries where MRL–E and MRL outperform baselines are bolded for both ALIGN and JFT-ViT.\n",
      "Rep. Size\n",
      "ALIGN\n",
      "ALIGN-MRL\n",
      "JFT-ViT\n",
      "\\nentation to enable dataset and representation aware retrieval. (4) Finally, the\n",
      "joint optimization of multi-objective MRL combined with end-to-end learnable search data-structure\n",
      "to have data-driven adaptive large-scale retrieval for web-scale search applications.\n",
      "In conclusion, we presented\n",
      "Matryoshka Representation Learning (MRL), a flexible represen-\n",
      "tation learning approach that encodes information at multiple granularities in a single embedding\n",
      "vector. This enables the MRL to adapt to a downstream task’s statistical complexity as well as\n",
      "the available compute resources. We demonstrate that MRL can be used for large-scale adaptive\n",
      "classification as well as adaptive retrieval. On standard benchmarks, MRL matches the accuracy of\n",
      "the fixed-feature baseline despite using 14× smaller repres\\n,\n",
      "(1)\n",
      "where L: RL × [L] →R+ is the multi-class softmax cross-entropy loss function. This is a standard\n",
      "optimization problem that can be solved using sub-gradient descent methods. We set all the impor-\n",
      "tance scales, cm = 1 for all m ∈M; see Section 5 for ablations. Lastly, despite only optimizing\n",
      "for O(log(d)) nested dimensions, MRL results in accurate representations, that interpolate, for\n",
      "dimensions that fall between the chosen granularity of the representations (Section 4.2).\n",
      "We call this formulation as Matryoshka Representation Learning (MRL). A natural way to make\n",
      "this efficient is through weight-tying across all the linear classifiers, i.e., by defining W(m) = W1:m\n",
      "for a set of common weights W ∈RL×d. This would reduce the memory cost due to the linear\n",
      "classifiers by almost half, whic\\ne distribution, without sacrificing accuracy on other classes (Table 16 in\n",
      "Appendix G). Additionally we find the accuracy between low-dimensional and high-dimensional\n",
      "representations is marginal for pretrain classes. We hypothesize that the higher-dimensional represen-\n",
      "tations are required to differentiate the classes when few training examples of each are known. This\n",
      "results provides further evidence that different tasks require varying capacity based on their difficulty.\n",
      "Disagreement across Dimensions.\n",
      "The information packing in Matryoshka Representations\n",
      "often results in gradual increase of accuracy with increase in capacity. However, we observed that\n",
      "8\n",
      "(a)\n",
      "(b)\n",
      "(c)\n",
      "Figure 9: Grad-CAM [80] progression of predictions in MRL model across 8, 16, 32 and 2048\n",
      "dimensions. (a) 8-dimensional rep\\n.98\n",
      "23.72\n",
      "2048\n",
      "77.10\n",
      "76.65\n",
      "76.95\n",
      "64.69\n",
      "64.17\n",
      "64.93\n",
      "37.10\n",
      "34.29\n",
      "35.07\n",
      "2.93\n",
      "3.49\n",
      "3.59\n",
      "24.05\n",
      "23.01\n",
      "23.70\n",
      "Matryoshka Representation Learning-2048 dimensional model. This also showed that some in-\n",
      "stances and classes could benefit from lower-dimensional representations.\n",
      "Discussion of Oracle Accuracy\n",
      "Based on our observed model disagreements for different rep-\n",
      "resentation sizes d, we defined an optimal oracle accuracy [58] for MRL. We labeled an image as\n",
      "correctly predicted if classification using any representation size was correct. The percentage of\n",
      "total samples of ImageNet-1K that were firstly correctly predicted using each representation size d is\n",
      "shown in Table 22. This defined an upper bound on the performance of MRL models, as 18.46%\n",
      "of the ImageNet-1K validation set were incorrectly pre\\nnts and screenshots, if\n",
      "applicable? [N/A]\n",
      "(b) Did you describe any potential participant risks, with links to Institutional Review\n",
      "Board (IRB) approvals, if applicable? [N/A]\n",
      "(c) Did you include the estimated hourly wage paid to participants and the total amount\n",
      "spent on participant compensation? [N/A]\n",
      "17\n",
      "Contents\n",
      "1\n",
      "Introduction\n",
      "1\n",
      "2\n",
      "Related Work\n",
      "3\n",
      "3\n",
      "Matryoshka Representation Learning\n",
      "3\n",
      "4\n",
      "Applications\n",
      "4\n",
      "4.1\n",
      "Representation Learning\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "4\n",
      "4.2\n",
      "Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "5\n",
      "4.2.1\n",
      "Adaptive Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "6\n",
      "4.3\n",
      "Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "6\n",
      "4.3.1\n",
      "Adaptive Retrieva\\ndim. Similarly, Figure 3\n",
      "showcases the comparison of learned representation quality through 1-NN accuracy on ImageNet-1K\n",
      "(trainset with 1.3M samples as the database and validation set with 50K samples as the queries).\n",
      "Matryoshka Representations are up to 2% more accurate than their fixed-feature counterparts for\n",
      "the lower-dimensions while being as accurate elsewhere. 1-NN accuracy is an excellent proxy, at no\n",
      "additional training cost, to gauge the utility of learned representations in the downstream tasks.\n",
      "We also evaluate the quality of the representations from training ViT-B/16 on JFT-300M alongside the\n",
      "ViT-B/16 vision encoder of the ALIGN model – two web-scale setups. Due to the expensive nature of\n",
      "these experiments, we only train the highest capacity fixed feature model and choose rand\\nectation using the distribution of representation\n",
      "sizes. As shown in Table 3 and Figure 6, we observed that in expectation, we only needed a ∼37\n",
      "sized representation to achieve 76.3% classification accuracy on ImageNet-1K, which was roughly\n",
      "14× smaller than the FF–512 baseline. Even if we computed the expectation as a weighted average\n",
      "over the cumulative sum of representation sizes {8, 24, 56, . . .}, due to the nature of multiple linear\n",
      "heads for MRL, we ended up with an expected size of 62 that still provided a roughly 8.2× efficient\n",
      "representation than the FF–512 baseline. However, MRL–E alleviates this extra compute with a\n",
      "minimal drop in accuracy.\n",
      "D.2\n",
      "JFT, ALIGN and BERT\n",
      "We examine the k-NN classification accuracy of learned Matryoshka Representations via\n",
      "ALIGN–MRL and JFT-ViT–MRL in \\n retrieval (Section 4.3.1). Finally, as MRL explicitly\n",
      "learns coarse-to-fine representation vectors, intuitively it should share more semantic information\n",
      "among its various dimensions (Figure 5). This is reflected in up to 2% accuracy gains in long-tail\n",
      "continual learning settings while being as robust as the original embeddings. Furthermore, due to its\n",
      "coarse-to-fine grained nature, MRL can also be used as method to analyze hardness of classification\n",
      "among instances and information bottlenecks.\n",
      "We make the following key contributions:\n",
      "1. We introduce\n",
      "Matryoshka Representation Learning (MRL) to obtain flexible representa-\n",
      "tions (Matryoshka Representations) for adaptive deployment (Section 3).\n",
      "2. Up to 14× faster yet accurate large-scale classification and retrieval using MRL (Section 4).\n",
      "3\\nshows that MRL also\n",
      "improves the cosine similarity span between positive and random image-text pairs.\n",
      "Few-shot and Long-tail Learning.\n",
      "We exhaustively evaluated few-shot learning on MRL models\n",
      "using nearest class mean [79]. Table 15 in Appendix G shows that that representations learned\n",
      "through MRL perform comparably to FF representations across varying shots and number of classes.\n",
      "Matryoshka Representations realize a unique pattern while evaluating on FLUID [92], a long-tail\n",
      "sequential learning framework. We observed that MRL provides up to 2% accuracy higher on novel\n",
      "classes in the tail of the distribution, without sacrificing accuracy on other classes (Table 16 in\n",
      "Appendix G). Additionally we find the accuracy between low-dimensional and high-dimensional\n",
      "representations is marginal for p\\n8\n",
      "FF\n",
      "86.40\n",
      "37.09\n",
      "71.74\n",
      "10.77\n",
      "37.04\n",
      "52.67\n",
      "MRL\n",
      "85.60\n",
      "36.83\n",
      "70.34\n",
      "12.88\n",
      "37.46\n",
      "52.18\n",
      "MRL–E\n",
      "83.01\n",
      "29.99\n",
      "65.37\n",
      "7.60\n",
      "31.97\n",
      "47.16\n",
      "Table 17: Top-1 classification accuracy (%) on out-of-domain datasets (ImageNet-V2/R/A/Sketch) to\n",
      "examine robustness of Matryoshka Representation Learning. Note that these results are without\n",
      "any fine tuning on these datasets.\n",
      "ImageNet-V1\n",
      "ImageNet-V2\n",
      "ImageNet-R\n",
      "ImageNet-A\n",
      "ImageNet-Sketch\n",
      "Rep. Size\n",
      "FF\n",
      "MRL–E\n",
      "MRL\n",
      "FF\n",
      "MRL–E\n",
      "MRL\n",
      "FF\n",
      "MRL–E\n",
      "MRL\n",
      "FF\n",
      "MRL–E\n",
      "MRL\n",
      "FF\n",
      "MRL–E\n",
      "MRL\n",
      "8\n",
      "65.86\n",
      "56.92\n",
      "67.46\n",
      "54.05\n",
      "47.40\n",
      "55.59\n",
      "24.60\n",
      "22.98\n",
      "23.57\n",
      "2.92\n",
      "3.63\n",
      "3.39\n",
      "17.73\n",
      "15.07\n",
      "17.98\n",
      "16\n",
      "73.10\n",
      "72.38\n",
      "73.80\n",
      "60.52\n",
      "60.48\n",
      "61.71\n",
      "28.51\n",
      "28.45\n",
      "28.85\n",
      "3.00\n",
      "3.55\n",
      "3.59\n",
      "21.70\n",
      "20.38\n",
      "21.77\n",
      "32\n",
      "74.68\n",
      "74.80\n",
      "75.26\n",
      "62.24\n",
      "62.23\n",
      "63.05\n",
      "31.28\n",
      "30.79\n",
      "31.47\n",
      "2.60\n",
      "3.65\n",
      "3.57\n",
      "22.03\n",
      "21.87\n",
      "22.48\n",
      "64\n",
      "75.45\n",
      "75.48\n",
      "76.17\n",
      "63.51\n",
      "63.15\n",
      "63.99\n",
      "32.96\n",
      "\\nall 1,000 ImageNet-1K classes.\n",
      "ObjectNet [2] contains 50K images across 313 object classes, each containing ∼160 images each.\n",
      "C\n",
      "Matryoshka Representation Learning Model Training\n",
      "We trained all ResNet50–MRL models using the efficient dataloaders of FFCV [56]. We utilized the\n",
      "rn50_40_epochs.yaml configuration file of FFCV to train all MRL models defined below:\n",
      "• MRL: ResNet50 model with the fc layer replaced by MRL_Linear_Layer(efficient=False)\n",
      "• MRL–E: ResNet50 model with the fc layer replaced by MRL_Linear_Layer(efficient=True)\n",
      "• FF–k: ResNet50 model with the fc layer replaced by torch.nn.Linear(k, num_classes),\n",
      "where k ∈[8, 16, 32, 64, 128, 256, 512, 1024, 2048]. We will henceforth refer to these models as\n",
      "simply FF, with the k value denoting representation size.\n",
      "We trained all ResNet50 m\\necognition, pages\n",
      "11162–11173, 2021.\n",
      "[19] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional\n",
      "transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n",
      "[20] T. G. Dietterich and G. Bakiri. Solving multiclass learning problems via error-correcting\n",
      "output codes. Journal of artificial intelligence research, 2:263–286, 1994.\n",
      "[21] S. K. Divvala, A. Farhadi, and C. Guestrin. Learning everything about anything: Webly-\n",
      "supervised visual concept learning. In Proceedings of the IEEE Conference on Computer\n",
      "Vision and Pattern Recognition, pages 3270–3277, 2014.\n",
      "[22] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. De-\n",
      "hghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transfor\\nyers can be found\n",
      "in the ResNet50 architecture [29]. All models were finetuned with the FFCV pipeline, with same\n",
      "training configuration as in the end-to-end training aside from changing lr = 0.1 and epochs = 10. We\n",
      "observed that finetuning the linear layer alone was insufficient to learn Matryoshka Representations\n",
      "at lower dimensionalities. Adding more and more non-linear conv+ReLU layers steadily improved\n",
      "classification accuracy of d = 8 from 5% to 60% after finetuning, which was only 6% less than\n",
      "training MRL end-to-end for 40 epochs. This difference was successively less pronounced as we\n",
      "increased dimensionality past d = 64, to within 1.5% for all larger dimensionalities. The full results\n",
      "of this ablation can be seen in Table 26.\n",
      "Relative Importance.\n",
      "We performed an ablation of MRL over\\nclear\n",
      "trend. When we repeated this experiment with independently trained FF models, we noticed that 950\n",
      "classes did not show a clear trend. This motivated us to leverage the disagreement as well as gradual\n",
      "improvement of accuracy at different representation sizes by training Matryoshka Representations.\n",
      "Figure 12 showcases the progression of relative per-class accuracy distribution compared to the\n",
      "29\n",
      "Table 16: Accuracy (%) categories indicates whether classes were present during ImageNet pretraining\n",
      "and head/tail indicates classes that have greater/less than 50 examples in the streaming test set. We\n",
      "observed that MRL performed better than the baseline on novel tail classes by ∼2% on average.\n",
      "Rep. Size\n",
      "Method\n",
      "Pretrain\n",
      "- Head (>50)\n",
      "Novel\n",
      "- Head (>50)\n",
      "Pretrain\n",
      "- Tail (<50)\n",
      "Novel\n",
      "- Tail (<50)\n",
      "M\\ns.\n",
      "Table 25 quantifies the performance with different representation size.\n",
      "K\n",
      "Ablation Studies\n",
      "K.1\n",
      "MRL Training Paradigm\n",
      "Matryoshka Representations via Finetuning.\n",
      "To observe if nesting can be induced in models that\n",
      "were not explicitly trained with nesting from scratch, we loaded a pretrained FF-2048 ResNet50 model\n",
      "and initialized a new MRL layer, as defined in Algorithm 2, Appendix C. We then unfroze different\n",
      "layers of the backbone to observe how much non-linearity in the form of unfrozen conv layers needed\n",
      "to be present to enforce nesting into a pretrained FF model. A description of these layers can be found\n",
      "in the ResNet50 architecture [29]. All models were finetuned with the FFCV pipeline, with same\n",
      "training configuration as in the end-to-end training aside from changing lr = 0.1 and e\\nasun, A. Torralba, and S. Fidler. Aligning\n",
      "books and movies: Towards story-like visual explanations by watching movies and reading\n",
      "books. In Proceedings of the IEEE international conference on computer vision, pages 19–27,\n",
      "2015.\n",
      "16\n",
      "Checklist\n",
      "1. For all authors...\n",
      "(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s\n",
      "contributions and scope? [Yes]\n",
      "(b) Did you describe the limitations of your work? [Yes] See Section 6\n",
      "(c) Did you discuss any potential negative societal impacts of your work? [N/A] Our work\n",
      "does not have any additional negative societal impact on top of the existing impact of\n",
      "representation learning. However, a study on the trade-off between representation size\n",
      "and the tendency to encode biases is an interesting future direction along the \\n68\n",
      "70.14\n",
      "69.54\n",
      "69.01\n",
      "68.41\n",
      "2048\n",
      "70.98\n",
      "65.20\n",
      "63.57\n",
      "62.56\n",
      "61.60\n",
      "70.18\n",
      "69.52\n",
      "68.98\n",
      "68.35\n",
      "1024\n",
      "2048\n",
      "1312\n",
      "70.97\n",
      "65.20\n",
      "63.57\n",
      "62.56\n",
      "61.60\n",
      "70.18\n",
      "69.52\n",
      "68.98\n",
      "68.35\n",
      "These results provide further evidence that different tasks require varying capacity based on their\n",
      "difficulty.\n",
      "H\n",
      "Robustness Experiments\n",
      "We evaluated the robustness of MRL models on out-of-domain datasets (ImageNetV2/R/A/Sketch)\n",
      "and compared them to the FF baseline. Each of these datasets is described in Appendix B. The\n",
      "results in Table 17 demonstrate that learning Matryoshka Representations does not hurt out-of-\n",
      "domain generalization relative to FF models, and Matryoshka Representations in fact improve\n",
      "the performance on ImageNet-A. For a ALIGN–MRL model, we examine the the robustness via\n",
      "zero-shot retrieval on out-of-domain datasets, i\\nties without the\n",
      "additional expense of multiple model forward passes for the web-scale databases. FF models\n",
      "also generate independent databases which become prohibitively expense to store and switch in\n",
      "between. Matryoshka Representations enable adaptive retrieval (AR) which alleviates the need\n",
      "to use full-capacity representations, d = 2048, for all data and downstream tasks. Lastly, all the\n",
      "vector compression techniques [60, 45] used as part of the ANNS pipelines are complimentary to\n",
      "Matryoshka Representations and can further improve the efficiency-vs-accuracy trade-off.\n",
      "4.3.1\n",
      "Adaptive Retrieval\n",
      "We benchmark MRL in the adaptive retrieval setting (AR) [50]. For a given query image, we obtained\n",
      "a shortlist, K = 200, of images from the database using a lower-dimensional representation, e.g.\n",
      "D\\n, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,\n",
      "A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International\n",
      "journal of computer vision, 115(3):211–252, 2015.\n",
      "[77] R. Salakhutdinov and G. Hinton. Learning a nonlinear embedding by preserving class neigh-\n",
      "bourhood structure. In Artificial Intelligence and Statistics, pages 412–419. PMLR, 2007.\n",
      "[78] R. Salakhutdinov and G. Hinton. Semantic hashing. International Journal of Approximate\n",
      "Reasoning, 50(7):969–978, 2009.\n",
      "[79] J. S. Sánchez, F. Pla, and F. J. Ferri. On the use of neighbourhood-based non-parametric\n",
      "classifiers. Pattern Recognition Letters, 18(11-13):1179–1186, 1997.\n",
      "[80] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-cam:\n",
      "Visual explanation\\nar(k, num_classes),\n",
      "where k ∈[8, 16, 32, 64, 128, 256, 512, 1024, 2048]. We will henceforth refer to these models as\n",
      "simply FF, with the k value denoting representation size.\n",
      "We trained all ResNet50 models with a learning rate of 0.475 with a cyclic learning rate schedule [83].\n",
      "This was after appropriate scaling (0.25×) of the learning rate specified in the configuration file to\n",
      "accommodate for 2xA100 NVIDIA GPUs available for training, compared to the 8xA100 GPUs\n",
      "utilized in the FFCV benchmarks. We trained with a batch size of 256 per GPU, momentum [86] of\n",
      "0.9, and an SGD optimizer with a weight decay of 1e-4.\n",
      "Our code (Appendix A) makes minimal modifications to the training pipeline provided by FFCV to\n",
      "learn Matryoshka Representations.\n",
      "We trained ViT-B/16 models for JFT-300M on a 8x8 clo\\nsentation size for both top-1 and mAP@10, and especially\n",
      "at low representation size (Ds ≤32). MRL–E loses out to FF significantly only at Ds = 8. This\n",
      "indicates that training ResNet50 models via the MRL training paradigm improves retrieval at low\n",
      "representation size over models explicitly trained at those representation size (FF-8...2048).\n",
      "We carried out all retrieval experiments at Ds ∈{8, 16, 32, 64, 128, 256, 512, 1024, 2048}, as\n",
      "these were the representation sizes which were a part of the nesting_list at which losses\n",
      "were added during training, as seen in Algorithm 1, Appendix A. To examine whether MRL\n",
      "is able to learn Matryoshka Representations at dimensions in between the representation size\n",
      "for which it was trained, we also tabulate the performance of MRL at interpolated Ds ∈\n",
      "{12, 2\\nce of MRL model on 31-way classification (1 extra class is for reject token) on\n",
      "ImageNet-1K superclasses.\n",
      "Rep. Size\n",
      "8\n",
      "16\n",
      "32\n",
      "64\n",
      "128\n",
      "256\n",
      "512\n",
      "1024\n",
      "2048\n",
      "MRL\n",
      "85.57\n",
      "88.67\n",
      "89.48\n",
      "89.82\n",
      "89.97\n",
      "90.11\n",
      "90.18\n",
      "90.22\n",
      "90.21\n",
      "Matryoshka Representations at Arbitrary Granularities.\n",
      "To train MRL, we used nested di-\n",
      "mensions at logarithmic granularities M = {8, 16, . . . , 1024, 2048} as detailed in Section 3. We\n",
      "made this choice for two empirically-driven reasons: a) The accuracy improvement with increasing\n",
      "representation size was more logarithmic than linear (as shown by FF models in Figure 2). This indi-\n",
      "cated that optimizing for granularities increasing in a non-logarithmic fashion would be sub-optimal\n",
      "both for maximum performance and expected efficiency; b) If we have m arbitrary granularities,\n",
      "the expected\\nat optimizing for granularities increasing in a non-logarithmic fashion would be sub-optimal\n",
      "both for maximum performance and expected efficiency; b) If we have m arbitrary granularities,\n",
      "the expected cost of the linear classifier to train MRL scales as O(L ∗(m2)) while logarithmic\n",
      "granularities result in O(L ∗2log(d)) space and compute costs.\n",
      "To demonstrate this effect, we learned Matryoshka Representations with uniform (MRL-Uniform)\n",
      "nesting dimensions m\n",
      "∈\n",
      "M\n",
      "=\n",
      "{8, 212, 416, 620, 824, 1028, 1232, 1436, 1640, 1844, 2048}.\n",
      "We\n",
      "evaluated\n",
      "this\n",
      "model\n",
      "at\n",
      "the\n",
      "standard\n",
      "(MRL-log)\n",
      "dimensions\n",
      "m\n",
      "∈\n",
      "M\n",
      "=\n",
      "{8, 16, 32, 64, 128, 256, 512, 1024, 2048} for ease of comparison to reported numbers using 1-NN ac-\n",
      "curacy (%). As shown in Table 29, we observed that while performance interpolated, MRL-Uniform\n",
      "suffered\\nable 7: Masked Language Modelling (MLM) accuracy(%) of FF and MRL models on the validation\n",
      "set.\n",
      "Rep. Size\n",
      "BERT-FF\n",
      "BERT-MRL\n",
      "12\n",
      "60.12\n",
      "59.92\n",
      "24\n",
      "62.49\n",
      "62.05\n",
      "48\n",
      "63.85\n",
      "63.40\n",
      "96\n",
      "64.32\n",
      "64.15\n",
      "192\n",
      "64.70\n",
      "64.58\n",
      "384\n",
      "65.03\n",
      "64.81\n",
      "768\n",
      "65.54\n",
      "65.00\n",
      "size and the interpolated representation size as we increase Ds, which demonstrates that MRL is\n",
      "able to learn Matryoshka Representations at nearly all representation size m ∈[8, 2048] despite\n",
      "optimizing only for |M| nested representation sizes.\n",
      "We examined the robustness of MRL for retrieval on out-of-domain datasets ImageNetV2 and\n",
      "ImageNet-4K, as shown in Table 9 and Table 10 respectively. On ImageNetV2, we observed that MRL\n",
      "outperformed FF at all Ds on top-1 Accuracy and mAP@10, and MRL–E outperformed FF at all\n",
      "Ds except Ds = 8. This demonstrates the robustness\\nnd deployment.\n",
      "MRL learns coarse-to-fine representations that are at least as accurate and rich as\n",
      "independently trained low-dimensional representations. The flexibility within the\n",
      "learned Matryoshka Representations offer: (a) up to 14× smaller embedding\n",
      "size for ImageNet-1K classification at the same level of accuracy; (b) up to 14×\n",
      "real-world speed-ups for large-scale retrieval on ImageNet-1K and 4K; and (c) up\n",
      "to 2% accuracy improvements for long-tail few-shot classification, all while being\n",
      "as robust as the original representations. Finally, we show that MRL extends seam-\n",
      "lessly to web-scale datasets (ImageNet, JFT) across various modalities – vision\n",
      "(ViT, ResNet), vision + language (ALIGN) and language (BERT). MRL code and\n",
      "pretrained models are open-sourced at https://github.com/RAIVN\\nd Table 10 respectively. On ImageNetV2, we observed that MRL\n",
      "outperformed FF at all Ds on top-1 Accuracy and mAP@10, and MRL–E outperformed FF at all\n",
      "Ds except Ds = 8. This demonstrates the robustness of the learned Matryoshka Representations\n",
      "for out-of-domain image retrieval.\n",
      "F\n",
      "Adaptive Retrieval\n",
      "The time complexity of retrieving a shortlist of k-NN often scales as O(d), where d =Ds, for a\n",
      "fixed k and N. We thus will have a theoretical 256× higher cost for Ds = 2048 over Ds = 8. We\n",
      "discuss search complexity in more detail in Appendix I. In an attempt to replicate performance at\n",
      "higher Ds while using less FLOPs, we perform adaptive retrieval via retrieving a k-NN shortlist with\n",
      "representation size Ds, and then re-ranking the shortlist with representations of size Dr. Adaptive\n",
      "retrieval for\\n improve efficiency for large-scale classification and retrieval\n",
      "without any significant loss of accuracy. While there are potentially several applications of coarse-to-\n",
      "fine Matryoshka Representations, in this work we focus on two key building blocks of real-world\n",
      "ML systems: large-scale classification and retrieval. For classification, we use adaptive cascades with\n",
      "the variable-size representations from a model trained with MRL, significantly reducing the average\n",
      "dimension of embeddings needed to achieve a particular accuracy. For example, on ImageNet-1K,\n",
      "MRL + adaptive classification results in up to a 14× smaller representation size at the same accuracy\n",
      "as baselines (Section 4.2.1). Similarly, we use MRL in an adaptive retrieval system. Given a query,\n",
      "we shortlist retrieval candidates \\nt). Every combination of Ds & Dr falls above the Pareto\n",
      "line (orange dots) of single-shot retrieval with a fixed representation size while having configurations\n",
      "that are as accurate while being up to 14× faster in real-world deployment. Funnel retrieval is almost\n",
      "as accurate as the baseline while alleviating some of the parameter choices of Adaptive Retrieval.\n",
      "point, further strengthening the use-case for Matryoshka Representation Learning and adaptive\n",
      "retrieval.\n",
      "Even with adaptive retrieval, it is hard to determine the choice of Ds & Dr. In order to alleviate this\n",
      "issue to an extent, we propose Funnel Retrieval, a consistent cascade for adaptive retrieval. Funnel\n",
      "thins out the initial shortlist by a repeated re-ranking and shortlisting with a series of increasing\n",
      "capacity representations.\\nion size for MRL &\n",
      "FF models showing the capture of underlying\n",
      "hierarchy through tight information bottlenecks.\n",
      "8\n",
      "16\n",
      "32\n",
      "64\n",
      "128\n",
      "256\n",
      "512\n",
      "1024\n",
      "2048\n",
      "Representation Size\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "Top-1 Accuracy (%)\n",
      "measuring device\n",
      "building\n",
      "garment\n",
      "tool\n",
      "nourishment\n",
      "protective covering\n",
      "vessel\n",
      "oscine\n",
      "Figure 11:\n",
      "Diverse per-superclass accuracy\n",
      "trends across representation sizes for ResNet50-\n",
      "MRL on ImageNet-1K.\n",
      "9\n",
      "occurs with both MRL and FF models; MRL is more accurate across dimensions. This shows that\n",
      "tight information bottlenecks while not highly accurate for fine-grained classification, do capture\n",
      "required semantic information for coarser classification that could be leveraged for adaptive routing\n",
      "for retrieval and classification. Mutifidelity of Matryoshka Representation naturally captures the\n",
      "und\\nYang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig.\n",
      "Scaling up visual and vision-language representation learning with noisy text supervision. In\n",
      "International Conference on Machine Learning, pages 4904–4916. PMLR, 2021.\n",
      "[47] J. Johnson, M. Douze, and H. Jégou. Billion-scale similarity search with GPUs. IEEE\n",
      "Transactions on Big Data, 7(3):535–547, 2019.\n",
      "[48] W. B. Johnson. Extensions of lipschitz mappings into a hilbert space. Contemp. Math., 26:\n",
      "189–206, 1984.\n",
      "[49] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, S. Bates, S. Bhatia,\n",
      "N. Boden, A. Borchers, et al. In-datacenter performance analysis of a tensor processing unit.\n",
      "In Proceedings of the 44th annual international symposium on computer architecture, pages\n",
      "1–12, 2017.\n",
      "[50] T.\\n accuracy with increase in capacity. However, we observed that\n",
      "8\n",
      "(a)\n",
      "(b)\n",
      "(c)\n",
      "Figure 9: Grad-CAM [80] progression of predictions in MRL model across 8, 16, 32 and 2048\n",
      "dimensions. (a) 8-dimensional representation confuses due to presence of other relevant objects (with\n",
      "a larger field of view) in the scene and predicts “shower cap” ; (b) 8-dim model confuses within\n",
      "the same super-class of “boa” ; (c) 8 and 16-dim models incorrectly focus on the eyes of the doll\n",
      "(\"sunglasses\") and not the \"sweatshirt\" which is correctly in focus at higher dimensions; MRL fails\n",
      "gracefully in these scenarios and shows potential use cases of disagreement across dimensions.\n",
      "this trend was not ubiquitous and certain instances and classes were more accurate when evaluated\n",
      "with lower-dimensions (Figure 12 in Appendi\\nConference on Machine Learning, pages 5389–5400. PMLR,\n",
      "2019.\n",
      "[73] O. Rippel, M. Gelbart, and R. Adams. Learning ordered representations with nested dropout.\n",
      "In International Conference on Machine Learning, pages 1746–1754. PMLR, 2014.\n",
      "[74] J. Rissanen. Modeling by shortest data description. Automatica, 14(5):465–471, 1978.\n",
      "[75] S. Ruder, M. E. Peters, S. Swayamdipta, and T. Wolf. Transfer learning in natural language\n",
      "processing. In Proceedings of the 2019 conference of the North American chapter of the\n",
      "association for computational linguistics: Tutorials, pages 15–18, 2019.\n",
      "[76] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,\n",
      "A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International\n",
      "journal of computer vision, 115\\nows potential use cases of disagreement across dimensions.\n",
      "this trend was not ubiquitous and certain instances and classes were more accurate when evaluated\n",
      "with lower-dimensions (Figure 12 in Appendix J). With perfect routing of instances to appropriate\n",
      "dimension, MRL can gain up to 4.6% classification accuracy. At the same time, the low-dimensional\n",
      "models are less accurate either due to confusion within the same superclass [24] of the ImageNet\n",
      "hierarchy or presence of multiple objects of interest. Figure 9 showcases 2 such examples for 8-\n",
      "dimensional representation. These results along with Appendix J put forward the potential for MRL\n",
      "to be a systematic framework for analyzing the utility and efficiency of information bottlenecks.\n",
      "Superclass Accuracy.\n",
      "As the information bottleneck become\\nch\n",
      "using hierarchical navigable small world graphs. IEEE transactions on pattern analysis and\n",
      "machine intelligence, 42(4):824–836, 2018.\n",
      "[63] J. Masci, U. Meier, D. Cire¸san, and J. Schmidhuber. Stacked convolutional auto-encoders for\n",
      "hierarchical feature extraction. In International conference on artificial neural networks, pages\n",
      "52–59. Springer, 2011.\n",
      "[64] P. Mitra, C. Murthy, and S. K. Pal. Unsupervised feature selection using feature similarity.\n",
      "IEEE transactions on pattern analysis and machine intelligence, 24(3):301–312, 2002.\n",
      "[65] V. Nanda, T. Speicher, J. P. Dickerson, S. Feizi, K. P. Gummadi, and A. Weller. Diffused\n",
      "redundancy in pre-trained representations. arXiv preprint arXiv:2306.00183, 2023.\n",
      "[66] P. Nayak. Understanding searches better than ever before. Google AI Blog, 2019. \\nny\n",
      "representation size. The remaining 81.54% constitutes the oracle accuracy.\n",
      "Rep. Size\n",
      "8\n",
      "16\n",
      "32\n",
      "64\n",
      "128\n",
      "256\n",
      "512\n",
      "1024\n",
      "2048\n",
      "Always\n",
      "Wrong\n",
      "Correctly\n",
      "Predicted\n",
      "67.46\n",
      "8.78\n",
      "2.58\n",
      "1.35\n",
      "0.64\n",
      "0.31\n",
      "0.20\n",
      "0.12\n",
      "0.06\n",
      "18.46\n",
      "of disagreement arising when the models got confused within the same superclass. For example,\n",
      "ImageNet-1K has multiple \"snake\" classes, and models often confuse a snake image for an incorrect\n",
      "species of snake.\n",
      "Superclass Performance\n",
      "We created a 30 superclass subset of the validation set based on wordnet\n",
      "hierarchy (Table 24) to quantify the performance of MRL model on ImageNet-1K superclasses.\n",
      "Table 25 quantifies the performance with different representation size.\n",
      "K\n",
      "Ablation Studies\n",
      "K.1\n",
      "MRL Training Paradigm\n",
      "Matryoshka Representations via Finetuning.\n",
      "To observe if nesting can be induced \\n Ds = 64 on ImageNet-1K and ImageNet-4K, at 32× less\n",
      "MFLOPs. This demonstrates the value of intelligent routing techniques which utilize appropriately\n",
      "sized Matryoshka Representations for retrieval.\n",
      "24\n",
      "Table 8: Retrieve a shortlist of 200-NN with Ds sized representations on ImageNet-1K via exact\n",
      "search with L2 distance metric. Top-1 and mAP@10 entries (%) where MRL–E and MRL outperform\n",
      "FF at their respective representation sizes are bolded.\n",
      "Model\n",
      "Ds\n",
      "MFlops\n",
      "Top-1\n",
      "Top-5\n",
      "Top-10\n",
      "mAP@10\n",
      "mAP@25\n",
      "mAP@50\n",
      "mAP@100\n",
      "P@10\n",
      "P@25\n",
      "P@50\n",
      "P@100\n",
      "FF\n",
      "8\n",
      "10\n",
      "58.93\n",
      "75.76\n",
      "80.25\n",
      "53.42\n",
      "52.29\n",
      "51.84\n",
      "51.57\n",
      "59.32\n",
      "59.28\n",
      "59.25\n",
      "59.21\n",
      "16\n",
      "20\n",
      "66.77\n",
      "80.88\n",
      "84.40\n",
      "61.63\n",
      "60.51\n",
      "59.98\n",
      "59.62\n",
      "66.76\n",
      "66.58\n",
      "66.43\n",
      "66.27\n",
      "32\n",
      "41\n",
      "68.84\n",
      "82.58\n",
      "86.14\n",
      "63.35\n",
      "62.08\n",
      "61.36\n",
      "60.76\n",
      "68.43\n",
      "68.13\n",
      "67.83\n",
      "67.48\n",
      "64\n",
      "82\n",
      "69.41\n",
      "83.56\n",
      "87.33\n",
      "63.26\n",
      "61.64\n",
      "60.63\n",
      "59.67\n",
      "68.4\\nines significantly, which indicates that\n",
      "pretrained models lack the multifidelity of Matryoshka Representations and are incapable of fitting\n",
      "an accurate linear classifier at low representation sizes.\n",
      "We compared the performance of MRL models at various representation sizes via 1-nearest neighbors\n",
      "(1-NN) image classification accuracy on ImageNet-1K in Table 2 and Figure 3. We provide detailed\n",
      "information regarding the k-NN search pipeline in Appendix E. We compared against a baseline\n",
      "of attempting to enforce nesting to a FF-2048 model by 1) Random Feature Selection (Rand. FS):\n",
      "considering the first m dimensions of FF-2048 for NN lookup, and 2) FF+SVD: performing SVD\n",
      "on the FF-2048 representations at the specified representation size, 3) FF+JL: performing random\n",
      "projection according to the J\\nre 5: Despite optimizing MRL only for\n",
      "O(log(d)) dimensions for ResNet50 and ViT-\n",
      "B/16 models; the accuracy in the intermediate\n",
      "dimensions shows interpolating behaviour.\n",
      "4.2.1\n",
      "Adaptive Classification\n",
      "The flexibility and coarse-to-fine granularity within Matryoshka Representations allows model\n",
      "cascades [90] for Adaptive Classification (AC) [28]. Unlike standard model cascades [95], MRL does\n",
      "not require multiple expensive neural network forward passes. To perform AC with an MRL trained\n",
      "model, we learn thresholds on the maximum softmax probability [33] for each nested classifier on\n",
      "a holdout validation set. We then use these thresholds to decide when to transition to the higher\n",
      "dimensional representation (e.g 8 →16 →32) of the MRL model. Appendix D.1 discusses the\n",
      "implementation and learning o\\nGummadi, and A. Weller. Diffused\n",
      "redundancy in pre-trained representations. arXiv preprint arXiv:2306.00183, 2023.\n",
      "[66] P. Nayak. Understanding searches better than ever before. Google AI Blog, 2019. URL https:\n",
      "//blog.google/products/search/search-language-understanding-bert/.\n",
      "[67] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\n",
      "N. Gimelshein, L. Antiga, et al.\n",
      "Pytorch: An imperative style, high-performance deep\n",
      "learning library. Advances in neural information processing systems, 32, 2019.\n",
      "[68] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer.\n",
      "Deep contextualized word representations. In Proceedings of the 2018 Conference of the\n",
      "North American Chapter of the Association for Computational Linguistics: Human Language\n",
      "Te\\ns, 27, 2014.\n",
      "[99] H.-F. Yu, K. Zhong, J. Zhang, W.-C. Chang, and I. S. Dhillon. Pecos: Prediction for enormous\n",
      "and correlated output spaces. Journal of Machine Learning Research, 23(98):1–32, 2022.\n",
      "[100] J. Yu, L. Yang, N. Xu, J. Yang, and T. Huang. Slimmable neural networks. arXiv preprint\n",
      "arXiv:1812.08928, 2018.\n",
      "[101] R. Zellers, J. Lu, X. Lu, Y. Yu, Y. Zhao, M. Salehi, A. Kusupati, J. Hessel, A. Farhadi, and\n",
      "Y. Choi. Merlot reserve: Neural script knowledge through vision and language and sound.\n",
      "arXiv preprint arXiv:2201.02639, 2022.\n",
      "[102] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning\n",
      "books and movies: Towards story-like visual explanations by watching movies and reading\n",
      "books. In Proceedings of the IEEE international conference on compute\\n-accuracy\n",
      "trade-off\n",
      "for\n",
      "adaptive\n",
      "retrieval\n",
      "using\n",
      "Matryoshka Representations compared to single-shot using fixed features with ResNet50\n",
      "on ImageNet-1K. We observed that all AR settings lied above the Pareto frontier of single-shot\n",
      "retrieval with varying representation sizes. In particular for ImageNet-1K, we show that the AR\n",
      "model with Ds = 16 & Dr = 2048 is as accurate as single-shot retrieval with d = 2048 while being\n",
      "∼128× more efficient in theory and ∼14× faster in practice (compared using HNSW on the same\n",
      "hardware). We show similar trends with ImageNet-4K, but note that we require Ds = 64 given\n",
      "the increased difficulty of the dataset. This results in ∼32× and ∼6× theoretical and in-practice\n",
      "speedups respectively. Lastly, while K = 200 works well for our adaptive retrieval experiments, \\nithin the same larger network. However, the weights for each progressively smaller\n",
      "network can be different and often require distinct forward passes to isolate the final representations.\n",
      "This is detrimental for adaptive inference due to the need for re-encoding the entire retrieval database\n",
      "with expensive sub-net forward passes of varying capacities. Several works [23, 26, 65, 59] investigate\n",
      "the notions of intrinsic dimensionality and redundancy of representations and objective spaces pointing\n",
      "to minimum description length [74]. Finally, ordered representations proposed by Rippel et al. [73]\n",
      "use nested dropout in the context of autoencoders to learn nested representations. MRL differentiates\n",
      "itself in formulation by optimizing only for O(log(d)) nesting dimensions instead of O(d). Despit\\n) utilization\n",
      "of the representation for downstream applications [50, 89]. Compute costs for the latter part of the\n",
      "pipeline scale with the embedding dimensionality as well as the data size (N) and label space (L).\n",
      "At web-scale [15, 85] this utilization cost overshadows the feature computation cost. The rigidity in\n",
      "these representations forces the use of high-dimensional embedding vectors across multiple tasks\n",
      "despite the varying resource and accuracy constraints that require flexibility.\n",
      "Human perception of the natural world has a naturally coarse-to-fine granularity [28, 32]. However,\n",
      "perhaps due to the inductive bias of gradient-based training [84], deep learning models tend to diffuse\n",
      "“information” across the entire representation vector. The desired elasticity is usually enabled in the\\nt MRL can be used for large-scale adaptive\n",
      "classification as well as adaptive retrieval. On standard benchmarks, MRL matches the accuracy of\n",
      "the fixed-feature baseline despite using 14× smaller representation size on average. Furthermore, the\n",
      "Matryoshka Representation based adaptive shortlisting and re-ranking system ensures comparable\n",
      "mAP@10 to the baseline while being 128× cheaper in FLOPs and 14× faster in wall-clock time.\n",
      "Finally, most of the efficiency techniques for model inference and vector search are complementary\n",
      "to MRL\n",
      "further assisting in deployment at the compute-extreme environments.\n",
      "Acknowledgments\n",
      "We are grateful to Srinadh Bhojanapalli, Lovish Madaan, Raghav Somani, Ludwig Schmidt, and\n",
      "Venkata Sailesh Sanampudi for helpful discussions and feedback. Aditya Kusupati also tha\\n5736\n",
      "0.6060\n",
      "1.2781\n",
      "2.7047\n",
      "HNSW32\n",
      "0.1193\n",
      "0.1455\n",
      "0.1833\n",
      "0.2145\n",
      "0.2333\n",
      "0.2670\n",
      "observation on the expected dimensionality for 76.30% top-1 classification accuracy being just\n",
      "d ∼37. We leave the design and learning of a more optimal policy for future work.\n",
      "Grad-CAM Examples\n",
      "We analyzed the nature of model disagreement across representation\n",
      "sizes with MRL models with the help of Grad-CAM visualization [80]. We observed there were\n",
      "certain classes in ImageNet-1K such as \"tools\", \"vegetables\" and \"meat cutting knife\" which were\n",
      "occasionally located around multiple objects and a cluttered environment. In such scenarios, we\n",
      "observed that smaller representation size models would often get confused due to other objects and fail\n",
      "to extract the object of interest which generated the correct label. We als\\n on top of the existing impact of\n",
      "representation learning. However, a study on the trade-off between representation size\n",
      "and the tendency to encode biases is an interesting future direction along the lines of\n",
      "existing literature [36, 37]. A part of this is already presented in Section 5.\n",
      "(d) Have you read the ethics review guidelines and ensured that your paper conforms to\n",
      "them? [Yes]\n",
      "2. If you are including theoretical results...\n",
      "(a) Did you state the full set of assumptions of all theoretical results? [N/A]\n",
      "(b) Did you include complete proofs of all theoretical results? [N/A]\n",
      "3. If you ran experiments...\n",
      "(a) Did you include the code, data, and instructions needed to reproduce the main ex-\n",
      "perimental results (either in the supplemental material or as a URL)? [Yes] See sup-\n",
      "plemental mater\\n2020.\n",
      "[70] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understand-\n",
      "ing by generative pre-training. OpenAI Blog, 2018. URL https://openai.com/blog/\n",
      "language-unsupervised/.\n",
      "14\n",
      "[71] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\n",
      "P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language su-\n",
      "pervision. In International Conference on Machine Learning, pages 8748–8763. PMLR,\n",
      "2021.\n",
      "[72] B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to\n",
      "imagenet? In International Conference on Machine Learning, pages 5389–5400. PMLR,\n",
      "2019.\n",
      "[73] O. Rippel, M. Gelbart, and R. Adams. Learning ordered representations with nested dropout.\n",
      "In International Conference on Machine Lear\\nrXiv preprint arXiv:1911.05248, 2019.\n",
      "[37] S. Hooker, N. Moorosi, G. Clark, S. Bengio, and E. Denton. Characterising bias in compressed\n",
      "models. arXiv preprint arXiv:2010.03058, 2020.\n",
      "[38] H. Hotelling. Analysis of a complex of statistical variables into principal components. Journal\n",
      "of educational psychology, 24(6):417, 1933.\n",
      "[39] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and\n",
      "H. Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications.\n",
      "arXiv preprint arXiv:1704.04861, 2017.\n",
      "[40] J. Howard and S. Ruder. Universal language model fine-tuning for text classification. arXiv\n",
      "preprint arXiv:1801.06146, 2018.\n",
      "[41] H. Hu, D. Dey, M. Hebert, and J. A. Bagnell. Learning anytime predictions in neural networks\n",
      "via adaptive loss bal\\nrs.\n",
      "We also found that for both MRL and FF, as the shot number decreased, the required representa-\n",
      "tion size to reach optimal accuracy decreased (Table 15). For example, we observed that 1-shot\n",
      "performance at 32 representation size had equal accuracy to 2048 representation size.\n",
      "FLUID.\n",
      "For the long-tailed setting we evaluated MRL on the FLUID benchmark [92] which\n",
      "contains a mixture of pretrain and new classes. Table 16 shows the evaluation of the learned\n",
      "representation on FLUID. We observed that MRL provided up to 2% higher accuracy on novel\n",
      "classes in the tail of the distribution, without sacrificing accuracy on other classes. Additionally we\n",
      "found the accuracy between low-dimensional and high-dimensional representations was marginal for\n",
      "pretrain classes. For example, the 64-dimensional M\\n= {12, 24, 48, 96, 192, 384, 768} as\n",
      "the explicitly optimized nested dimensions respectively. Lastly, we extensively compare the MRL\n",
      "and MRL–E models to independently trained low-dimensional (fixed feature) representations (FF),\n",
      "dimensionality reduction (SVD), sub-net method (slimmable networks [100]) and randomly selected\n",
      "features of the highest capacity FF model.\n",
      "In section 4.2, we evaluate the quality and capacity of the learned representations through linear\n",
      "classification/probe (LP) and 1-nearest neighbour (1-NN) accuracy. Experiments show that MRL\n",
      "models remove the dependence on |M| resource-intensive independently trained models for the\n",
      "coarse-to-fine representations while being as accurate. Lastly, we show that despite optimizing only\n",
      "for |M| dimensions, MRL models diffuse the info\\ning 1.8B image-text pairs.\n",
      "ImageNet Robustness Datasets\n",
      "We experimented on the following datasets to examine the robust-\n",
      "ness of MRL models:\n",
      "ImageNetV2 [72] is a collection of 10K images sampled a decade after the original construction of\n",
      "ImageNet [16]. ImageNetV2 contains 10 examples each from the 1,000 classes of ImageNet-1K.\n",
      "ImageNet-A [35] contains 7.5K real-world adversarially filtered images from 200 ImageNet-\n",
      "1K classes.\n",
      "ImageNet-R [34] contains 30K artistic image renditions for 200 of the original ImageNet-1K classes.\n",
      "ImageNet-Sketch [94] contains 50K sketches, evenly distributed over all 1,000 ImageNet-1K classes.\n",
      "ObjectNet [2] contains 50K images across 313 object classes, each containing ∼160 images each.\n",
      "C\n",
      "Matryoshka Representation Learning Model Training\n",
      "We trained all ResNet5\\nViT-B/16 vision\n",
      "encoder and BERT language encoder on ALIGN data [46] and (c) Masked language modelling:\n",
      "BERT [19] on English Wikipedia and BooksCorpus [102]. Please refer to Appendices B and C for\n",
      "details regarding the model architectures, datasets and training specifics.\n",
      "4\n",
      "8\n",
      "16\n",
      "32\n",
      "64\n",
      "128\n",
      "256\n",
      "512\n",
      "1024\n",
      "2048\n",
      "Representation Size\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "Top-1 Accuracy (%)\n",
      "MRL\n",
      "MRL-E\n",
      "FF\n",
      "SVD\n",
      "Slim. Net\n",
      "Rand. LP\n",
      "Figure 2: ImageNet-1K linear classification ac-\n",
      "curacy of ResNet50 models. MRL is as accurate\n",
      "as the independently trained FF models for every\n",
      "representation size.\n",
      "8\n",
      "16\n",
      "32\n",
      "64\n",
      "128\n",
      "256\n",
      "512\n",
      "1024\n",
      "2048\n",
      "Representation Size\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "1-NN Accuracy (%)\n",
      "MRL\n",
      "MRL-E\n",
      "FF\n",
      "SVD\n",
      "Slim. Net\n",
      "Rand. FS\n",
      "Figure 3:\n",
      "ImageNet-1K 1-NN accuracy of\n",
      "ResNet50 models measuring the representation\n",
      "quality for downstream task. MRL ou\\nV. Vanhoucke,\n",
      "V. Vasudevan, F. Viégas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and\n",
      "X. Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL\n",
      "https://www.tensorflow.org/. Software available from tensorflow.org.\n",
      "[2] A. Barbu, D. Mayo, J. Alverio, W. Luo, C. Wang, D. Gutfreund, J. Tenenbaum, and B. Katz.\n",
      "Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition\n",
      "models. Advances in neural information processing systems, 32, 2019.\n",
      "[3] S. Bengio, J. Weston, and D. Grangier. Label embedding trees for large multi-class tasks.\n",
      "Advances in Neural Information Processing Systems, 23, 2010.\n",
      "[4] Y. Bengio. Deep learning of representations for unsupervised and transfer learning. In\n",
      "Proceedings of ICML workshop on unsupervised\\nn artificial intelligence and statistics, pages 297–304. JMLR Workshop and Conference\n",
      "Proceedings, 2010.\n",
      "[28] M. G. Harris and C. D. Giachritsis. Coarse-grained information dominates fine-grained\n",
      "information in judgments of time-to-contact from retinal flow. Vision research, 40(6):601–611,\n",
      "2000.\n",
      "[29] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In\n",
      "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–\n",
      "778, 2016.\n",
      "[30] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual\n",
      "representation learning. In Proceedings of the IEEE/CVF conference on computer vision and\n",
      "pattern recognition, pages 9729–9738, 2020.\n",
      "[31] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick. Masked autoencoders\\nle datasets (ImageNet, JFT) across various modalities – vision\n",
      "(ViT, ResNet), vision + language (ALIGN) and language (BERT). MRL code and\n",
      "pretrained models are open-sourced at https://github.com/RAIVNLab/MRL.\n",
      "1\n",
      "Introduction\n",
      "Learned representations [57] are fundamental building blocks of real-world ML systems [66, 91].\n",
      "Trained once and frozen, d-dimensional representations encode rich information and can be used\n",
      "to perform multiple downstream tasks [4]. The deployment of deep representations has two steps:\n",
      "(1) an expensive yet constant-cost forward pass to compute the representation [29] and (2) utilization\n",
      "of the representation for downstream applications [50, 89]. Compute costs for the latter part of the\n",
      "pipeline scale with the embedding dimensionality as well as the data size (N) and lab\\n. Yang, and S. Kumar. Pre-training tasks for embedding-\n",
      "based large-scale retrieval. arXiv preprint arXiv:2002.03932, 2020.\n",
      "[11] W.-C. Chang, D. Jiang, H.-F. Yu, C. H. Teo, J. Zhang, K. Zhong, K. Kolluri, Q. Hu,\n",
      "N. Shandilya, V. Ievgrafov, et al. Extreme multi-label learning for semantic matching in\n",
      "product search. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discov-\n",
      "ery & Data Mining, pages 2643–2651, 2021.\n",
      "[12] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning\n",
      "of visual representations. In International conference on machine learning, pages 1597–1607.\n",
      "PMLR, 2020.\n",
      "[13] Y. Chen, Z. Liu, H. Xu, T. Darrell, and X. Wang. Meta-baseline: exploring simple meta-\n",
      "learning for few-shot learning. In Proceedings of the IEEE/CVF Internationa\\ne, and L. Zettlemoyer.\n",
      "Deep contextualized word representations. In Proceedings of the 2018 Conference of the\n",
      "North American Chapter of the Association for Computational Linguistics: Human Language\n",
      "Technologies, Volume 1 (Long Papers), pages 2227–2237, New Orleans, Louisiana, June\n",
      "2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1202. URL https:\n",
      "//aclanthology.org/N18-1202.\n",
      "[69] Y. Prabhu, A. Kusupati, N. Gupta, and M. Varma. Extreme regression for dynamic search\n",
      "advertising. In Proceedings of the 13th International Conference on Web Search and Data\n",
      "Mining, pages 456–464, 2020.\n",
      "[70] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understand-\n",
      "ing by generative pre-training. OpenAI Blog, 2018. URL https://openai.com/blog/\n",
      "language-unsupervise\\n results in Section 5.1 reveal interesting weaknesses of MRL that would be logical directions\n",
      "for future work. (1) Optimizing the weightings of the nested losses to obtain a Pareto optimal\n",
      "accuracy-vs-efficiency trade-off – a potential solution could emerge from adaptive loss balancing\n",
      "aspects of anytime neural networks [41]. (2) Using different losses at various fidelities aimed at\n",
      "solving a specific aspect of adaptive deployment – e.g. high recall for 8-dimension and robustness\n",
      "for 2048-dimension. (3) Learning a search data-structure, like differentiable k-d tree, on top of\n",
      "Matryoshka Representation to enable dataset and representation aware retrieval. (4) Finally, the\n",
      "joint optimization of multi-objective MRL combined with end-to-end learnable search data-structure\n",
      "to have data-driven a\\nrmance of ResNet50 representations on ImageNet-1K across\n",
      "dimensionalities for MRL, MRL–E, FF, slimmable networks along with post-hoc compression\n",
      "of vectors using SVD and random feature selection. Matryoshka Representations are often the\n",
      "most accurate while being up to 3% better than the FF baselines. Similar to classification, post-hoc\n",
      "compression and slimmable network baselines suffer from significant drop-off in retrieval mAP@10\n",
      "with ≤256 dimensions. Appendix E discusses the mAP@10 of the same models on ImageNet-4K.\n",
      "MRL models are capable of performing accurate retrieval at various granularities without the\n",
      "additional expense of multiple model forward passes for the web-scale databases. FF models\n",
      "also generate independent databases which become prohibitively expense to store and switch i\\nlg 1 and 2 provided below to train supervised ResNet50–MRL models on ImageNet-1K.\n",
      "We provide this code as a template to extend MRL to any domain.\n",
      "Algorithm 1 Pytorch code for Matryoshka Cross-Entropy Loss\n",
      "class Matryoshka_CE_Loss(nn.Module):\n",
      "def __init__(self, relative_importance, **kwargs):\n",
      "super(Matryoshka_CE_Loss, self).__init__()\n",
      "self.criterion = nn.CrossEntropyLoss(**kwargs)\n",
      "self.relative_importance = relative_importance # usually set\n",
      "to all ones\n",
      "def forward(self, output, target):\n",
      "loss=0\n",
      "for i in range(len(output)):\n",
      "loss+= self.relative_importance[i] * self.criterion(output[\n",
      "i], target)\n",
      "return loss\n",
      "Algorithm 2 Pytorch code for MRL Linear Layer\n",
      "class MRL_Linear_Layer(nn.Module):\n",
      "def __init__(self, nesting_list: List, num_classes=1000, efficient=\n",
      "False, **kwargs):\n",
      "super(MRL_Linear_Layer\\n\n"
     ]
    }
   ],
   "source": [
    "context_prompt = \"\"\n",
    "for (score, text) in context:\n",
    "    context_prompt += (text + \"\\\\n\")\n",
    "print(context_prompt)\n",
    "\n",
    "p_sys_ddic = pt.system_prompt(context_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Context:\n",
      "oshka Representation Learning (MRL) to induce flexibility in the learned\n",
      "representation. MRL learns representations of varying capacities within the same high-dimensional\n",
      "vector through explicit optimization of O(log(d)) lower-dimensional vectors in a nested fashion,\n",
      "hence the name Matryoshka. MRL can be adapted to any existing representation pipeline and\n",
      "is easily extended to many standard tasks in computer vision and natural language processing.\n",
      "Figure 1 illustrates the core idea of Matryoshka Representation Learning (MRL) and the adaptive\n",
      "deployment settings of the learned Matryoshka Representations.\n",
      "Adaptive Retrieval\n",
      "Shortlisting\n",
      "Re-ranking\n",
      "Adaptive Classification\n",
      "Training\n",
      "Inference\n",
      "<latexit sha1_base64=\"eh9hk+peBkdsPY6v+r4rONmxYLY=\">A\n",
      "B7nicbVBNSwMxEJ2tX7V+VT16CRbBU9kVoR6LXjxWsB/QLiWb\\n, and signif-\n",
      "icantly for lower ones. This demonstrates that training to learn Matryoshka Representations\n",
      "is feasible and extendable even for extremely large scale datasets.\n",
      "We also demonstrate that\n",
      "Matryoshka Representations are learned at interpolated dimensions for both ALIGN and JFT-\n",
      "ViT, as shown in Table 5, despite not being trained explicitly at these dimensions. Lastly, Table 6\n",
      "shows that MRL training leads to a increase in the cosine similarity span between positive and\n",
      "random image-text pairs.\n",
      "We also evaluated the capability of Matryoshka Representations to extend to other natural language\n",
      "processing via masked language modeling (MLM) with BERT [19], whose results are tabulated\n",
      "in Table 7. Without any hyper-parameter tuning, we observed Matryoshka Representations to be\n",
      "within 0.\\nib+53USE1x4KZdxYpiks0NBIrCJcFYB7nPFqBFjSwhV3GbFdEgUocYWVbQluPNfXiTN06rVN3bs3LtMq+jAIdwBVw4RxqcA1aAFB\n",
      "c/wCm/oEb2gd/QxG1C+c4B/AH6/AGZEJHn</latexit>\n",
      "Figure\n",
      "1:\n",
      "Matryoshka Representation Learning\n",
      "is\n",
      "adaptable to any representation learning setup and begets\n",
      "a Matryoshka Representation z\n",
      "by optimizing the orig-\n",
      "inal\n",
      "loss\n",
      "L(.)\n",
      "at\n",
      "O(log(d))\n",
      "chosen\n",
      "representation\n",
      "sizes.\n",
      "Matryoshka Representation can be utilized effectively for adap-\n",
      "tive deployment across environments and downstream tasks.\n",
      "The first m-dimensions, m ∈[d], of\n",
      "the Matryoshka Representation is\n",
      "an information-rich low-dimensional\n",
      "vector, at no additional training cost,\n",
      "that is as accurate as an indepen-\n",
      "dently trained m-dimensional repre-\n",
      "sentation.\n",
      "The information within\n",
      "the Matryoshka Representation in-\n",
      "creases with the dimensionality \\nerse set of ap-\n",
      "plications along with an extensive evaluation of the learned multifidelity representations. Further,\n",
      "we showcase the downstream applications of the learned Matryoshka Representations for flexible\n",
      "large-scale deployment through (a) Adaptive Classification (AC) and (b) Adaptive Retrieval (AR).\n",
      "4.1\n",
      "Representation Learning\n",
      "We adapt Matryoshka Representation Learning (MRL) to various representation learning setups\n",
      "(a) Supervised learning for vision: ResNet50 [29] on ImageNet-1K [76] and ViT-B/16 [22] on\n",
      "JFT-300M [85], (b) Contrastive learning for vision + language: ALIGN model with ViT-B/16 vision\n",
      "encoder and BERT language encoder on ALIGN data [46] and (c) Masked language modelling:\n",
      "BERT [19] on English Wikipedia and BooksCorpus [102]. Please refer to Appendices B and C for\n",
      "det\\n weight-tying across all the linear classifiers, i.e., by defining W(m) = W1:m\n",
      "for a set of common weights W ∈RL×d. This would reduce the memory cost due to the linear\n",
      "classifiers by almost half, which would be crucial in cases of extremely large output spaces [89, 99].\n",
      "This variant is called Efficient Matryoshka Representation Learning (MRL–E). Refer to Alg 1\n",
      "and Alg 2 in Appendix A for the building blocks of Matryoshka Representation Learning (MRL).\n",
      "Adaptation to Learning Frameworks.\n",
      "MRL can be adapted seamlessly to most representation\n",
      "learning frameworks at web-scale with minimal modifications (Section 4.1). For example, MRL’s\n",
      "adaptation to masked language modelling reduces to MRL–E due to the weight-tying between the\n",
      "input embedding matrix and the linear classifier. For contrastive lea\\nons (Section 4.1). For example, MRL’s\n",
      "adaptation to masked language modelling reduces to MRL–E due to the weight-tying between the\n",
      "input embedding matrix and the linear classifier. For contrastive learning, both in context of vision &\n",
      "vision + language, MRL is applied to both the embeddings that are being contrasted with each other.\n",
      "The presence of normalization on the representation needs to be handled independently for each of\n",
      "the nesting dimension for best results (see Appendix C for more details).\n",
      "4\n",
      "Applications\n",
      "In this section, we discuss Matryoshka Representation Learning (MRL) for a diverse set of ap-\n",
      "plications along with an extensive evaluation of the learned multifidelity representations. Further,\n",
      "we showcase the downstream applications of the learned Matryoshka Representations f\\nMatryoshka Representation Learning\n",
      "Aditya Kusupati∗†⋄, Gantavya Bhatt∗†, Aniket Rege∗†,\n",
      "Matthew Wallingford†, Aditya Sinha⋄, Vivek Ramanujan†, William Howard-Snyder†,\n",
      "Kaifeng Chen⋄, Sham Kakade‡, Prateek Jain⋄and Ali Farhadi†\n",
      "†University of Washington, ⋄Google Research, ‡Harvard University\n",
      "{kusupati,ali}@cs.washington.edu, prajain@google.com\n",
      "Abstract\n",
      "Learned representations are a central component in modern ML systems, serv-\n",
      "ing a multitude of downstream tasks. When training such representations, it\n",
      "is often the case that computational and statistical constraints for each down-\n",
      "stream task are unknown. In this context, rigid fixed-capacity representations\n",
      "can be either over or under-accommodating to the task at hand. This leads us\n",
      "to ask: can we design a flexible representation that can ad\\nguage applications are built [40] on large language models [8] that are pretrained [68, 75]\n",
      "in a un/self-supervised fashion with masked language modelling [19] or autoregressive training [70].\n",
      "Matryoshka Representation Learning (MRL) is complementary to all these setups and can be\n",
      "adapted with minimal overhead (Section 3). MRL equips representations with multifidelity at no\n",
      "additional cost which enables adaptive deployment based on the data and task (Section 4).\n",
      "Efficient Classification and Retrieval.\n",
      "Efficiency in classification and retrieval during inference\n",
      "can be studied with respect to the high yet constant deep featurization costs or the search cost which\n",
      "scales with the size of the label space and data. Efficient neural networks address the first issue\n",
      "through a variety of algorithm\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\n",
      "arXiv:2205.13147v4  [cs.LG]  8 Feb 2024\n",
      "due to training/maintenance overhead, numerous expensive forward passes through all of the data,\n",
      "storage and memory cost for multiple copies of encoded data, expensive on-the-fly feature selection\n",
      "or a significant drop in accuracy. By encoding coarse-to-fine-grained representations, which are as\n",
      "accurate as the independently trained counterparts, we learn with minimal overhead a representation\n",
      "that can be deployed adaptively at no additional cost during inference.\n",
      "We introduce\n",
      "Matryoshka Representation Learning (MRL) to induce flexibility in the learned\n",
      "representation. MRL learns representations of varying capacities within the same high-dimensional\n",
      "vector through explicit optim\\nP7tBRJKQCk04VqDnFi7KZaEU7HpW6iaIzJEPdpx1CBQ6rcdJ/DPeNEsBeJM0TGk7UnxspDpUahb6ZzNKqWS8T/M6ie6duSkTcaKpINDvYRDHcGsDBgwSYnmI0MwkcxkhWSAJSbaVFYyJaDZL/8lzaMqcqro5rhcu\n",
      "8jrKIJdsAcqAIFTUANXoA4agIAH8ARewKv1aD1b9b7dLRg5Tvb4Besj2/eCZSw</latexit>\n",
      "<latexit sha1_base64=\"OPHM4ACsGr0VI7qMpDgoN+t2ICI=\">AB9XicbVDLSgMx\n",
      "FL3xWeur6tJNsAh1U2ZE0GXRjQsXFewD2rFk0kwbmskMSUapQ/DjQtF3Pov7vwbM+0stPVA4HDOvdyT48eCa+M432hpeWV1b2wUdzc2t7ZLe3tN3WUKMoaNBKRavtEM8Elaxh\n",
      "uBGvHipHQF6zlj64yv/XAlOaRvDPjmHkhGUgecEqMle67ITHDQJERvqk8nfRKZafqTIEXiZuTMuSo90pf3X5Ek5BJQwXRuM6sfFSogyngk2K3USzmNARGbCOpZKETHvpNPUEH1\n",
      "ulj4NI2ScNnq/N1ISaj0OfTuZpdTzXib+53USE1x4KZdxYpiks0NBIrCJcFYB7nPFqBFjSwhV3GbFdEgUocYWVbQluPNfXiTN06rVN3bs3LtMq+jAIdwBVw4RxqcA1aAFB\n",
      "c/wCm/oEb2gd/QxG1C+c4B/AH6/AGZEJHn</latexit>\n",
      "Figure\n",
      "1:\n",
      "Matryoshka Representation Learning\n",
      "is\n",
      "adapta\\nscuss the design choices in Section 4 for each of the representation learning settings.\n",
      "For the ease of exposition, we present the formulation for fully supervised representation learning\n",
      "via multi-class classification. Matryoshka Representation Learning modifies the typical setting\n",
      "to become a multi-scale representation learning problem on the same task. For example, we train\n",
      "ResNet50 [29] on ImageNet-1K [76] which embeds a 224 × 224 pixel image into a d = 2048\n",
      "representation vector and then passed through a linear classifier to make a prediction, ˆy among the\n",
      "L = 1000 labels. For MRL, we choose M = {8, 16, . . . , 1024, 2048} as the nesting dimensions.\n",
      "Suppose we are given a labelled dataset D = {(x1, y1), . . . , (xN, yN)} where xi ∈X is an input\n",
      "point and yi ∈[L] is the label of xi for\\n unknown. In this context, rigid fixed-capacity representations\n",
      "can be either over or under-accommodating to the task at hand. This leads us\n",
      "to ask: can we design a flexible representation that can adapt to multiple down-\n",
      "stream tasks with varying computational resources? Our main contribution is\n",
      "Matryoshka Representation Learning (MRL) which encodes information at\n",
      "different granularities and allows a single embedding to adapt to the computational\n",
      "constraints of downstream tasks. MRL minimally modifies existing representation\n",
      "learning pipelines and imposes no additional cost during inference and deployment.\n",
      "MRL learns coarse-to-fine representations that are at least as accurate and rich as\n",
      "independently trained low-dimensional representations. The flexibility within the\n",
      "learned Matryoshka \\nleviates this extra compute with a\n",
      "minimal drop in accuracy.\n",
      "D.2\n",
      "JFT, ALIGN and BERT\n",
      "We examine the k-NN classification accuracy of learned Matryoshka Representations via\n",
      "ALIGN–MRL and JFT-ViT–MRL in Table 4.\n",
      "For ALIGN [46], we observed that learning\n",
      "Matryoshka Representations via ALIGN–MRL improved classification accuracy at nearly all\n",
      "dimensions when compared to ALIGN. We observed a similar trend when training ViT-B/16 [22]\n",
      "for JFT-300M [85] classification, where learning Matryoshka Representations via MRL and\n",
      "MRL–E on top of JFT-ViT improved classification accuracy for nearly all dimensions, and signif-\n",
      "icantly for lower ones. This demonstrates that training to learn Matryoshka Representations\n",
      "is feasible and extendable even for extremely large scale datasets.\n",
      "We also demonstrate that\n",
      "M\\nf general purpose representations for computer vision [4, 98]. These representations\n",
      "are typically learned through supervised and un/self-supervised learning paradigms. Supervised\n",
      "pretraining [29, 51, 82] casts representation learning as a multi-class/label classification problem,\n",
      "while un/self-supervised learning learns representation via proxy tasks like instance classification [97]\n",
      "and reconstruction [31, 63]. Recent advances [12, 30] in contrastive learning [27] enabled learning\n",
      "from web-scale data [21] that powers large-capacity cross-modal models [18, 46, 71, 101]. Similarly,\n",
      "natural language applications are built [40] on large language models [8] that are pretrained [68, 75]\n",
      "in a un/self-supervised fashion with masked language modelling [19] or autoregressive training [70].\n",
      "Matryos\\nno additional training cost,\n",
      "that is as accurate as an indepen-\n",
      "dently trained m-dimensional repre-\n",
      "sentation.\n",
      "The information within\n",
      "the Matryoshka Representation in-\n",
      "creases with the dimensionality creat-\n",
      "ing a coarse-to-fine grained represen-\n",
      "tation, all without significant training\n",
      "or additional deployment overhead.\n",
      "MRL equips the representation vector\n",
      "with the desired flexibility and multi-\n",
      "fidelity that can ensure a near-optimal\n",
      "accuracy-vs-compute trade-off. With\n",
      "these advantages, MRL enables adap-\n",
      "tive deployment based on accuracy\n",
      "and compute constraints.\n",
      "The Matryoshka Representations improve efficiency for large-scale classification and retrieval\n",
      "without any significant loss of accuracy. While there are potentially several applications of coarse-to-\n",
      "fine Matryoshka Representation\\n are as accu-\n",
      "rate as independently trained counterparts without the multiple expensive forward passes.\n",
      "Matryoshka Representations provide an intermediate abstraction between high-dimensional vec-\n",
      "tors and their efficient ANNS indices through the adaptive embeddings nested within the original\n",
      "representation vector (Section 4). All other aforementioned efficiency techniques are complementary\n",
      "and can be readily applied to the learned Matryoshka Representations obtained from MRL.\n",
      "Several works in efficient neural network literature [9, 93, 100] aim at packing neural networks of\n",
      "varying capacity within the same larger network. However, the weights for each progressively smaller\n",
      "network can be different and often require distinct forward passes to isolate the final representations.\n",
      "This is detr\\ne whether MRL\n",
      "is able to learn Matryoshka Representations at dimensions in between the representation size\n",
      "for which it was trained, we also tabulate the performance of MRL at interpolated Ds ∈\n",
      "{12, 24, 48, 96, 192, 384, 768, 1536} as MRL–Interpolated and MRL–E–Interpolated (see Table 8).\n",
      "We observed that performance scaled nearly monotonically between the original representation\n",
      "23\n",
      "Table 6: Cosine similarity between embeddings\n",
      "Avg. Cosine Similarity\n",
      "ALIGN\n",
      "ALIGN-MRL\n",
      "Positive Text to Image\n",
      "0.27\n",
      "0.49\n",
      "Random Text to Image\n",
      "8e-3\n",
      "-4e-03\n",
      "Random Image to Image\n",
      "0.10\n",
      "0.08\n",
      "Random Text to Text\n",
      "0.22\n",
      "0.07\n",
      "Table 7: Masked Language Modelling (MLM) accuracy(%) of FF and MRL models on the validation\n",
      "set.\n",
      "Rep. Size\n",
      "BERT-FF\n",
      "BERT-MRL\n",
      "12\n",
      "60.12\n",
      "59.92\n",
      "24\n",
      "62.49\n",
      "62.05\n",
      "48\n",
      "63.85\n",
      "63.40\n",
      "96\n",
      "64.32\n",
      "64.15\n",
      "192\n",
      "64.70\n",
      "64.58\n",
      "3\\n\n",
      "use nested dropout in the context of autoencoders to learn nested representations. MRL differentiates\n",
      "itself in formulation by optimizing only for O(log(d)) nesting dimensions instead of O(d). Despite\n",
      "this, MRL diffuses information to intermediate dimensions interpolating between the optimized\n",
      "Matryoshka Representation sizes accurately (Figure 5); making web-scale feasible.\n",
      "3\n",
      "Matryoshka Representation Learning\n",
      "For d ∈N, consider a set M ⊂[d] of representation sizes. For a datapoint x in the input do-\n",
      "main X, our goal is to learn a d-dimensional representation vector z ∈Rd. For every m ∈M,\n",
      "3\n",
      "Matryoshka Representation Learning (MRL) enables each of the first m dimensions of the em-\n",
      "bedding vector, z1:m ∈Rm to be independently capable of being a transferable and general purpose\n",
      "representatio\\no obtain flexible representa-\n",
      "tions (Matryoshka Representations) for adaptive deployment (Section 3).\n",
      "2. Up to 14× faster yet accurate large-scale classification and retrieval using MRL (Section 4).\n",
      "3. Seamless adaptation of MRL across modalities (vision - ResNet & ViT, vision + language -\n",
      "ALIGN, language - BERT) and to web-scale data (ImageNet-1K/4K, JFT-300M and ALIGN data).\n",
      "4. Further analysis of MRL’s representations in the context of other downstream tasks (Section 5).\n",
      "2\n",
      "2\n",
      "Related Work\n",
      "Representation Learning.\n",
      "Large-scale datasets like ImageNet [16, 76] and JFT [85] enabled\n",
      "the learning of general purpose representations for computer vision [4, 98]. These representations\n",
      "are typically learned through supervised and un/self-supervised learning paradigms. Supervised\n",
      "pretraining [29, 51,\\natryoshka Representation Learning (MRL) enables each of the first m dimensions of the em-\n",
      "bedding vector, z1:m ∈Rm to be independently capable of being a transferable and general purpose\n",
      "representation of the datapoint x. We obtain z using a deep neural network F( · ; θF ): X →Rd\n",
      "parameterized by learnable weights θF , i.e., z := F(x; θF ). The multi-granularity is captured through\n",
      "the set of the chosen dimensions M, that contains less than log(d) elements, i.e., |M| ≤⌊log(d)⌋.\n",
      "The usual set M consists of consistent halving until the representation size hits a low information\n",
      "bottleneck. We discuss the design choices in Section 4 for each of the representation learning settings.\n",
      "For the ease of exposition, we present the formulation for fully supervised representation learning\n",
      "via multi-cl\\n2\n",
      "60.48\n",
      "61.71\n",
      "28.51\n",
      "28.45\n",
      "28.85\n",
      "3.00\n",
      "3.55\n",
      "3.59\n",
      "21.70\n",
      "20.38\n",
      "21.77\n",
      "32\n",
      "74.68\n",
      "74.80\n",
      "75.26\n",
      "62.24\n",
      "62.23\n",
      "63.05\n",
      "31.28\n",
      "30.79\n",
      "31.47\n",
      "2.60\n",
      "3.65\n",
      "3.57\n",
      "22.03\n",
      "21.87\n",
      "22.48\n",
      "64\n",
      "75.45\n",
      "75.48\n",
      "76.17\n",
      "63.51\n",
      "63.15\n",
      "63.99\n",
      "32.96\n",
      "32.13\n",
      "33.39\n",
      "2.87\n",
      "3.99\n",
      "3.76\n",
      "22.13\n",
      "22.56\n",
      "23.43\n",
      "128\n",
      "75.47\n",
      "76.05\n",
      "76.46\n",
      "63.67\n",
      "63.52\n",
      "64.69\n",
      "33.93\n",
      "33.48\n",
      "34.54\n",
      "2.81\n",
      "3.71\n",
      "3.73\n",
      "22.73\n",
      "22.73\n",
      "23.70\n",
      "256\n",
      "75.78\n",
      "76.31\n",
      "76.66\n",
      "64.13\n",
      "63.80\n",
      "64.71\n",
      "34.80\n",
      "33.91\n",
      "34.85\n",
      "2.77\n",
      "3.65\n",
      "3.60\n",
      "22.63\n",
      "22.88\n",
      "23.59\n",
      "512\n",
      "76.30\n",
      "76.48\n",
      "76.82\n",
      "64.11\n",
      "64.09\n",
      "64.78\n",
      "35.53\n",
      "34.20\n",
      "34.97\n",
      "2.37\n",
      "3.57\n",
      "3.59\n",
      "23.41\n",
      "22.89\n",
      "23.67\n",
      "1024\n",
      "76.74\n",
      "76.60\n",
      "76.93\n",
      "64.43\n",
      "64.20\n",
      "64.95\n",
      "36.06\n",
      "34.22\n",
      "34.99\n",
      "2.53\n",
      "3.56\n",
      "3.68\n",
      "23.44\n",
      "22.98\n",
      "23.72\n",
      "2048\n",
      "77.10\n",
      "76.65\n",
      "76.95\n",
      "64.69\n",
      "64.17\n",
      "64.93\n",
      "37.10\n",
      "34.29\n",
      "35.07\n",
      "2.93\n",
      "3.49\n",
      "3.59\n",
      "24.05\n",
      "23.01\n",
      "23.70\n",
      "Matryoshka Representation Learning-2048 dimensional model. This also showed that some in-\n",
      "stances \\n equipped with retrieval capabilities that can bring forward every\n",
      "instance [7]. Approximate Nearest Neighbor Search (ANNS) [42] makes it feasible with efficient\n",
      "indexing [14] and traversal [5, 6] to present the users with the most similar documents/images from\n",
      "the database for a requested query. Widely adopted HNSW [62] (O(d log(N))) is as accurate as\n",
      "exact retrieval (O(dN)) at the cost of a graph-based index overhead for RAM and disk [44].\n",
      "MRL tackles the linear dependence on embedding size,\n",
      "d,\n",
      "by learning multifidelity\n",
      "Matryoshka Representations.\n",
      "Lower-dimensional Matryoshka Representations are as accu-\n",
      "rate as independently trained counterparts without the multiple expensive forward passes.\n",
      "Matryoshka Representations provide an intermediate abstraction between high-dimensional vec-\n",
      "tor\\nrd)” had a clear visual distinction between the object and background and\n",
      "thus predictions using 8 dimensions also led to a good inter-class separability within the superclass.\n",
      "5.1\n",
      "Ablations\n",
      "Table 26 in Appendix K presents that Matryoshka Representations can be enabled within off-the-\n",
      "shelf pretrained models with inexpensive partial finetuning thus paving a way for ubiquitous adoption\n",
      "of MRL. At the same time, Table 27 in Appendix C indicates that with optimal weighting of the\n",
      "nested losses we could improve accuracy of lower-dimensions representations without accuracy\n",
      "loss. Tables 28 and 29 in Appendix C ablate over the choice of initial granularity and spacing of the\n",
      "granularites. Table 28 reaffirms the design choice to shun extremely low dimensions that have poor\n",
      "classification accuracy \\nght decay of 1e-4.\n",
      "Our code (Appendix A) makes minimal modifications to the training pipeline provided by FFCV to\n",
      "learn Matryoshka Representations.\n",
      "We trained ViT-B/16 models for JFT-300M on a 8x8 cloud TPU pod [49] using Tensorflow [1] with a\n",
      "batchsize of 128 and trained for 300K steps. Similarly, ALIGN models were trained using Tensorflow\n",
      "on 8x8 cloud TPU pod for 1M steps with a batchsize of 64 per TPU. Both these models were trained\n",
      "with adafactor optimizer [81] with a linear learning rate decay starting at 1e-3.\n",
      "Lastly, we trained a BERT-Base model on English Wikipedia and BookCorpus. We trained our models\n",
      "in Tensorflow using a 4x4 cloud TPU pod with a total batchsize of 1024. We used AdamW [61]\n",
      "optimizer with a linear learning rate decay starting at 1e-4 and trained for 450K steps.\n",
      "In\\nired semantic information for coarser classification that could be leveraged for adaptive routing\n",
      "for retrieval and classification. Mutifidelity of Matryoshka Representation naturally captures the\n",
      "underlying hierarchy of the class labels with one single model. Lastly, Figure 11 showcases the\n",
      "accuracy trends per superclass with MRL. The utility of additional dimensions in distinguishing\n",
      "a class from others within the same superclass is evident for “garment” which has up to 11%\n",
      "improvement for 8 →16 dimensional representation transition. We also observed that superclasses\n",
      "such as “oscine (songbird)” had a clear visual distinction between the object and background and\n",
      "thus predictions using 8 dimensions also led to a good inter-class separability within the superclass.\n",
      "5.1\n",
      "Ablations\n",
      "Table 26 \\nation (MRL–AC)\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "21\n",
      "D.2\n",
      "JFT, ALIGN and BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "22\n",
      "E\n",
      "Image Retrieval\n",
      "22\n",
      "F\n",
      "Adaptive Retrieval\n",
      "24\n",
      "G Few-shot and Sample Efficiency\n",
      "25\n",
      "H Robustness Experiments\n",
      "27\n",
      "I\n",
      "In Practice Costs\n",
      "27\n",
      "J\n",
      "Analysis of Model Disagreement\n",
      "29\n",
      "K Ablation Studies\n",
      "32\n",
      "K.1\n",
      "MRL Training Paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "32\n",
      "K.2\n",
      "Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "33\n",
      "18\n",
      "A\n",
      "Code for Matryoshka Representation Learning\n",
      "(MRL)\n",
      "We use Alg 1 and 2 provided below to train supervised ResNet50–MRL models on ImageNet-1K.\n",
      "We provide this code as a template to extend MRL to any domain.\n",
      "Algorithm 1 Pytorch code for Matryoshka Cross-Entropy \\ndaptive Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "6\n",
      "4.3\n",
      "Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "6\n",
      "4.3.1\n",
      "Adaptive Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "7\n",
      "5\n",
      "Further Analysis and Ablations\n",
      "8\n",
      "5.1\n",
      "Ablations\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "10\n",
      "6\n",
      "Discussion and Conclusions\n",
      "10\n",
      "A Code for Matryoshka Representation Learning\n",
      "(MRL)\n",
      "19\n",
      "B\n",
      "Datasets\n",
      "20\n",
      "C Matryoshka Representation Learning Model Training\n",
      "20\n",
      "D Classification Results\n",
      "21\n",
      "D.1 Adaptive Classification (MRL–AC)\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "21\n",
      "D.2\n",
      "JFT, ALIGN and BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "22\n",
      "E\n",
      "Image Retrieval\n",
      "22\n",
      "F\n",
      "Adaptive Retrieval\\nanguage\n",
      "processing via masked language modeling (MLM) with BERT [19], whose results are tabulated\n",
      "in Table 7. Without any hyper-parameter tuning, we observed Matryoshka Representations to be\n",
      "within 0.5% of FF representations for BERT MLM validation accuracy. This is a promising initial\n",
      "result that could help with large-scale adaptive document retrieval using BERT–MRL.\n",
      "E\n",
      "Image Retrieval\n",
      "We evaluated the strength of Matryoshka Representations via image retrieval on ImageNet-1K (the\n",
      "training distribution), as well as on out-of-domain datasets ImageNetV2 and ImageNet-4K for all\n",
      "22\n",
      "Table 4: ViT-B/16 and ViT-B/16-MRL top-1 and top-5 k-NN accuracy (%) for ALIGN and JFT. Top-1\n",
      "entries where MRL–E and MRL outperform baselines are bolded for both ALIGN and JFT-ViT.\n",
      "Rep. Size\n",
      "ALIGN\n",
      "ALIGN-MRL\n",
      "JFT-ViT\n",
      "\\nentation to enable dataset and representation aware retrieval. (4) Finally, the\n",
      "joint optimization of multi-objective MRL combined with end-to-end learnable search data-structure\n",
      "to have data-driven adaptive large-scale retrieval for web-scale search applications.\n",
      "In conclusion, we presented\n",
      "Matryoshka Representation Learning (MRL), a flexible represen-\n",
      "tation learning approach that encodes information at multiple granularities in a single embedding\n",
      "vector. This enables the MRL to adapt to a downstream task’s statistical complexity as well as\n",
      "the available compute resources. We demonstrate that MRL can be used for large-scale adaptive\n",
      "classification as well as adaptive retrieval. On standard benchmarks, MRL matches the accuracy of\n",
      "the fixed-feature baseline despite using 14× smaller repres\\n,\n",
      "(1)\n",
      "where L: RL × [L] →R+ is the multi-class softmax cross-entropy loss function. This is a standard\n",
      "optimization problem that can be solved using sub-gradient descent methods. We set all the impor-\n",
      "tance scales, cm = 1 for all m ∈M; see Section 5 for ablations. Lastly, despite only optimizing\n",
      "for O(log(d)) nested dimensions, MRL results in accurate representations, that interpolate, for\n",
      "dimensions that fall between the chosen granularity of the representations (Section 4.2).\n",
      "We call this formulation as Matryoshka Representation Learning (MRL). A natural way to make\n",
      "this efficient is through weight-tying across all the linear classifiers, i.e., by defining W(m) = W1:m\n",
      "for a set of common weights W ∈RL×d. This would reduce the memory cost due to the linear\n",
      "classifiers by almost half, whic\\ne distribution, without sacrificing accuracy on other classes (Table 16 in\n",
      "Appendix G). Additionally we find the accuracy between low-dimensional and high-dimensional\n",
      "representations is marginal for pretrain classes. We hypothesize that the higher-dimensional represen-\n",
      "tations are required to differentiate the classes when few training examples of each are known. This\n",
      "results provides further evidence that different tasks require varying capacity based on their difficulty.\n",
      "Disagreement across Dimensions.\n",
      "The information packing in Matryoshka Representations\n",
      "often results in gradual increase of accuracy with increase in capacity. However, we observed that\n",
      "8\n",
      "(a)\n",
      "(b)\n",
      "(c)\n",
      "Figure 9: Grad-CAM [80] progression of predictions in MRL model across 8, 16, 32 and 2048\n",
      "dimensions. (a) 8-dimensional rep\\n.98\n",
      "23.72\n",
      "2048\n",
      "77.10\n",
      "76.65\n",
      "76.95\n",
      "64.69\n",
      "64.17\n",
      "64.93\n",
      "37.10\n",
      "34.29\n",
      "35.07\n",
      "2.93\n",
      "3.49\n",
      "3.59\n",
      "24.05\n",
      "23.01\n",
      "23.70\n",
      "Matryoshka Representation Learning-2048 dimensional model. This also showed that some in-\n",
      "stances and classes could benefit from lower-dimensional representations.\n",
      "Discussion of Oracle Accuracy\n",
      "Based on our observed model disagreements for different rep-\n",
      "resentation sizes d, we defined an optimal oracle accuracy [58] for MRL. We labeled an image as\n",
      "correctly predicted if classification using any representation size was correct. The percentage of\n",
      "total samples of ImageNet-1K that were firstly correctly predicted using each representation size d is\n",
      "shown in Table 22. This defined an upper bound on the performance of MRL models, as 18.46%\n",
      "of the ImageNet-1K validation set were incorrectly pre\\nnts and screenshots, if\n",
      "applicable? [N/A]\n",
      "(b) Did you describe any potential participant risks, with links to Institutional Review\n",
      "Board (IRB) approvals, if applicable? [N/A]\n",
      "(c) Did you include the estimated hourly wage paid to participants and the total amount\n",
      "spent on participant compensation? [N/A]\n",
      "17\n",
      "Contents\n",
      "1\n",
      "Introduction\n",
      "1\n",
      "2\n",
      "Related Work\n",
      "3\n",
      "3\n",
      "Matryoshka Representation Learning\n",
      "3\n",
      "4\n",
      "Applications\n",
      "4\n",
      "4.1\n",
      "Representation Learning\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "4\n",
      "4.2\n",
      "Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "5\n",
      "4.2.1\n",
      "Adaptive Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "6\n",
      "4.3\n",
      "Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "6\n",
      "4.3.1\n",
      "Adaptive Retrieva\\ndim. Similarly, Figure 3\n",
      "showcases the comparison of learned representation quality through 1-NN accuracy on ImageNet-1K\n",
      "(trainset with 1.3M samples as the database and validation set with 50K samples as the queries).\n",
      "Matryoshka Representations are up to 2% more accurate than their fixed-feature counterparts for\n",
      "the lower-dimensions while being as accurate elsewhere. 1-NN accuracy is an excellent proxy, at no\n",
      "additional training cost, to gauge the utility of learned representations in the downstream tasks.\n",
      "We also evaluate the quality of the representations from training ViT-B/16 on JFT-300M alongside the\n",
      "ViT-B/16 vision encoder of the ALIGN model – two web-scale setups. Due to the expensive nature of\n",
      "these experiments, we only train the highest capacity fixed feature model and choose rand\\nectation using the distribution of representation\n",
      "sizes. As shown in Table 3 and Figure 6, we observed that in expectation, we only needed a ∼37\n",
      "sized representation to achieve 76.3% classification accuracy on ImageNet-1K, which was roughly\n",
      "14× smaller than the FF–512 baseline. Even if we computed the expectation as a weighted average\n",
      "over the cumulative sum of representation sizes {8, 24, 56, . . .}, due to the nature of multiple linear\n",
      "heads for MRL, we ended up with an expected size of 62 that still provided a roughly 8.2× efficient\n",
      "representation than the FF–512 baseline. However, MRL–E alleviates this extra compute with a\n",
      "minimal drop in accuracy.\n",
      "D.2\n",
      "JFT, ALIGN and BERT\n",
      "We examine the k-NN classification accuracy of learned Matryoshka Representations via\n",
      "ALIGN–MRL and JFT-ViT–MRL in \\n retrieval (Section 4.3.1). Finally, as MRL explicitly\n",
      "learns coarse-to-fine representation vectors, intuitively it should share more semantic information\n",
      "among its various dimensions (Figure 5). This is reflected in up to 2% accuracy gains in long-tail\n",
      "continual learning settings while being as robust as the original embeddings. Furthermore, due to its\n",
      "coarse-to-fine grained nature, MRL can also be used as method to analyze hardness of classification\n",
      "among instances and information bottlenecks.\n",
      "We make the following key contributions:\n",
      "1. We introduce\n",
      "Matryoshka Representation Learning (MRL) to obtain flexible representa-\n",
      "tions (Matryoshka Representations) for adaptive deployment (Section 3).\n",
      "2. Up to 14× faster yet accurate large-scale classification and retrieval using MRL (Section 4).\n",
      "3\\nshows that MRL also\n",
      "improves the cosine similarity span between positive and random image-text pairs.\n",
      "Few-shot and Long-tail Learning.\n",
      "We exhaustively evaluated few-shot learning on MRL models\n",
      "using nearest class mean [79]. Table 15 in Appendix G shows that that representations learned\n",
      "through MRL perform comparably to FF representations across varying shots and number of classes.\n",
      "Matryoshka Representations realize a unique pattern while evaluating on FLUID [92], a long-tail\n",
      "sequential learning framework. We observed that MRL provides up to 2% accuracy higher on novel\n",
      "classes in the tail of the distribution, without sacrificing accuracy on other classes (Table 16 in\n",
      "Appendix G). Additionally we find the accuracy between low-dimensional and high-dimensional\n",
      "representations is marginal for p\\n8\n",
      "FF\n",
      "86.40\n",
      "37.09\n",
      "71.74\n",
      "10.77\n",
      "37.04\n",
      "52.67\n",
      "MRL\n",
      "85.60\n",
      "36.83\n",
      "70.34\n",
      "12.88\n",
      "37.46\n",
      "52.18\n",
      "MRL–E\n",
      "83.01\n",
      "29.99\n",
      "65.37\n",
      "7.60\n",
      "31.97\n",
      "47.16\n",
      "Table 17: Top-1 classification accuracy (%) on out-of-domain datasets (ImageNet-V2/R/A/Sketch) to\n",
      "examine robustness of Matryoshka Representation Learning. Note that these results are without\n",
      "any fine tuning on these datasets.\n",
      "ImageNet-V1\n",
      "ImageNet-V2\n",
      "ImageNet-R\n",
      "ImageNet-A\n",
      "ImageNet-Sketch\n",
      "Rep. Size\n",
      "FF\n",
      "MRL–E\n",
      "MRL\n",
      "FF\n",
      "MRL–E\n",
      "MRL\n",
      "FF\n",
      "MRL–E\n",
      "MRL\n",
      "FF\n",
      "MRL–E\n",
      "MRL\n",
      "FF\n",
      "MRL–E\n",
      "MRL\n",
      "8\n",
      "65.86\n",
      "56.92\n",
      "67.46\n",
      "54.05\n",
      "47.40\n",
      "55.59\n",
      "24.60\n",
      "22.98\n",
      "23.57\n",
      "2.92\n",
      "3.63\n",
      "3.39\n",
      "17.73\n",
      "15.07\n",
      "17.98\n",
      "16\n",
      "73.10\n",
      "72.38\n",
      "73.80\n",
      "60.52\n",
      "60.48\n",
      "61.71\n",
      "28.51\n",
      "28.45\n",
      "28.85\n",
      "3.00\n",
      "3.55\n",
      "3.59\n",
      "21.70\n",
      "20.38\n",
      "21.77\n",
      "32\n",
      "74.68\n",
      "74.80\n",
      "75.26\n",
      "62.24\n",
      "62.23\n",
      "63.05\n",
      "31.28\n",
      "30.79\n",
      "31.47\n",
      "2.60\n",
      "3.65\n",
      "3.57\n",
      "22.03\n",
      "21.87\n",
      "22.48\n",
      "64\n",
      "75.45\n",
      "75.48\n",
      "76.17\n",
      "63.51\n",
      "63.15\n",
      "63.99\n",
      "32.96\n",
      "\\nall 1,000 ImageNet-1K classes.\n",
      "ObjectNet [2] contains 50K images across 313 object classes, each containing ∼160 images each.\n",
      "C\n",
      "Matryoshka Representation Learning Model Training\n",
      "We trained all ResNet50–MRL models using the efficient dataloaders of FFCV [56]. We utilized the\n",
      "rn50_40_epochs.yaml configuration file of FFCV to train all MRL models defined below:\n",
      "• MRL: ResNet50 model with the fc layer replaced by MRL_Linear_Layer(efficient=False)\n",
      "• MRL–E: ResNet50 model with the fc layer replaced by MRL_Linear_Layer(efficient=True)\n",
      "• FF–k: ResNet50 model with the fc layer replaced by torch.nn.Linear(k, num_classes),\n",
      "where k ∈[8, 16, 32, 64, 128, 256, 512, 1024, 2048]. We will henceforth refer to these models as\n",
      "simply FF, with the k value denoting representation size.\n",
      "We trained all ResNet50 m\\necognition, pages\n",
      "11162–11173, 2021.\n",
      "[19] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional\n",
      "transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n",
      "[20] T. G. Dietterich and G. Bakiri. Solving multiclass learning problems via error-correcting\n",
      "output codes. Journal of artificial intelligence research, 2:263–286, 1994.\n",
      "[21] S. K. Divvala, A. Farhadi, and C. Guestrin. Learning everything about anything: Webly-\n",
      "supervised visual concept learning. In Proceedings of the IEEE Conference on Computer\n",
      "Vision and Pattern Recognition, pages 3270–3277, 2014.\n",
      "[22] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. De-\n",
      "hghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transfor\\nyers can be found\n",
      "in the ResNet50 architecture [29]. All models were finetuned with the FFCV pipeline, with same\n",
      "training configuration as in the end-to-end training aside from changing lr = 0.1 and epochs = 10. We\n",
      "observed that finetuning the linear layer alone was insufficient to learn Matryoshka Representations\n",
      "at lower dimensionalities. Adding more and more non-linear conv+ReLU layers steadily improved\n",
      "classification accuracy of d = 8 from 5% to 60% after finetuning, which was only 6% less than\n",
      "training MRL end-to-end for 40 epochs. This difference was successively less pronounced as we\n",
      "increased dimensionality past d = 64, to within 1.5% for all larger dimensionalities. The full results\n",
      "of this ablation can be seen in Table 26.\n",
      "Relative Importance.\n",
      "We performed an ablation of MRL over\\nclear\n",
      "trend. When we repeated this experiment with independently trained FF models, we noticed that 950\n",
      "classes did not show a clear trend. This motivated us to leverage the disagreement as well as gradual\n",
      "improvement of accuracy at different representation sizes by training Matryoshka Representations.\n",
      "Figure 12 showcases the progression of relative per-class accuracy distribution compared to the\n",
      "29\n",
      "Table 16: Accuracy (%) categories indicates whether classes were present during ImageNet pretraining\n",
      "and head/tail indicates classes that have greater/less than 50 examples in the streaming test set. We\n",
      "observed that MRL performed better than the baseline on novel tail classes by ∼2% on average.\n",
      "Rep. Size\n",
      "Method\n",
      "Pretrain\n",
      "- Head (>50)\n",
      "Novel\n",
      "- Head (>50)\n",
      "Pretrain\n",
      "- Tail (<50)\n",
      "Novel\n",
      "- Tail (<50)\n",
      "M\\ns.\n",
      "Table 25 quantifies the performance with different representation size.\n",
      "K\n",
      "Ablation Studies\n",
      "K.1\n",
      "MRL Training Paradigm\n",
      "Matryoshka Representations via Finetuning.\n",
      "To observe if nesting can be induced in models that\n",
      "were not explicitly trained with nesting from scratch, we loaded a pretrained FF-2048 ResNet50 model\n",
      "and initialized a new MRL layer, as defined in Algorithm 2, Appendix C. We then unfroze different\n",
      "layers of the backbone to observe how much non-linearity in the form of unfrozen conv layers needed\n",
      "to be present to enforce nesting into a pretrained FF model. A description of these layers can be found\n",
      "in the ResNet50 architecture [29]. All models were finetuned with the FFCV pipeline, with same\n",
      "training configuration as in the end-to-end training aside from changing lr = 0.1 and e\\nasun, A. Torralba, and S. Fidler. Aligning\n",
      "books and movies: Towards story-like visual explanations by watching movies and reading\n",
      "books. In Proceedings of the IEEE international conference on computer vision, pages 19–27,\n",
      "2015.\n",
      "16\n",
      "Checklist\n",
      "1. For all authors...\n",
      "(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s\n",
      "contributions and scope? [Yes]\n",
      "(b) Did you describe the limitations of your work? [Yes] See Section 6\n",
      "(c) Did you discuss any potential negative societal impacts of your work? [N/A] Our work\n",
      "does not have any additional negative societal impact on top of the existing impact of\n",
      "representation learning. However, a study on the trade-off between representation size\n",
      "and the tendency to encode biases is an interesting future direction along the \\n68\n",
      "70.14\n",
      "69.54\n",
      "69.01\n",
      "68.41\n",
      "2048\n",
      "70.98\n",
      "65.20\n",
      "63.57\n",
      "62.56\n",
      "61.60\n",
      "70.18\n",
      "69.52\n",
      "68.98\n",
      "68.35\n",
      "1024\n",
      "2048\n",
      "1312\n",
      "70.97\n",
      "65.20\n",
      "63.57\n",
      "62.56\n",
      "61.60\n",
      "70.18\n",
      "69.52\n",
      "68.98\n",
      "68.35\n",
      "These results provide further evidence that different tasks require varying capacity based on their\n",
      "difficulty.\n",
      "H\n",
      "Robustness Experiments\n",
      "We evaluated the robustness of MRL models on out-of-domain datasets (ImageNetV2/R/A/Sketch)\n",
      "and compared them to the FF baseline. Each of these datasets is described in Appendix B. The\n",
      "results in Table 17 demonstrate that learning Matryoshka Representations does not hurt out-of-\n",
      "domain generalization relative to FF models, and Matryoshka Representations in fact improve\n",
      "the performance on ImageNet-A. For a ALIGN–MRL model, we examine the the robustness via\n",
      "zero-shot retrieval on out-of-domain datasets, i\\nties without the\n",
      "additional expense of multiple model forward passes for the web-scale databases. FF models\n",
      "also generate independent databases which become prohibitively expense to store and switch in\n",
      "between. Matryoshka Representations enable adaptive retrieval (AR) which alleviates the need\n",
      "to use full-capacity representations, d = 2048, for all data and downstream tasks. Lastly, all the\n",
      "vector compression techniques [60, 45] used as part of the ANNS pipelines are complimentary to\n",
      "Matryoshka Representations and can further improve the efficiency-vs-accuracy trade-off.\n",
      "4.3.1\n",
      "Adaptive Retrieval\n",
      "We benchmark MRL in the adaptive retrieval setting (AR) [50]. For a given query image, we obtained\n",
      "a shortlist, K = 200, of images from the database using a lower-dimensional representation, e.g.\n",
      "D\\n, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,\n",
      "A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International\n",
      "journal of computer vision, 115(3):211–252, 2015.\n",
      "[77] R. Salakhutdinov and G. Hinton. Learning a nonlinear embedding by preserving class neigh-\n",
      "bourhood structure. In Artificial Intelligence and Statistics, pages 412–419. PMLR, 2007.\n",
      "[78] R. Salakhutdinov and G. Hinton. Semantic hashing. International Journal of Approximate\n",
      "Reasoning, 50(7):969–978, 2009.\n",
      "[79] J. S. Sánchez, F. Pla, and F. J. Ferri. On the use of neighbourhood-based non-parametric\n",
      "classifiers. Pattern Recognition Letters, 18(11-13):1179–1186, 1997.\n",
      "[80] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-cam:\n",
      "Visual explanation\\nar(k, num_classes),\n",
      "where k ∈[8, 16, 32, 64, 128, 256, 512, 1024, 2048]. We will henceforth refer to these models as\n",
      "simply FF, with the k value denoting representation size.\n",
      "We trained all ResNet50 models with a learning rate of 0.475 with a cyclic learning rate schedule [83].\n",
      "This was after appropriate scaling (0.25×) of the learning rate specified in the configuration file to\n",
      "accommodate for 2xA100 NVIDIA GPUs available for training, compared to the 8xA100 GPUs\n",
      "utilized in the FFCV benchmarks. We trained with a batch size of 256 per GPU, momentum [86] of\n",
      "0.9, and an SGD optimizer with a weight decay of 1e-4.\n",
      "Our code (Appendix A) makes minimal modifications to the training pipeline provided by FFCV to\n",
      "learn Matryoshka Representations.\n",
      "We trained ViT-B/16 models for JFT-300M on a 8x8 clo\\nsentation size for both top-1 and mAP@10, and especially\n",
      "at low representation size (Ds ≤32). MRL–E loses out to FF significantly only at Ds = 8. This\n",
      "indicates that training ResNet50 models via the MRL training paradigm improves retrieval at low\n",
      "representation size over models explicitly trained at those representation size (FF-8...2048).\n",
      "We carried out all retrieval experiments at Ds ∈{8, 16, 32, 64, 128, 256, 512, 1024, 2048}, as\n",
      "these were the representation sizes which were a part of the nesting_list at which losses\n",
      "were added during training, as seen in Algorithm 1, Appendix A. To examine whether MRL\n",
      "is able to learn Matryoshka Representations at dimensions in between the representation size\n",
      "for which it was trained, we also tabulate the performance of MRL at interpolated Ds ∈\n",
      "{12, 2\\nce of MRL model on 31-way classification (1 extra class is for reject token) on\n",
      "ImageNet-1K superclasses.\n",
      "Rep. Size\n",
      "8\n",
      "16\n",
      "32\n",
      "64\n",
      "128\n",
      "256\n",
      "512\n",
      "1024\n",
      "2048\n",
      "MRL\n",
      "85.57\n",
      "88.67\n",
      "89.48\n",
      "89.82\n",
      "89.97\n",
      "90.11\n",
      "90.18\n",
      "90.22\n",
      "90.21\n",
      "Matryoshka Representations at Arbitrary Granularities.\n",
      "To train MRL, we used nested di-\n",
      "mensions at logarithmic granularities M = {8, 16, . . . , 1024, 2048} as detailed in Section 3. We\n",
      "made this choice for two empirically-driven reasons: a) The accuracy improvement with increasing\n",
      "representation size was more logarithmic than linear (as shown by FF models in Figure 2). This indi-\n",
      "cated that optimizing for granularities increasing in a non-logarithmic fashion would be sub-optimal\n",
      "both for maximum performance and expected efficiency; b) If we have m arbitrary granularities,\n",
      "the expected\\nat optimizing for granularities increasing in a non-logarithmic fashion would be sub-optimal\n",
      "both for maximum performance and expected efficiency; b) If we have m arbitrary granularities,\n",
      "the expected cost of the linear classifier to train MRL scales as O(L ∗(m2)) while logarithmic\n",
      "granularities result in O(L ∗2log(d)) space and compute costs.\n",
      "To demonstrate this effect, we learned Matryoshka Representations with uniform (MRL-Uniform)\n",
      "nesting dimensions m\n",
      "∈\n",
      "M\n",
      "=\n",
      "{8, 212, 416, 620, 824, 1028, 1232, 1436, 1640, 1844, 2048}.\n",
      "We\n",
      "evaluated\n",
      "this\n",
      "model\n",
      "at\n",
      "the\n",
      "standard\n",
      "(MRL-log)\n",
      "dimensions\n",
      "m\n",
      "∈\n",
      "M\n",
      "=\n",
      "{8, 16, 32, 64, 128, 256, 512, 1024, 2048} for ease of comparison to reported numbers using 1-NN ac-\n",
      "curacy (%). As shown in Table 29, we observed that while performance interpolated, MRL-Uniform\n",
      "suffered\\nable 7: Masked Language Modelling (MLM) accuracy(%) of FF and MRL models on the validation\n",
      "set.\n",
      "Rep. Size\n",
      "BERT-FF\n",
      "BERT-MRL\n",
      "12\n",
      "60.12\n",
      "59.92\n",
      "24\n",
      "62.49\n",
      "62.05\n",
      "48\n",
      "63.85\n",
      "63.40\n",
      "96\n",
      "64.32\n",
      "64.15\n",
      "192\n",
      "64.70\n",
      "64.58\n",
      "384\n",
      "65.03\n",
      "64.81\n",
      "768\n",
      "65.54\n",
      "65.00\n",
      "size and the interpolated representation size as we increase Ds, which demonstrates that MRL is\n",
      "able to learn Matryoshka Representations at nearly all representation size m ∈[8, 2048] despite\n",
      "optimizing only for |M| nested representation sizes.\n",
      "We examined the robustness of MRL for retrieval on out-of-domain datasets ImageNetV2 and\n",
      "ImageNet-4K, as shown in Table 9 and Table 10 respectively. On ImageNetV2, we observed that MRL\n",
      "outperformed FF at all Ds on top-1 Accuracy and mAP@10, and MRL–E outperformed FF at all\n",
      "Ds except Ds = 8. This demonstrates the robustness\\nnd deployment.\n",
      "MRL learns coarse-to-fine representations that are at least as accurate and rich as\n",
      "independently trained low-dimensional representations. The flexibility within the\n",
      "learned Matryoshka Representations offer: (a) up to 14× smaller embedding\n",
      "size for ImageNet-1K classification at the same level of accuracy; (b) up to 14×\n",
      "real-world speed-ups for large-scale retrieval on ImageNet-1K and 4K; and (c) up\n",
      "to 2% accuracy improvements for long-tail few-shot classification, all while being\n",
      "as robust as the original representations. Finally, we show that MRL extends seam-\n",
      "lessly to web-scale datasets (ImageNet, JFT) across various modalities – vision\n",
      "(ViT, ResNet), vision + language (ALIGN) and language (BERT). MRL code and\n",
      "pretrained models are open-sourced at https://github.com/RAIVN\\nd Table 10 respectively. On ImageNetV2, we observed that MRL\n",
      "outperformed FF at all Ds on top-1 Accuracy and mAP@10, and MRL–E outperformed FF at all\n",
      "Ds except Ds = 8. This demonstrates the robustness of the learned Matryoshka Representations\n",
      "for out-of-domain image retrieval.\n",
      "F\n",
      "Adaptive Retrieval\n",
      "The time complexity of retrieving a shortlist of k-NN often scales as O(d), where d =Ds, for a\n",
      "fixed k and N. We thus will have a theoretical 256× higher cost for Ds = 2048 over Ds = 8. We\n",
      "discuss search complexity in more detail in Appendix I. In an attempt to replicate performance at\n",
      "higher Ds while using less FLOPs, we perform adaptive retrieval via retrieving a k-NN shortlist with\n",
      "representation size Ds, and then re-ranking the shortlist with representations of size Dr. Adaptive\n",
      "retrieval for\\n improve efficiency for large-scale classification and retrieval\n",
      "without any significant loss of accuracy. While there are potentially several applications of coarse-to-\n",
      "fine Matryoshka Representations, in this work we focus on two key building blocks of real-world\n",
      "ML systems: large-scale classification and retrieval. For classification, we use adaptive cascades with\n",
      "the variable-size representations from a model trained with MRL, significantly reducing the average\n",
      "dimension of embeddings needed to achieve a particular accuracy. For example, on ImageNet-1K,\n",
      "MRL + adaptive classification results in up to a 14× smaller representation size at the same accuracy\n",
      "as baselines (Section 4.2.1). Similarly, we use MRL in an adaptive retrieval system. Given a query,\n",
      "we shortlist retrieval candidates \\nt). Every combination of Ds & Dr falls above the Pareto\n",
      "line (orange dots) of single-shot retrieval with a fixed representation size while having configurations\n",
      "that are as accurate while being up to 14× faster in real-world deployment. Funnel retrieval is almost\n",
      "as accurate as the baseline while alleviating some of the parameter choices of Adaptive Retrieval.\n",
      "point, further strengthening the use-case for Matryoshka Representation Learning and adaptive\n",
      "retrieval.\n",
      "Even with adaptive retrieval, it is hard to determine the choice of Ds & Dr. In order to alleviate this\n",
      "issue to an extent, we propose Funnel Retrieval, a consistent cascade for adaptive retrieval. Funnel\n",
      "thins out the initial shortlist by a repeated re-ranking and shortlisting with a series of increasing\n",
      "capacity representations.\\nion size for MRL &\n",
      "FF models showing the capture of underlying\n",
      "hierarchy through tight information bottlenecks.\n",
      "8\n",
      "16\n",
      "32\n",
      "64\n",
      "128\n",
      "256\n",
      "512\n",
      "1024\n",
      "2048\n",
      "Representation Size\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "Top-1 Accuracy (%)\n",
      "measuring device\n",
      "building\n",
      "garment\n",
      "tool\n",
      "nourishment\n",
      "protective covering\n",
      "vessel\n",
      "oscine\n",
      "Figure 11:\n",
      "Diverse per-superclass accuracy\n",
      "trends across representation sizes for ResNet50-\n",
      "MRL on ImageNet-1K.\n",
      "9\n",
      "occurs with both MRL and FF models; MRL is more accurate across dimensions. This shows that\n",
      "tight information bottlenecks while not highly accurate for fine-grained classification, do capture\n",
      "required semantic information for coarser classification that could be leveraged for adaptive routing\n",
      "for retrieval and classification. Mutifidelity of Matryoshka Representation naturally captures the\n",
      "und\\nYang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig.\n",
      "Scaling up visual and vision-language representation learning with noisy text supervision. In\n",
      "International Conference on Machine Learning, pages 4904–4916. PMLR, 2021.\n",
      "[47] J. Johnson, M. Douze, and H. Jégou. Billion-scale similarity search with GPUs. IEEE\n",
      "Transactions on Big Data, 7(3):535–547, 2019.\n",
      "[48] W. B. Johnson. Extensions of lipschitz mappings into a hilbert space. Contemp. Math., 26:\n",
      "189–206, 1984.\n",
      "[49] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, S. Bates, S. Bhatia,\n",
      "N. Boden, A. Borchers, et al. In-datacenter performance analysis of a tensor processing unit.\n",
      "In Proceedings of the 44th annual international symposium on computer architecture, pages\n",
      "1–12, 2017.\n",
      "[50] T.\\n accuracy with increase in capacity. However, we observed that\n",
      "8\n",
      "(a)\n",
      "(b)\n",
      "(c)\n",
      "Figure 9: Grad-CAM [80] progression of predictions in MRL model across 8, 16, 32 and 2048\n",
      "dimensions. (a) 8-dimensional representation confuses due to presence of other relevant objects (with\n",
      "a larger field of view) in the scene and predicts “shower cap” ; (b) 8-dim model confuses within\n",
      "the same super-class of “boa” ; (c) 8 and 16-dim models incorrectly focus on the eyes of the doll\n",
      "(\"sunglasses\") and not the \"sweatshirt\" which is correctly in focus at higher dimensions; MRL fails\n",
      "gracefully in these scenarios and shows potential use cases of disagreement across dimensions.\n",
      "this trend was not ubiquitous and certain instances and classes were more accurate when evaluated\n",
      "with lower-dimensions (Figure 12 in Appendi\\nConference on Machine Learning, pages 5389–5400. PMLR,\n",
      "2019.\n",
      "[73] O. Rippel, M. Gelbart, and R. Adams. Learning ordered representations with nested dropout.\n",
      "In International Conference on Machine Learning, pages 1746–1754. PMLR, 2014.\n",
      "[74] J. Rissanen. Modeling by shortest data description. Automatica, 14(5):465–471, 1978.\n",
      "[75] S. Ruder, M. E. Peters, S. Swayamdipta, and T. Wolf. Transfer learning in natural language\n",
      "processing. In Proceedings of the 2019 conference of the North American chapter of the\n",
      "association for computational linguistics: Tutorials, pages 15–18, 2019.\n",
      "[76] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,\n",
      "A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International\n",
      "journal of computer vision, 115\\nows potential use cases of disagreement across dimensions.\n",
      "this trend was not ubiquitous and certain instances and classes were more accurate when evaluated\n",
      "with lower-dimensions (Figure 12 in Appendix J). With perfect routing of instances to appropriate\n",
      "dimension, MRL can gain up to 4.6% classification accuracy. At the same time, the low-dimensional\n",
      "models are less accurate either due to confusion within the same superclass [24] of the ImageNet\n",
      "hierarchy or presence of multiple objects of interest. Figure 9 showcases 2 such examples for 8-\n",
      "dimensional representation. These results along with Appendix J put forward the potential for MRL\n",
      "to be a systematic framework for analyzing the utility and efficiency of information bottlenecks.\n",
      "Superclass Accuracy.\n",
      "As the information bottleneck become\\nch\n",
      "using hierarchical navigable small world graphs. IEEE transactions on pattern analysis and\n",
      "machine intelligence, 42(4):824–836, 2018.\n",
      "[63] J. Masci, U. Meier, D. Cire¸san, and J. Schmidhuber. Stacked convolutional auto-encoders for\n",
      "hierarchical feature extraction. In International conference on artificial neural networks, pages\n",
      "52–59. Springer, 2011.\n",
      "[64] P. Mitra, C. Murthy, and S. K. Pal. Unsupervised feature selection using feature similarity.\n",
      "IEEE transactions on pattern analysis and machine intelligence, 24(3):301–312, 2002.\n",
      "[65] V. Nanda, T. Speicher, J. P. Dickerson, S. Feizi, K. P. Gummadi, and A. Weller. Diffused\n",
      "redundancy in pre-trained representations. arXiv preprint arXiv:2306.00183, 2023.\n",
      "[66] P. Nayak. Understanding searches better than ever before. Google AI Blog, 2019. \\nny\n",
      "representation size. The remaining 81.54% constitutes the oracle accuracy.\n",
      "Rep. Size\n",
      "8\n",
      "16\n",
      "32\n",
      "64\n",
      "128\n",
      "256\n",
      "512\n",
      "1024\n",
      "2048\n",
      "Always\n",
      "Wrong\n",
      "Correctly\n",
      "Predicted\n",
      "67.46\n",
      "8.78\n",
      "2.58\n",
      "1.35\n",
      "0.64\n",
      "0.31\n",
      "0.20\n",
      "0.12\n",
      "0.06\n",
      "18.46\n",
      "of disagreement arising when the models got confused within the same superclass. For example,\n",
      "ImageNet-1K has multiple \"snake\" classes, and models often confuse a snake image for an incorrect\n",
      "species of snake.\n",
      "Superclass Performance\n",
      "We created a 30 superclass subset of the validation set based on wordnet\n",
      "hierarchy (Table 24) to quantify the performance of MRL model on ImageNet-1K superclasses.\n",
      "Table 25 quantifies the performance with different representation size.\n",
      "K\n",
      "Ablation Studies\n",
      "K.1\n",
      "MRL Training Paradigm\n",
      "Matryoshka Representations via Finetuning.\n",
      "To observe if nesting can be induced \\n Ds = 64 on ImageNet-1K and ImageNet-4K, at 32× less\n",
      "MFLOPs. This demonstrates the value of intelligent routing techniques which utilize appropriately\n",
      "sized Matryoshka Representations for retrieval.\n",
      "24\n",
      "Table 8: Retrieve a shortlist of 200-NN with Ds sized representations on ImageNet-1K via exact\n",
      "search with L2 distance metric. Top-1 and mAP@10 entries (%) where MRL–E and MRL outperform\n",
      "FF at their respective representation sizes are bolded.\n",
      "Model\n",
      "Ds\n",
      "MFlops\n",
      "Top-1\n",
      "Top-5\n",
      "Top-10\n",
      "mAP@10\n",
      "mAP@25\n",
      "mAP@50\n",
      "mAP@100\n",
      "P@10\n",
      "P@25\n",
      "P@50\n",
      "P@100\n",
      "FF\n",
      "8\n",
      "10\n",
      "58.93\n",
      "75.76\n",
      "80.25\n",
      "53.42\n",
      "52.29\n",
      "51.84\n",
      "51.57\n",
      "59.32\n",
      "59.28\n",
      "59.25\n",
      "59.21\n",
      "16\n",
      "20\n",
      "66.77\n",
      "80.88\n",
      "84.40\n",
      "61.63\n",
      "60.51\n",
      "59.98\n",
      "59.62\n",
      "66.76\n",
      "66.58\n",
      "66.43\n",
      "66.27\n",
      "32\n",
      "41\n",
      "68.84\n",
      "82.58\n",
      "86.14\n",
      "63.35\n",
      "62.08\n",
      "61.36\n",
      "60.76\n",
      "68.43\n",
      "68.13\n",
      "67.83\n",
      "67.48\n",
      "64\n",
      "82\n",
      "69.41\n",
      "83.56\n",
      "87.33\n",
      "63.26\n",
      "61.64\n",
      "60.63\n",
      "59.67\n",
      "68.4\\nines significantly, which indicates that\n",
      "pretrained models lack the multifidelity of Matryoshka Representations and are incapable of fitting\n",
      "an accurate linear classifier at low representation sizes.\n",
      "We compared the performance of MRL models at various representation sizes via 1-nearest neighbors\n",
      "(1-NN) image classification accuracy on ImageNet-1K in Table 2 and Figure 3. We provide detailed\n",
      "information regarding the k-NN search pipeline in Appendix E. We compared against a baseline\n",
      "of attempting to enforce nesting to a FF-2048 model by 1) Random Feature Selection (Rand. FS):\n",
      "considering the first m dimensions of FF-2048 for NN lookup, and 2) FF+SVD: performing SVD\n",
      "on the FF-2048 representations at the specified representation size, 3) FF+JL: performing random\n",
      "projection according to the J\\nre 5: Despite optimizing MRL only for\n",
      "O(log(d)) dimensions for ResNet50 and ViT-\n",
      "B/16 models; the accuracy in the intermediate\n",
      "dimensions shows interpolating behaviour.\n",
      "4.2.1\n",
      "Adaptive Classification\n",
      "The flexibility and coarse-to-fine granularity within Matryoshka Representations allows model\n",
      "cascades [90] for Adaptive Classification (AC) [28]. Unlike standard model cascades [95], MRL does\n",
      "not require multiple expensive neural network forward passes. To perform AC with an MRL trained\n",
      "model, we learn thresholds on the maximum softmax probability [33] for each nested classifier on\n",
      "a holdout validation set. We then use these thresholds to decide when to transition to the higher\n",
      "dimensional representation (e.g 8 →16 →32) of the MRL model. Appendix D.1 discusses the\n",
      "implementation and learning o\\nGummadi, and A. Weller. Diffused\n",
      "redundancy in pre-trained representations. arXiv preprint arXiv:2306.00183, 2023.\n",
      "[66] P. Nayak. Understanding searches better than ever before. Google AI Blog, 2019. URL https:\n",
      "//blog.google/products/search/search-language-understanding-bert/.\n",
      "[67] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\n",
      "N. Gimelshein, L. Antiga, et al.\n",
      "Pytorch: An imperative style, high-performance deep\n",
      "learning library. Advances in neural information processing systems, 32, 2019.\n",
      "[68] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer.\n",
      "Deep contextualized word representations. In Proceedings of the 2018 Conference of the\n",
      "North American Chapter of the Association for Computational Linguistics: Human Language\n",
      "Te\\ns, 27, 2014.\n",
      "[99] H.-F. Yu, K. Zhong, J. Zhang, W.-C. Chang, and I. S. Dhillon. Pecos: Prediction for enormous\n",
      "and correlated output spaces. Journal of Machine Learning Research, 23(98):1–32, 2022.\n",
      "[100] J. Yu, L. Yang, N. Xu, J. Yang, and T. Huang. Slimmable neural networks. arXiv preprint\n",
      "arXiv:1812.08928, 2018.\n",
      "[101] R. Zellers, J. Lu, X. Lu, Y. Yu, Y. Zhao, M. Salehi, A. Kusupati, J. Hessel, A. Farhadi, and\n",
      "Y. Choi. Merlot reserve: Neural script knowledge through vision and language and sound.\n",
      "arXiv preprint arXiv:2201.02639, 2022.\n",
      "[102] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning\n",
      "books and movies: Towards story-like visual explanations by watching movies and reading\n",
      "books. In Proceedings of the IEEE international conference on compute\\n-accuracy\n",
      "trade-off\n",
      "for\n",
      "adaptive\n",
      "retrieval\n",
      "using\n",
      "Matryoshka Representations compared to single-shot using fixed features with ResNet50\n",
      "on ImageNet-1K. We observed that all AR settings lied above the Pareto frontier of single-shot\n",
      "retrieval with varying representation sizes. In particular for ImageNet-1K, we show that the AR\n",
      "model with Ds = 16 & Dr = 2048 is as accurate as single-shot retrieval with d = 2048 while being\n",
      "∼128× more efficient in theory and ∼14× faster in practice (compared using HNSW on the same\n",
      "hardware). We show similar trends with ImageNet-4K, but note that we require Ds = 64 given\n",
      "the increased difficulty of the dataset. This results in ∼32× and ∼6× theoretical and in-practice\n",
      "speedups respectively. Lastly, while K = 200 works well for our adaptive retrieval experiments, \\nithin the same larger network. However, the weights for each progressively smaller\n",
      "network can be different and often require distinct forward passes to isolate the final representations.\n",
      "This is detrimental for adaptive inference due to the need for re-encoding the entire retrieval database\n",
      "with expensive sub-net forward passes of varying capacities. Several works [23, 26, 65, 59] investigate\n",
      "the notions of intrinsic dimensionality and redundancy of representations and objective spaces pointing\n",
      "to minimum description length [74]. Finally, ordered representations proposed by Rippel et al. [73]\n",
      "use nested dropout in the context of autoencoders to learn nested representations. MRL differentiates\n",
      "itself in formulation by optimizing only for O(log(d)) nesting dimensions instead of O(d). Despit\\n) utilization\n",
      "of the representation for downstream applications [50, 89]. Compute costs for the latter part of the\n",
      "pipeline scale with the embedding dimensionality as well as the data size (N) and label space (L).\n",
      "At web-scale [15, 85] this utilization cost overshadows the feature computation cost. The rigidity in\n",
      "these representations forces the use of high-dimensional embedding vectors across multiple tasks\n",
      "despite the varying resource and accuracy constraints that require flexibility.\n",
      "Human perception of the natural world has a naturally coarse-to-fine granularity [28, 32]. However,\n",
      "perhaps due to the inductive bias of gradient-based training [84], deep learning models tend to diffuse\n",
      "“information” across the entire representation vector. The desired elasticity is usually enabled in the\\nt MRL can be used for large-scale adaptive\n",
      "classification as well as adaptive retrieval. On standard benchmarks, MRL matches the accuracy of\n",
      "the fixed-feature baseline despite using 14× smaller representation size on average. Furthermore, the\n",
      "Matryoshka Representation based adaptive shortlisting and re-ranking system ensures comparable\n",
      "mAP@10 to the baseline while being 128× cheaper in FLOPs and 14× faster in wall-clock time.\n",
      "Finally, most of the efficiency techniques for model inference and vector search are complementary\n",
      "to MRL\n",
      "further assisting in deployment at the compute-extreme environments.\n",
      "Acknowledgments\n",
      "We are grateful to Srinadh Bhojanapalli, Lovish Madaan, Raghav Somani, Ludwig Schmidt, and\n",
      "Venkata Sailesh Sanampudi for helpful discussions and feedback. Aditya Kusupati also tha\\n5736\n",
      "0.6060\n",
      "1.2781\n",
      "2.7047\n",
      "HNSW32\n",
      "0.1193\n",
      "0.1455\n",
      "0.1833\n",
      "0.2145\n",
      "0.2333\n",
      "0.2670\n",
      "observation on the expected dimensionality for 76.30% top-1 classification accuracy being just\n",
      "d ∼37. We leave the design and learning of a more optimal policy for future work.\n",
      "Grad-CAM Examples\n",
      "We analyzed the nature of model disagreement across representation\n",
      "sizes with MRL models with the help of Grad-CAM visualization [80]. We observed there were\n",
      "certain classes in ImageNet-1K such as \"tools\", \"vegetables\" and \"meat cutting knife\" which were\n",
      "occasionally located around multiple objects and a cluttered environment. In such scenarios, we\n",
      "observed that smaller representation size models would often get confused due to other objects and fail\n",
      "to extract the object of interest which generated the correct label. We als\\n on top of the existing impact of\n",
      "representation learning. However, a study on the trade-off between representation size\n",
      "and the tendency to encode biases is an interesting future direction along the lines of\n",
      "existing literature [36, 37]. A part of this is already presented in Section 5.\n",
      "(d) Have you read the ethics review guidelines and ensured that your paper conforms to\n",
      "them? [Yes]\n",
      "2. If you are including theoretical results...\n",
      "(a) Did you state the full set of assumptions of all theoretical results? [N/A]\n",
      "(b) Did you include complete proofs of all theoretical results? [N/A]\n",
      "3. If you ran experiments...\n",
      "(a) Did you include the code, data, and instructions needed to reproduce the main ex-\n",
      "perimental results (either in the supplemental material or as a URL)? [Yes] See sup-\n",
      "plemental mater\\n2020.\n",
      "[70] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understand-\n",
      "ing by generative pre-training. OpenAI Blog, 2018. URL https://openai.com/blog/\n",
      "language-unsupervised/.\n",
      "14\n",
      "[71] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\n",
      "P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language su-\n",
      "pervision. In International Conference on Machine Learning, pages 8748–8763. PMLR,\n",
      "2021.\n",
      "[72] B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to\n",
      "imagenet? In International Conference on Machine Learning, pages 5389–5400. PMLR,\n",
      "2019.\n",
      "[73] O. Rippel, M. Gelbart, and R. Adams. Learning ordered representations with nested dropout.\n",
      "In International Conference on Machine Lear\\nrXiv preprint arXiv:1911.05248, 2019.\n",
      "[37] S. Hooker, N. Moorosi, G. Clark, S. Bengio, and E. Denton. Characterising bias in compressed\n",
      "models. arXiv preprint arXiv:2010.03058, 2020.\n",
      "[38] H. Hotelling. Analysis of a complex of statistical variables into principal components. Journal\n",
      "of educational psychology, 24(6):417, 1933.\n",
      "[39] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and\n",
      "H. Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications.\n",
      "arXiv preprint arXiv:1704.04861, 2017.\n",
      "[40] J. Howard and S. Ruder. Universal language model fine-tuning for text classification. arXiv\n",
      "preprint arXiv:1801.06146, 2018.\n",
      "[41] H. Hu, D. Dey, M. Hebert, and J. A. Bagnell. Learning anytime predictions in neural networks\n",
      "via adaptive loss bal\\nrs.\n",
      "We also found that for both MRL and FF, as the shot number decreased, the required representa-\n",
      "tion size to reach optimal accuracy decreased (Table 15). For example, we observed that 1-shot\n",
      "performance at 32 representation size had equal accuracy to 2048 representation size.\n",
      "FLUID.\n",
      "For the long-tailed setting we evaluated MRL on the FLUID benchmark [92] which\n",
      "contains a mixture of pretrain and new classes. Table 16 shows the evaluation of the learned\n",
      "representation on FLUID. We observed that MRL provided up to 2% higher accuracy on novel\n",
      "classes in the tail of the distribution, without sacrificing accuracy on other classes. Additionally we\n",
      "found the accuracy between low-dimensional and high-dimensional representations was marginal for\n",
      "pretrain classes. For example, the 64-dimensional M\\n= {12, 24, 48, 96, 192, 384, 768} as\n",
      "the explicitly optimized nested dimensions respectively. Lastly, we extensively compare the MRL\n",
      "and MRL–E models to independently trained low-dimensional (fixed feature) representations (FF),\n",
      "dimensionality reduction (SVD), sub-net method (slimmable networks [100]) and randomly selected\n",
      "features of the highest capacity FF model.\n",
      "In section 4.2, we evaluate the quality and capacity of the learned representations through linear\n",
      "classification/probe (LP) and 1-nearest neighbour (1-NN) accuracy. Experiments show that MRL\n",
      "models remove the dependence on |M| resource-intensive independently trained models for the\n",
      "coarse-to-fine representations while being as accurate. Lastly, we show that despite optimizing only\n",
      "for |M| dimensions, MRL models diffuse the info\\ning 1.8B image-text pairs.\n",
      "ImageNet Robustness Datasets\n",
      "We experimented on the following datasets to examine the robust-\n",
      "ness of MRL models:\n",
      "ImageNetV2 [72] is a collection of 10K images sampled a decade after the original construction of\n",
      "ImageNet [16]. ImageNetV2 contains 10 examples each from the 1,000 classes of ImageNet-1K.\n",
      "ImageNet-A [35] contains 7.5K real-world adversarially filtered images from 200 ImageNet-\n",
      "1K classes.\n",
      "ImageNet-R [34] contains 30K artistic image renditions for 200 of the original ImageNet-1K classes.\n",
      "ImageNet-Sketch [94] contains 50K sketches, evenly distributed over all 1,000 ImageNet-1K classes.\n",
      "ObjectNet [2] contains 50K images across 313 object classes, each containing ∼160 images each.\n",
      "C\n",
      "Matryoshka Representation Learning Model Training\n",
      "We trained all ResNet5\\nViT-B/16 vision\n",
      "encoder and BERT language encoder on ALIGN data [46] and (c) Masked language modelling:\n",
      "BERT [19] on English Wikipedia and BooksCorpus [102]. Please refer to Appendices B and C for\n",
      "details regarding the model architectures, datasets and training specifics.\n",
      "4\n",
      "8\n",
      "16\n",
      "32\n",
      "64\n",
      "128\n",
      "256\n",
      "512\n",
      "1024\n",
      "2048\n",
      "Representation Size\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "Top-1 Accuracy (%)\n",
      "MRL\n",
      "MRL-E\n",
      "FF\n",
      "SVD\n",
      "Slim. Net\n",
      "Rand. LP\n",
      "Figure 2: ImageNet-1K linear classification ac-\n",
      "curacy of ResNet50 models. MRL is as accurate\n",
      "as the independently trained FF models for every\n",
      "representation size.\n",
      "8\n",
      "16\n",
      "32\n",
      "64\n",
      "128\n",
      "256\n",
      "512\n",
      "1024\n",
      "2048\n",
      "Representation Size\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "1-NN Accuracy (%)\n",
      "MRL\n",
      "MRL-E\n",
      "FF\n",
      "SVD\n",
      "Slim. Net\n",
      "Rand. FS\n",
      "Figure 3:\n",
      "ImageNet-1K 1-NN accuracy of\n",
      "ResNet50 models measuring the representation\n",
      "quality for downstream task. MRL ou\\nV. Vanhoucke,\n",
      "V. Vasudevan, F. Viégas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and\n",
      "X. Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL\n",
      "https://www.tensorflow.org/. Software available from tensorflow.org.\n",
      "[2] A. Barbu, D. Mayo, J. Alverio, W. Luo, C. Wang, D. Gutfreund, J. Tenenbaum, and B. Katz.\n",
      "Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition\n",
      "models. Advances in neural information processing systems, 32, 2019.\n",
      "[3] S. Bengio, J. Weston, and D. Grangier. Label embedding trees for large multi-class tasks.\n",
      "Advances in Neural Information Processing Systems, 23, 2010.\n",
      "[4] Y. Bengio. Deep learning of representations for unsupervised and transfer learning. In\n",
      "Proceedings of ICML workshop on unsupervised\\nn artificial intelligence and statistics, pages 297–304. JMLR Workshop and Conference\n",
      "Proceedings, 2010.\n",
      "[28] M. G. Harris and C. D. Giachritsis. Coarse-grained information dominates fine-grained\n",
      "information in judgments of time-to-contact from retinal flow. Vision research, 40(6):601–611,\n",
      "2000.\n",
      "[29] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In\n",
      "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–\n",
      "778, 2016.\n",
      "[30] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual\n",
      "representation learning. In Proceedings of the IEEE/CVF conference on computer vision and\n",
      "pattern recognition, pages 9729–9738, 2020.\n",
      "[31] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick. Masked autoencoders\\nle datasets (ImageNet, JFT) across various modalities – vision\n",
      "(ViT, ResNet), vision + language (ALIGN) and language (BERT). MRL code and\n",
      "pretrained models are open-sourced at https://github.com/RAIVNLab/MRL.\n",
      "1\n",
      "Introduction\n",
      "Learned representations [57] are fundamental building blocks of real-world ML systems [66, 91].\n",
      "Trained once and frozen, d-dimensional representations encode rich information and can be used\n",
      "to perform multiple downstream tasks [4]. The deployment of deep representations has two steps:\n",
      "(1) an expensive yet constant-cost forward pass to compute the representation [29] and (2) utilization\n",
      "of the representation for downstream applications [50, 89]. Compute costs for the latter part of the\n",
      "pipeline scale with the embedding dimensionality as well as the data size (N) and lab\\n. Yang, and S. Kumar. Pre-training tasks for embedding-\n",
      "based large-scale retrieval. arXiv preprint arXiv:2002.03932, 2020.\n",
      "[11] W.-C. Chang, D. Jiang, H.-F. Yu, C. H. Teo, J. Zhang, K. Zhong, K. Kolluri, Q. Hu,\n",
      "N. Shandilya, V. Ievgrafov, et al. Extreme multi-label learning for semantic matching in\n",
      "product search. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discov-\n",
      "ery & Data Mining, pages 2643–2651, 2021.\n",
      "[12] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning\n",
      "of visual representations. In International conference on machine learning, pages 1597–1607.\n",
      "PMLR, 2020.\n",
      "[13] Y. Chen, Z. Liu, H. Xu, T. Darrell, and X. Wang. Meta-baseline: exploring simple meta-\n",
      "learning for few-shot learning. In Proceedings of the IEEE/CVF Internationa\\ne, and L. Zettlemoyer.\n",
      "Deep contextualized word representations. In Proceedings of the 2018 Conference of the\n",
      "North American Chapter of the Association for Computational Linguistics: Human Language\n",
      "Technologies, Volume 1 (Long Papers), pages 2227–2237, New Orleans, Louisiana, June\n",
      "2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1202. URL https:\n",
      "//aclanthology.org/N18-1202.\n",
      "[69] Y. Prabhu, A. Kusupati, N. Gupta, and M. Varma. Extreme regression for dynamic search\n",
      "advertising. In Proceedings of the 13th International Conference on Web Search and Data\n",
      "Mining, pages 456–464, 2020.\n",
      "[70] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understand-\n",
      "ing by generative pre-training. OpenAI Blog, 2018. URL https://openai.com/blog/\n",
      "language-unsupervise\\n results in Section 5.1 reveal interesting weaknesses of MRL that would be logical directions\n",
      "for future work. (1) Optimizing the weightings of the nested losses to obtain a Pareto optimal\n",
      "accuracy-vs-efficiency trade-off – a potential solution could emerge from adaptive loss balancing\n",
      "aspects of anytime neural networks [41]. (2) Using different losses at various fidelities aimed at\n",
      "solving a specific aspect of adaptive deployment – e.g. high recall for 8-dimension and robustness\n",
      "for 2048-dimension. (3) Learning a search data-structure, like differentiable k-d tree, on top of\n",
      "Matryoshka Representation to enable dataset and representation aware retrieval. (4) Finally, the\n",
      "joint optimization of multi-objective MRL combined with end-to-end learnable search data-structure\n",
      "to have data-driven a\\nrmance of ResNet50 representations on ImageNet-1K across\n",
      "dimensionalities for MRL, MRL–E, FF, slimmable networks along with post-hoc compression\n",
      "of vectors using SVD and random feature selection. Matryoshka Representations are often the\n",
      "most accurate while being up to 3% better than the FF baselines. Similar to classification, post-hoc\n",
      "compression and slimmable network baselines suffer from significant drop-off in retrieval mAP@10\n",
      "with ≤256 dimensions. Appendix E discusses the mAP@10 of the same models on ImageNet-4K.\n",
      "MRL models are capable of performing accurate retrieval at various granularities without the\n",
      "additional expense of multiple model forward passes for the web-scale databases. FF models\n",
      "also generate independent databases which become prohibitively expense to store and switch i\\nlg 1 and 2 provided below to train supervised ResNet50–MRL models on ImageNet-1K.\n",
      "We provide this code as a template to extend MRL to any domain.\n",
      "Algorithm 1 Pytorch code for Matryoshka Cross-Entropy Loss\n",
      "class Matryoshka_CE_Loss(nn.Module):\n",
      "def __init__(self, relative_importance, **kwargs):\n",
      "super(Matryoshka_CE_Loss, self).__init__()\n",
      "self.criterion = nn.CrossEntropyLoss(**kwargs)\n",
      "self.relative_importance = relative_importance # usually set\n",
      "to all ones\n",
      "def forward(self, output, target):\n",
      "loss=0\n",
      "for i in range(len(output)):\n",
      "loss+= self.relative_importance[i] * self.criterion(output[\n",
      "i], target)\n",
      "return loss\n",
      "Algorithm 2 Pytorch code for MRL Linear Layer\n",
      "class MRL_Linear_Layer(nn.Module):\n",
      "def __init__(self, nesting_list: List, num_classes=1000, efficient=\n",
      "False, **kwargs):\n",
      "super(MRL_Linear_Layer\\n\n",
      "Based on the context provided above, answer the following question. Do not use any external knowledge.\n",
      "If the answer is not present in the context, respond with \"I don't know.\"\n",
      "If the context is empty or irrelevant to the question, respond with \"No relevant information found in the context.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the full sy prompt comntent\n",
    "print(p_sys_ddic['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_user_ddic = pt.user_prompt(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Query:  What is Matryoshka Representation Learning? Act as a oriental story teller.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the full sy prompt comntent\n",
    "print(p_user_ddic['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': ' Context:\\noshka Representation Learning (MRL) to induce flexibility in the learned\\nrepresentation. MRL learns representations of varying capacities within the same high-dimensional\\nvector through explicit optimization of O(log(d)) lower-dimensional vectors in a nested fashion,\\nhence the name Matryoshka. MRL can be adapted to any existing representation pipeline and\\nis easily extended to many standard tasks in computer vision and natural language processing.\\nFigure 1 illustrates the core idea of Matryoshka Representation Learning (MRL) and the adaptive\\ndeployment settings of the learned Matryoshka Representations.\\nAdaptive Retrieval\\nShortlisting\\nRe-ranking\\nAdaptive Classification\\nTraining\\nInference\\n<latexit sha1_base64=\"eh9hk+peBkdsPY6v+r4rONmxYLY=\">A\\nB7nicbVBNSwMxEJ2tX7V+VT16CRbBU9kVoR6LXjxWsB/QLiWb\\\\n, and signif-\\nicantly for lower ones. This demonstrates that training to learn Matryoshka Representations\\nis feasible and extendable even for extremely large scale datasets.\\nWe also demonstrate that\\nMatryoshka Representations are learned at interpolated dimensions for both ALIGN and JFT-\\nViT, as shown in Table 5, despite not being trained explicitly at these dimensions. Lastly, Table 6\\nshows that MRL training leads to a increase in the cosine similarity span between positive and\\nrandom image-text pairs.\\nWe also evaluated the capability of Matryoshka Representations to extend to other natural language\\nprocessing via masked language modeling (MLM) with BERT [19], whose results are tabulated\\nin Table 7. Without any hyper-parameter tuning, we observed Matryoshka Representations to be\\nwithin 0.\\\\nib+53USE1x4KZdxYpiks0NBIrCJcFYB7nPFqBFjSwhV3GbFdEgUocYWVbQluPNfXiTN06rVN3bs3LtMq+jAIdwBVw4RxqcA1aAFB\\nc/wCm/oEb2gd/QxG1C+c4B/AH6/AGZEJHn</latexit>\\nFigure\\n1:\\nMatryoshka Representation Learning\\nis\\nadaptable to any representation learning setup and begets\\na Matryoshka Representation z\\nby optimizing the orig-\\ninal\\nloss\\nL(.)\\nat\\nO(log(d))\\nchosen\\nrepresentation\\nsizes.\\nMatryoshka Representation can be utilized effectively for adap-\\ntive deployment across environments and downstream tasks.\\nThe first m-dimensions, m ∈[d], of\\nthe Matryoshka Representation is\\nan information-rich low-dimensional\\nvector, at no additional training cost,\\nthat is as accurate as an indepen-\\ndently trained m-dimensional repre-\\nsentation.\\nThe information within\\nthe Matryoshka Representation in-\\ncreases with the dimensionality \\\\nerse set of ap-\\nplications along with an extensive evaluation of the learned multifidelity representations. Further,\\nwe showcase the downstream applications of the learned Matryoshka Representations for flexible\\nlarge-scale deployment through (a) Adaptive Classification (AC) and (b) Adaptive Retrieval (AR).\\n4.1\\nRepresentation Learning\\nWe adapt Matryoshka Representation Learning (MRL) to various representation learning setups\\n(a) Supervised learning for vision: ResNet50 [29] on ImageNet-1K [76] and ViT-B/16 [22] on\\nJFT-300M [85], (b) Contrastive learning for vision + language: ALIGN model with ViT-B/16 vision\\nencoder and BERT language encoder on ALIGN data [46] and (c) Masked language modelling:\\nBERT [19] on English Wikipedia and BooksCorpus [102]. Please refer to Appendices B and C for\\ndet\\\\n weight-tying across all the linear classifiers, i.e., by defining W(m) = W1:m\\nfor a set of common weights W ∈RL×d. This would reduce the memory cost due to the linear\\nclassifiers by almost half, which would be crucial in cases of extremely large output spaces [89, 99].\\nThis variant is called Efficient Matryoshka Representation Learning (MRL–E). Refer to Alg 1\\nand Alg 2 in Appendix A for the building blocks of Matryoshka Representation Learning (MRL).\\nAdaptation to Learning Frameworks.\\nMRL can be adapted seamlessly to most representation\\nlearning frameworks at web-scale with minimal modifications (Section 4.1). For example, MRL’s\\nadaptation to masked language modelling reduces to MRL–E due to the weight-tying between the\\ninput embedding matrix and the linear classifier. For contrastive lea\\\\nons (Section 4.1). For example, MRL’s\\nadaptation to masked language modelling reduces to MRL–E due to the weight-tying between the\\ninput embedding matrix and the linear classifier. For contrastive learning, both in context of vision &\\nvision + language, MRL is applied to both the embeddings that are being contrasted with each other.\\nThe presence of normalization on the representation needs to be handled independently for each of\\nthe nesting dimension for best results (see Appendix C for more details).\\n4\\nApplications\\nIn this section, we discuss Matryoshka Representation Learning (MRL) for a diverse set of ap-\\nplications along with an extensive evaluation of the learned multifidelity representations. Further,\\nwe showcase the downstream applications of the learned Matryoshka Representations f\\\\nMatryoshka Representation Learning\\nAditya Kusupati∗†⋄, Gantavya Bhatt∗†, Aniket Rege∗†,\\nMatthew Wallingford†, Aditya Sinha⋄, Vivek Ramanujan†, William Howard-Snyder†,\\nKaifeng Chen⋄, Sham Kakade‡, Prateek Jain⋄and Ali Farhadi†\\n†University of Washington, ⋄Google Research, ‡Harvard University\\n{kusupati,ali}@cs.washington.edu, prajain@google.com\\nAbstract\\nLearned representations are a central component in modern ML systems, serv-\\ning a multitude of downstream tasks. When training such representations, it\\nis often the case that computational and statistical constraints for each down-\\nstream task are unknown. In this context, rigid fixed-capacity representations\\ncan be either over or under-accommodating to the task at hand. This leads us\\nto ask: can we design a flexible representation that can ad\\\\nguage applications are built [40] on large language models [8] that are pretrained [68, 75]\\nin a un/self-supervised fashion with masked language modelling [19] or autoregressive training [70].\\nMatryoshka Representation Learning (MRL) is complementary to all these setups and can be\\nadapted with minimal overhead (Section 3). MRL equips representations with multifidelity at no\\nadditional cost which enables adaptive deployment based on the data and task (Section 4).\\nEfficient Classification and Retrieval.\\nEfficiency in classification and retrieval during inference\\ncan be studied with respect to the high yet constant deep featurization costs or the search cost which\\nscales with the size of the label space and data. Efficient neural networks address the first issue\\nthrough a variety of algorithm\\\\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\\narXiv:2205.13147v4  [cs.LG]  8 Feb 2024\\ndue to training/maintenance overhead, numerous expensive forward passes through all of the data,\\nstorage and memory cost for multiple copies of encoded data, expensive on-the-fly feature selection\\nor a significant drop in accuracy. By encoding coarse-to-fine-grained representations, which are as\\naccurate as the independently trained counterparts, we learn with minimal overhead a representation\\nthat can be deployed adaptively at no additional cost during inference.\\nWe introduce\\nMatryoshka Representation Learning (MRL) to induce flexibility in the learned\\nrepresentation. MRL learns representations of varying capacities within the same high-dimensional\\nvector through explicit optim\\\\nP7tBRJKQCk04VqDnFi7KZaEU7HpW6iaIzJEPdpx1CBQ6rcdJ/DPeNEsBeJM0TGk7UnxspDpUahb6ZzNKqWS8T/M6ie6duSkTcaKpINDvYRDHcGsDBgwSYnmI0MwkcxkhWSAJSbaVFYyJaDZL/8lzaMqcqro5rhcu\\n8jrKIJdsAcqAIFTUANXoA4agIAH8ARewKv1aD1b9b7dLRg5Tvb4Besj2/eCZSw</latexit>\\n<latexit sha1_base64=\"OPHM4ACsGr0VI7qMpDgoN+t2ICI=\">AB9XicbVDLSgMx\\nFL3xWeur6tJNsAh1U2ZE0GXRjQsXFewD2rFk0kwbmskMSUapQ/DjQtF3Pov7vwbM+0stPVA4HDOvdyT48eCa+M432hpeWV1b2wUdzc2t7ZLe3tN3WUKMoaNBKRavtEM8Elaxh\\nuBGvHipHQF6zlj64yv/XAlOaRvDPjmHkhGUgecEqMle67ITHDQJERvqk8nfRKZafqTIEXiZuTMuSo90pf3X5Ek5BJQwXRuM6sfFSogyngk2K3USzmNARGbCOpZKETHvpNPUEH1\\nulj4NI2ScNnq/N1ISaj0OfTuZpdTzXib+53USE1x4KZdxYpiks0NBIrCJcFYB7nPFqBFjSwhV3GbFdEgUocYWVbQluPNfXiTN06rVN3bs3LtMq+jAIdwBVw4RxqcA1aAFB\\nc/wCm/oEb2gd/QxG1C+c4B/AH6/AGZEJHn</latexit>\\nFigure\\n1:\\nMatryoshka Representation Learning\\nis\\nadapta\\\\nscuss the design choices in Section 4 for each of the representation learning settings.\\nFor the ease of exposition, we present the formulation for fully supervised representation learning\\nvia multi-class classification. Matryoshka Representation Learning modifies the typical setting\\nto become a multi-scale representation learning problem on the same task. For example, we train\\nResNet50 [29] on ImageNet-1K [76] which embeds a 224 × 224 pixel image into a d = 2048\\nrepresentation vector and then passed through a linear classifier to make a prediction, ˆy among the\\nL = 1000 labels. For MRL, we choose M = {8, 16, . . . , 1024, 2048} as the nesting dimensions.\\nSuppose we are given a labelled dataset D = {(x1, y1), . . . , (xN, yN)} where xi ∈X is an input\\npoint and yi ∈[L] is the label of xi for\\\\n unknown. In this context, rigid fixed-capacity representations\\ncan be either over or under-accommodating to the task at hand. This leads us\\nto ask: can we design a flexible representation that can adapt to multiple down-\\nstream tasks with varying computational resources? Our main contribution is\\nMatryoshka Representation Learning (MRL) which encodes information at\\ndifferent granularities and allows a single embedding to adapt to the computational\\nconstraints of downstream tasks. MRL minimally modifies existing representation\\nlearning pipelines and imposes no additional cost during inference and deployment.\\nMRL learns coarse-to-fine representations that are at least as accurate and rich as\\nindependently trained low-dimensional representations. The flexibility within the\\nlearned Matryoshka \\\\nleviates this extra compute with a\\nminimal drop in accuracy.\\nD.2\\nJFT, ALIGN and BERT\\nWe examine the k-NN classification accuracy of learned Matryoshka Representations via\\nALIGN–MRL and JFT-ViT–MRL in Table 4.\\nFor ALIGN [46], we observed that learning\\nMatryoshka Representations via ALIGN–MRL improved classification accuracy at nearly all\\ndimensions when compared to ALIGN. We observed a similar trend when training ViT-B/16 [22]\\nfor JFT-300M [85] classification, where learning Matryoshka Representations via MRL and\\nMRL–E on top of JFT-ViT improved classification accuracy for nearly all dimensions, and signif-\\nicantly for lower ones. This demonstrates that training to learn Matryoshka Representations\\nis feasible and extendable even for extremely large scale datasets.\\nWe also demonstrate that\\nM\\\\nf general purpose representations for computer vision [4, 98]. These representations\\nare typically learned through supervised and un/self-supervised learning paradigms. Supervised\\npretraining [29, 51, 82] casts representation learning as a multi-class/label classification problem,\\nwhile un/self-supervised learning learns representation via proxy tasks like instance classification [97]\\nand reconstruction [31, 63]. Recent advances [12, 30] in contrastive learning [27] enabled learning\\nfrom web-scale data [21] that powers large-capacity cross-modal models [18, 46, 71, 101]. Similarly,\\nnatural language applications are built [40] on large language models [8] that are pretrained [68, 75]\\nin a un/self-supervised fashion with masked language modelling [19] or autoregressive training [70].\\nMatryos\\\\nno additional training cost,\\nthat is as accurate as an indepen-\\ndently trained m-dimensional repre-\\nsentation.\\nThe information within\\nthe Matryoshka Representation in-\\ncreases with the dimensionality creat-\\ning a coarse-to-fine grained represen-\\ntation, all without significant training\\nor additional deployment overhead.\\nMRL equips the representation vector\\nwith the desired flexibility and multi-\\nfidelity that can ensure a near-optimal\\naccuracy-vs-compute trade-off. With\\nthese advantages, MRL enables adap-\\ntive deployment based on accuracy\\nand compute constraints.\\nThe Matryoshka Representations improve efficiency for large-scale classification and retrieval\\nwithout any significant loss of accuracy. While there are potentially several applications of coarse-to-\\nfine Matryoshka Representation\\\\n are as accu-\\nrate as independently trained counterparts without the multiple expensive forward passes.\\nMatryoshka Representations provide an intermediate abstraction between high-dimensional vec-\\ntors and their efficient ANNS indices through the adaptive embeddings nested within the original\\nrepresentation vector (Section 4). All other aforementioned efficiency techniques are complementary\\nand can be readily applied to the learned Matryoshka Representations obtained from MRL.\\nSeveral works in efficient neural network literature [9, 93, 100] aim at packing neural networks of\\nvarying capacity within the same larger network. However, the weights for each progressively smaller\\nnetwork can be different and often require distinct forward passes to isolate the final representations.\\nThis is detr\\\\ne whether MRL\\nis able to learn Matryoshka Representations at dimensions in between the representation size\\nfor which it was trained, we also tabulate the performance of MRL at interpolated Ds ∈\\n{12, 24, 48, 96, 192, 384, 768, 1536} as MRL–Interpolated and MRL–E–Interpolated (see Table 8).\\nWe observed that performance scaled nearly monotonically between the original representation\\n23\\nTable 6: Cosine similarity between embeddings\\nAvg. Cosine Similarity\\nALIGN\\nALIGN-MRL\\nPositive Text to Image\\n0.27\\n0.49\\nRandom Text to Image\\n8e-3\\n-4e-03\\nRandom Image to Image\\n0.10\\n0.08\\nRandom Text to Text\\n0.22\\n0.07\\nTable 7: Masked Language Modelling (MLM) accuracy(%) of FF and MRL models on the validation\\nset.\\nRep. Size\\nBERT-FF\\nBERT-MRL\\n12\\n60.12\\n59.92\\n24\\n62.49\\n62.05\\n48\\n63.85\\n63.40\\n96\\n64.32\\n64.15\\n192\\n64.70\\n64.58\\n3\\\\n\\nuse nested dropout in the context of autoencoders to learn nested representations. MRL differentiates\\nitself in formulation by optimizing only for O(log(d)) nesting dimensions instead of O(d). Despite\\nthis, MRL diffuses information to intermediate dimensions interpolating between the optimized\\nMatryoshka Representation sizes accurately (Figure 5); making web-scale feasible.\\n3\\nMatryoshka Representation Learning\\nFor d ∈N, consider a set M ⊂[d] of representation sizes. For a datapoint x in the input do-\\nmain X, our goal is to learn a d-dimensional representation vector z ∈Rd. For every m ∈M,\\n3\\nMatryoshka Representation Learning (MRL) enables each of the first m dimensions of the em-\\nbedding vector, z1:m ∈Rm to be independently capable of being a transferable and general purpose\\nrepresentatio\\\\no obtain flexible representa-\\ntions (Matryoshka Representations) for adaptive deployment (Section 3).\\n2. Up to 14× faster yet accurate large-scale classification and retrieval using MRL (Section 4).\\n3. Seamless adaptation of MRL across modalities (vision - ResNet & ViT, vision + language -\\nALIGN, language - BERT) and to web-scale data (ImageNet-1K/4K, JFT-300M and ALIGN data).\\n4. Further analysis of MRL’s representations in the context of other downstream tasks (Section 5).\\n2\\n2\\nRelated Work\\nRepresentation Learning.\\nLarge-scale datasets like ImageNet [16, 76] and JFT [85] enabled\\nthe learning of general purpose representations for computer vision [4, 98]. These representations\\nare typically learned through supervised and un/self-supervised learning paradigms. Supervised\\npretraining [29, 51,\\\\natryoshka Representation Learning (MRL) enables each of the first m dimensions of the em-\\nbedding vector, z1:m ∈Rm to be independently capable of being a transferable and general purpose\\nrepresentation of the datapoint x. We obtain z using a deep neural network F( · ; θF ): X →Rd\\nparameterized by learnable weights θF , i.e., z := F(x; θF ). The multi-granularity is captured through\\nthe set of the chosen dimensions M, that contains less than log(d) elements, i.e., |M| ≤⌊log(d)⌋.\\nThe usual set M consists of consistent halving until the representation size hits a low information\\nbottleneck. We discuss the design choices in Section 4 for each of the representation learning settings.\\nFor the ease of exposition, we present the formulation for fully supervised representation learning\\nvia multi-cl\\\\n2\\n60.48\\n61.71\\n28.51\\n28.45\\n28.85\\n3.00\\n3.55\\n3.59\\n21.70\\n20.38\\n21.77\\n32\\n74.68\\n74.80\\n75.26\\n62.24\\n62.23\\n63.05\\n31.28\\n30.79\\n31.47\\n2.60\\n3.65\\n3.57\\n22.03\\n21.87\\n22.48\\n64\\n75.45\\n75.48\\n76.17\\n63.51\\n63.15\\n63.99\\n32.96\\n32.13\\n33.39\\n2.87\\n3.99\\n3.76\\n22.13\\n22.56\\n23.43\\n128\\n75.47\\n76.05\\n76.46\\n63.67\\n63.52\\n64.69\\n33.93\\n33.48\\n34.54\\n2.81\\n3.71\\n3.73\\n22.73\\n22.73\\n23.70\\n256\\n75.78\\n76.31\\n76.66\\n64.13\\n63.80\\n64.71\\n34.80\\n33.91\\n34.85\\n2.77\\n3.65\\n3.60\\n22.63\\n22.88\\n23.59\\n512\\n76.30\\n76.48\\n76.82\\n64.11\\n64.09\\n64.78\\n35.53\\n34.20\\n34.97\\n2.37\\n3.57\\n3.59\\n23.41\\n22.89\\n23.67\\n1024\\n76.74\\n76.60\\n76.93\\n64.43\\n64.20\\n64.95\\n36.06\\n34.22\\n34.99\\n2.53\\n3.56\\n3.68\\n23.44\\n22.98\\n23.72\\n2048\\n77.10\\n76.65\\n76.95\\n64.69\\n64.17\\n64.93\\n37.10\\n34.29\\n35.07\\n2.93\\n3.49\\n3.59\\n24.05\\n23.01\\n23.70\\nMatryoshka Representation Learning-2048 dimensional model. This also showed that some in-\\nstances \\\\n equipped with retrieval capabilities that can bring forward every\\ninstance [7]. Approximate Nearest Neighbor Search (ANNS) [42] makes it feasible with efficient\\nindexing [14] and traversal [5, 6] to present the users with the most similar documents/images from\\nthe database for a requested query. Widely adopted HNSW [62] (O(d log(N))) is as accurate as\\nexact retrieval (O(dN)) at the cost of a graph-based index overhead for RAM and disk [44].\\nMRL tackles the linear dependence on embedding size,\\nd,\\nby learning multifidelity\\nMatryoshka Representations.\\nLower-dimensional Matryoshka Representations are as accu-\\nrate as independently trained counterparts without the multiple expensive forward passes.\\nMatryoshka Representations provide an intermediate abstraction between high-dimensional vec-\\ntor\\\\nrd)” had a clear visual distinction between the object and background and\\nthus predictions using 8 dimensions also led to a good inter-class separability within the superclass.\\n5.1\\nAblations\\nTable 26 in Appendix K presents that Matryoshka Representations can be enabled within off-the-\\nshelf pretrained models with inexpensive partial finetuning thus paving a way for ubiquitous adoption\\nof MRL. At the same time, Table 27 in Appendix C indicates that with optimal weighting of the\\nnested losses we could improve accuracy of lower-dimensions representations without accuracy\\nloss. Tables 28 and 29 in Appendix C ablate over the choice of initial granularity and spacing of the\\ngranularites. Table 28 reaffirms the design choice to shun extremely low dimensions that have poor\\nclassification accuracy \\\\nght decay of 1e-4.\\nOur code (Appendix A) makes minimal modifications to the training pipeline provided by FFCV to\\nlearn Matryoshka Representations.\\nWe trained ViT-B/16 models for JFT-300M on a 8x8 cloud TPU pod [49] using Tensorflow [1] with a\\nbatchsize of 128 and trained for 300K steps. Similarly, ALIGN models were trained using Tensorflow\\non 8x8 cloud TPU pod for 1M steps with a batchsize of 64 per TPU. Both these models were trained\\nwith adafactor optimizer [81] with a linear learning rate decay starting at 1e-3.\\nLastly, we trained a BERT-Base model on English Wikipedia and BookCorpus. We trained our models\\nin Tensorflow using a 4x4 cloud TPU pod with a total batchsize of 1024. We used AdamW [61]\\noptimizer with a linear learning rate decay starting at 1e-4 and trained for 450K steps.\\nIn\\\\nired semantic information for coarser classification that could be leveraged for adaptive routing\\nfor retrieval and classification. Mutifidelity of Matryoshka Representation naturally captures the\\nunderlying hierarchy of the class labels with one single model. Lastly, Figure 11 showcases the\\naccuracy trends per superclass with MRL. The utility of additional dimensions in distinguishing\\na class from others within the same superclass is evident for “garment” which has up to 11%\\nimprovement for 8 →16 dimensional representation transition. We also observed that superclasses\\nsuch as “oscine (songbird)” had a clear visual distinction between the object and background and\\nthus predictions using 8 dimensions also led to a good inter-class separability within the superclass.\\n5.1\\nAblations\\nTable 26 \\\\nation (MRL–AC)\\n. . . . . . . . . . . . . . . . . . . . . . . . .\\n21\\nD.2\\nJFT, ALIGN and BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n22\\nE\\nImage Retrieval\\n22\\nF\\nAdaptive Retrieval\\n24\\nG Few-shot and Sample Efficiency\\n25\\nH Robustness Experiments\\n27\\nI\\nIn Practice Costs\\n27\\nJ\\nAnalysis of Model Disagreement\\n29\\nK Ablation Studies\\n32\\nK.1\\nMRL Training Paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n32\\nK.2\\nRetrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n33\\n18\\nA\\nCode for Matryoshka Representation Learning\\n(MRL)\\nWe use Alg 1 and 2 provided below to train supervised ResNet50–MRL models on ImageNet-1K.\\nWe provide this code as a template to extend MRL to any domain.\\nAlgorithm 1 Pytorch code for Matryoshka Cross-Entropy \\\\ndaptive Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n4.3\\nRetrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n4.3.1\\nAdaptive Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n7\\n5\\nFurther Analysis and Ablations\\n8\\n5.1\\nAblations\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n10\\n6\\nDiscussion and Conclusions\\n10\\nA Code for Matryoshka Representation Learning\\n(MRL)\\n19\\nB\\nDatasets\\n20\\nC Matryoshka Representation Learning Model Training\\n20\\nD Classification Results\\n21\\nD.1 Adaptive Classification (MRL–AC)\\n. . . . . . . . . . . . . . . . . . . . . . . . .\\n21\\nD.2\\nJFT, ALIGN and BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n22\\nE\\nImage Retrieval\\n22\\nF\\nAdaptive Retrieval\\\\nanguage\\nprocessing via masked language modeling (MLM) with BERT [19], whose results are tabulated\\nin Table 7. Without any hyper-parameter tuning, we observed Matryoshka Representations to be\\nwithin 0.5% of FF representations for BERT MLM validation accuracy. This is a promising initial\\nresult that could help with large-scale adaptive document retrieval using BERT–MRL.\\nE\\nImage Retrieval\\nWe evaluated the strength of Matryoshka Representations via image retrieval on ImageNet-1K (the\\ntraining distribution), as well as on out-of-domain datasets ImageNetV2 and ImageNet-4K for all\\n22\\nTable 4: ViT-B/16 and ViT-B/16-MRL top-1 and top-5 k-NN accuracy (%) for ALIGN and JFT. Top-1\\nentries where MRL–E and MRL outperform baselines are bolded for both ALIGN and JFT-ViT.\\nRep. Size\\nALIGN\\nALIGN-MRL\\nJFT-ViT\\n\\\\nentation to enable dataset and representation aware retrieval. (4) Finally, the\\njoint optimization of multi-objective MRL combined with end-to-end learnable search data-structure\\nto have data-driven adaptive large-scale retrieval for web-scale search applications.\\nIn conclusion, we presented\\nMatryoshka Representation Learning (MRL), a flexible represen-\\ntation learning approach that encodes information at multiple granularities in a single embedding\\nvector. This enables the MRL to adapt to a downstream task’s statistical complexity as well as\\nthe available compute resources. We demonstrate that MRL can be used for large-scale adaptive\\nclassification as well as adaptive retrieval. On standard benchmarks, MRL matches the accuracy of\\nthe fixed-feature baseline despite using 14× smaller repres\\\\n,\\n(1)\\nwhere L: RL × [L] →R+ is the multi-class softmax cross-entropy loss function. This is a standard\\noptimization problem that can be solved using sub-gradient descent methods. We set all the impor-\\ntance scales, cm = 1 for all m ∈M; see Section 5 for ablations. Lastly, despite only optimizing\\nfor O(log(d)) nested dimensions, MRL results in accurate representations, that interpolate, for\\ndimensions that fall between the chosen granularity of the representations (Section 4.2).\\nWe call this formulation as Matryoshka Representation Learning (MRL). A natural way to make\\nthis efficient is through weight-tying across all the linear classifiers, i.e., by defining W(m) = W1:m\\nfor a set of common weights W ∈RL×d. This would reduce the memory cost due to the linear\\nclassifiers by almost half, whic\\\\ne distribution, without sacrificing accuracy on other classes (Table 16 in\\nAppendix G). Additionally we find the accuracy between low-dimensional and high-dimensional\\nrepresentations is marginal for pretrain classes. We hypothesize that the higher-dimensional represen-\\ntations are required to differentiate the classes when few training examples of each are known. This\\nresults provides further evidence that different tasks require varying capacity based on their difficulty.\\nDisagreement across Dimensions.\\nThe information packing in Matryoshka Representations\\noften results in gradual increase of accuracy with increase in capacity. However, we observed that\\n8\\n(a)\\n(b)\\n(c)\\nFigure 9: Grad-CAM [80] progression of predictions in MRL model across 8, 16, 32 and 2048\\ndimensions. (a) 8-dimensional rep\\\\n.98\\n23.72\\n2048\\n77.10\\n76.65\\n76.95\\n64.69\\n64.17\\n64.93\\n37.10\\n34.29\\n35.07\\n2.93\\n3.49\\n3.59\\n24.05\\n23.01\\n23.70\\nMatryoshka Representation Learning-2048 dimensional model. This also showed that some in-\\nstances and classes could benefit from lower-dimensional representations.\\nDiscussion of Oracle Accuracy\\nBased on our observed model disagreements for different rep-\\nresentation sizes d, we defined an optimal oracle accuracy [58] for MRL. We labeled an image as\\ncorrectly predicted if classification using any representation size was correct. The percentage of\\ntotal samples of ImageNet-1K that were firstly correctly predicted using each representation size d is\\nshown in Table 22. This defined an upper bound on the performance of MRL models, as 18.46%\\nof the ImageNet-1K validation set were incorrectly pre\\\\nnts and screenshots, if\\napplicable? [N/A]\\n(b) Did you describe any potential participant risks, with links to Institutional Review\\nBoard (IRB) approvals, if applicable? [N/A]\\n(c) Did you include the estimated hourly wage paid to participants and the total amount\\nspent on participant compensation? [N/A]\\n17\\nContents\\n1\\nIntroduction\\n1\\n2\\nRelated Work\\n3\\n3\\nMatryoshka Representation Learning\\n3\\n4\\nApplications\\n4\\n4.1\\nRepresentation Learning\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n4\\n4.2\\nClassification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n5\\n4.2.1\\nAdaptive Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n4.3\\nRetrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n6\\n4.3.1\\nAdaptive Retrieva\\\\ndim. Similarly, Figure 3\\nshowcases the comparison of learned representation quality through 1-NN accuracy on ImageNet-1K\\n(trainset with 1.3M samples as the database and validation set with 50K samples as the queries).\\nMatryoshka Representations are up to 2% more accurate than their fixed-feature counterparts for\\nthe lower-dimensions while being as accurate elsewhere. 1-NN accuracy is an excellent proxy, at no\\nadditional training cost, to gauge the utility of learned representations in the downstream tasks.\\nWe also evaluate the quality of the representations from training ViT-B/16 on JFT-300M alongside the\\nViT-B/16 vision encoder of the ALIGN model – two web-scale setups. Due to the expensive nature of\\nthese experiments, we only train the highest capacity fixed feature model and choose rand\\\\nectation using the distribution of representation\\nsizes. As shown in Table 3 and Figure 6, we observed that in expectation, we only needed a ∼37\\nsized representation to achieve 76.3% classification accuracy on ImageNet-1K, which was roughly\\n14× smaller than the FF–512 baseline. Even if we computed the expectation as a weighted average\\nover the cumulative sum of representation sizes {8, 24, 56, . . .}, due to the nature of multiple linear\\nheads for MRL, we ended up with an expected size of 62 that still provided a roughly 8.2× efficient\\nrepresentation than the FF–512 baseline. However, MRL–E alleviates this extra compute with a\\nminimal drop in accuracy.\\nD.2\\nJFT, ALIGN and BERT\\nWe examine the k-NN classification accuracy of learned Matryoshka Representations via\\nALIGN–MRL and JFT-ViT–MRL in \\\\n retrieval (Section 4.3.1). Finally, as MRL explicitly\\nlearns coarse-to-fine representation vectors, intuitively it should share more semantic information\\namong its various dimensions (Figure 5). This is reflected in up to 2% accuracy gains in long-tail\\ncontinual learning settings while being as robust as the original embeddings. Furthermore, due to its\\ncoarse-to-fine grained nature, MRL can also be used as method to analyze hardness of classification\\namong instances and information bottlenecks.\\nWe make the following key contributions:\\n1. We introduce\\nMatryoshka Representation Learning (MRL) to obtain flexible representa-\\ntions (Matryoshka Representations) for adaptive deployment (Section 3).\\n2. Up to 14× faster yet accurate large-scale classification and retrieval using MRL (Section 4).\\n3\\\\nshows that MRL also\\nimproves the cosine similarity span between positive and random image-text pairs.\\nFew-shot and Long-tail Learning.\\nWe exhaustively evaluated few-shot learning on MRL models\\nusing nearest class mean [79]. Table 15 in Appendix G shows that that representations learned\\nthrough MRL perform comparably to FF representations across varying shots and number of classes.\\nMatryoshka Representations realize a unique pattern while evaluating on FLUID [92], a long-tail\\nsequential learning framework. We observed that MRL provides up to 2% accuracy higher on novel\\nclasses in the tail of the distribution, without sacrificing accuracy on other classes (Table 16 in\\nAppendix G). Additionally we find the accuracy between low-dimensional and high-dimensional\\nrepresentations is marginal for p\\\\n8\\nFF\\n86.40\\n37.09\\n71.74\\n10.77\\n37.04\\n52.67\\nMRL\\n85.60\\n36.83\\n70.34\\n12.88\\n37.46\\n52.18\\nMRL–E\\n83.01\\n29.99\\n65.37\\n7.60\\n31.97\\n47.16\\nTable 17: Top-1 classification accuracy (%) on out-of-domain datasets (ImageNet-V2/R/A/Sketch) to\\nexamine robustness of Matryoshka Representation Learning. Note that these results are without\\nany fine tuning on these datasets.\\nImageNet-V1\\nImageNet-V2\\nImageNet-R\\nImageNet-A\\nImageNet-Sketch\\nRep. Size\\nFF\\nMRL–E\\nMRL\\nFF\\nMRL–E\\nMRL\\nFF\\nMRL–E\\nMRL\\nFF\\nMRL–E\\nMRL\\nFF\\nMRL–E\\nMRL\\n8\\n65.86\\n56.92\\n67.46\\n54.05\\n47.40\\n55.59\\n24.60\\n22.98\\n23.57\\n2.92\\n3.63\\n3.39\\n17.73\\n15.07\\n17.98\\n16\\n73.10\\n72.38\\n73.80\\n60.52\\n60.48\\n61.71\\n28.51\\n28.45\\n28.85\\n3.00\\n3.55\\n3.59\\n21.70\\n20.38\\n21.77\\n32\\n74.68\\n74.80\\n75.26\\n62.24\\n62.23\\n63.05\\n31.28\\n30.79\\n31.47\\n2.60\\n3.65\\n3.57\\n22.03\\n21.87\\n22.48\\n64\\n75.45\\n75.48\\n76.17\\n63.51\\n63.15\\n63.99\\n32.96\\n\\\\nall 1,000 ImageNet-1K classes.\\nObjectNet [2] contains 50K images across 313 object classes, each containing ∼160 images each.\\nC\\nMatryoshka Representation Learning Model Training\\nWe trained all ResNet50–MRL models using the efficient dataloaders of FFCV [56]. We utilized the\\nrn50_40_epochs.yaml configuration file of FFCV to train all MRL models defined below:\\n• MRL: ResNet50 model with the fc layer replaced by MRL_Linear_Layer(efficient=False)\\n• MRL–E: ResNet50 model with the fc layer replaced by MRL_Linear_Layer(efficient=True)\\n• FF–k: ResNet50 model with the fc layer replaced by torch.nn.Linear(k, num_classes),\\nwhere k ∈[8, 16, 32, 64, 128, 256, 512, 1024, 2048]. We will henceforth refer to these models as\\nsimply FF, with the k value denoting representation size.\\nWe trained all ResNet50 m\\\\necognition, pages\\n11162–11173, 2021.\\n[19] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional\\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n[20] T. G. Dietterich and G. Bakiri. Solving multiclass learning problems via error-correcting\\noutput codes. Journal of artificial intelligence research, 2:263–286, 1994.\\n[21] S. K. Divvala, A. Farhadi, and C. Guestrin. Learning everything about anything: Webly-\\nsupervised visual concept learning. In Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition, pages 3270–3277, 2014.\\n[22] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. De-\\nhghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transfor\\\\nyers can be found\\nin the ResNet50 architecture [29]. All models were finetuned with the FFCV pipeline, with same\\ntraining configuration as in the end-to-end training aside from changing lr = 0.1 and epochs = 10. We\\nobserved that finetuning the linear layer alone was insufficient to learn Matryoshka Representations\\nat lower dimensionalities. Adding more and more non-linear conv+ReLU layers steadily improved\\nclassification accuracy of d = 8 from 5% to 60% after finetuning, which was only 6% less than\\ntraining MRL end-to-end for 40 epochs. This difference was successively less pronounced as we\\nincreased dimensionality past d = 64, to within 1.5% for all larger dimensionalities. The full results\\nof this ablation can be seen in Table 26.\\nRelative Importance.\\nWe performed an ablation of MRL over\\\\nclear\\ntrend. When we repeated this experiment with independently trained FF models, we noticed that 950\\nclasses did not show a clear trend. This motivated us to leverage the disagreement as well as gradual\\nimprovement of accuracy at different representation sizes by training Matryoshka Representations.\\nFigure 12 showcases the progression of relative per-class accuracy distribution compared to the\\n29\\nTable 16: Accuracy (%) categories indicates whether classes were present during ImageNet pretraining\\nand head/tail indicates classes that have greater/less than 50 examples in the streaming test set. We\\nobserved that MRL performed better than the baseline on novel tail classes by ∼2% on average.\\nRep. Size\\nMethod\\nPretrain\\n- Head (>50)\\nNovel\\n- Head (>50)\\nPretrain\\n- Tail (<50)\\nNovel\\n- Tail (<50)\\nM\\\\ns.\\nTable 25 quantifies the performance with different representation size.\\nK\\nAblation Studies\\nK.1\\nMRL Training Paradigm\\nMatryoshka Representations via Finetuning.\\nTo observe if nesting can be induced in models that\\nwere not explicitly trained with nesting from scratch, we loaded a pretrained FF-2048 ResNet50 model\\nand initialized a new MRL layer, as defined in Algorithm 2, Appendix C. We then unfroze different\\nlayers of the backbone to observe how much non-linearity in the form of unfrozen conv layers needed\\nto be present to enforce nesting into a pretrained FF model. A description of these layers can be found\\nin the ResNet50 architecture [29]. All models were finetuned with the FFCV pipeline, with same\\ntraining configuration as in the end-to-end training aside from changing lr = 0.1 and e\\\\nasun, A. Torralba, and S. Fidler. Aligning\\nbooks and movies: Towards story-like visual explanations by watching movies and reading\\nbooks. In Proceedings of the IEEE international conference on computer vision, pages 19–27,\\n2015.\\n16\\nChecklist\\n1. For all authors...\\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s\\ncontributions and scope? [Yes]\\n(b) Did you describe the limitations of your work? [Yes] See Section 6\\n(c) Did you discuss any potential negative societal impacts of your work? [N/A] Our work\\ndoes not have any additional negative societal impact on top of the existing impact of\\nrepresentation learning. However, a study on the trade-off between representation size\\nand the tendency to encode biases is an interesting future direction along the \\\\n68\\n70.14\\n69.54\\n69.01\\n68.41\\n2048\\n70.98\\n65.20\\n63.57\\n62.56\\n61.60\\n70.18\\n69.52\\n68.98\\n68.35\\n1024\\n2048\\n1312\\n70.97\\n65.20\\n63.57\\n62.56\\n61.60\\n70.18\\n69.52\\n68.98\\n68.35\\nThese results provide further evidence that different tasks require varying capacity based on their\\ndifficulty.\\nH\\nRobustness Experiments\\nWe evaluated the robustness of MRL models on out-of-domain datasets (ImageNetV2/R/A/Sketch)\\nand compared them to the FF baseline. Each of these datasets is described in Appendix B. The\\nresults in Table 17 demonstrate that learning Matryoshka Representations does not hurt out-of-\\ndomain generalization relative to FF models, and Matryoshka Representations in fact improve\\nthe performance on ImageNet-A. For a ALIGN–MRL model, we examine the the robustness via\\nzero-shot retrieval on out-of-domain datasets, i\\\\nties without the\\nadditional expense of multiple model forward passes for the web-scale databases. FF models\\nalso generate independent databases which become prohibitively expense to store and switch in\\nbetween. Matryoshka Representations enable adaptive retrieval (AR) which alleviates the need\\nto use full-capacity representations, d = 2048, for all data and downstream tasks. Lastly, all the\\nvector compression techniques [60, 45] used as part of the ANNS pipelines are complimentary to\\nMatryoshka Representations and can further improve the efficiency-vs-accuracy trade-off.\\n4.3.1\\nAdaptive Retrieval\\nWe benchmark MRL in the adaptive retrieval setting (AR) [50]. For a given query image, we obtained\\na shortlist, K = 200, of images from the database using a lower-dimensional representation, e.g.\\nD\\\\n, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,\\nA. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International\\njournal of computer vision, 115(3):211–252, 2015.\\n[77] R. Salakhutdinov and G. Hinton. Learning a nonlinear embedding by preserving class neigh-\\nbourhood structure. In Artificial Intelligence and Statistics, pages 412–419. PMLR, 2007.\\n[78] R. Salakhutdinov and G. Hinton. Semantic hashing. International Journal of Approximate\\nReasoning, 50(7):969–978, 2009.\\n[79] J. S. Sánchez, F. Pla, and F. J. Ferri. On the use of neighbourhood-based non-parametric\\nclassifiers. Pattern Recognition Letters, 18(11-13):1179–1186, 1997.\\n[80] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-cam:\\nVisual explanation\\\\nar(k, num_classes),\\nwhere k ∈[8, 16, 32, 64, 128, 256, 512, 1024, 2048]. We will henceforth refer to these models as\\nsimply FF, with the k value denoting representation size.\\nWe trained all ResNet50 models with a learning rate of 0.475 with a cyclic learning rate schedule [83].\\nThis was after appropriate scaling (0.25×) of the learning rate specified in the configuration file to\\naccommodate for 2xA100 NVIDIA GPUs available for training, compared to the 8xA100 GPUs\\nutilized in the FFCV benchmarks. We trained with a batch size of 256 per GPU, momentum [86] of\\n0.9, and an SGD optimizer with a weight decay of 1e-4.\\nOur code (Appendix A) makes minimal modifications to the training pipeline provided by FFCV to\\nlearn Matryoshka Representations.\\nWe trained ViT-B/16 models for JFT-300M on a 8x8 clo\\\\nsentation size for both top-1 and mAP@10, and especially\\nat low representation size (Ds ≤32). MRL–E loses out to FF significantly only at Ds = 8. This\\nindicates that training ResNet50 models via the MRL training paradigm improves retrieval at low\\nrepresentation size over models explicitly trained at those representation size (FF-8...2048).\\nWe carried out all retrieval experiments at Ds ∈{8, 16, 32, 64, 128, 256, 512, 1024, 2048}, as\\nthese were the representation sizes which were a part of the nesting_list at which losses\\nwere added during training, as seen in Algorithm 1, Appendix A. To examine whether MRL\\nis able to learn Matryoshka Representations at dimensions in between the representation size\\nfor which it was trained, we also tabulate the performance of MRL at interpolated Ds ∈\\n{12, 2\\\\nce of MRL model on 31-way classification (1 extra class is for reject token) on\\nImageNet-1K superclasses.\\nRep. Size\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\nMRL\\n85.57\\n88.67\\n89.48\\n89.82\\n89.97\\n90.11\\n90.18\\n90.22\\n90.21\\nMatryoshka Representations at Arbitrary Granularities.\\nTo train MRL, we used nested di-\\nmensions at logarithmic granularities M = {8, 16, . . . , 1024, 2048} as detailed in Section 3. We\\nmade this choice for two empirically-driven reasons: a) The accuracy improvement with increasing\\nrepresentation size was more logarithmic than linear (as shown by FF models in Figure 2). This indi-\\ncated that optimizing for granularities increasing in a non-logarithmic fashion would be sub-optimal\\nboth for maximum performance and expected efficiency; b) If we have m arbitrary granularities,\\nthe expected\\\\nat optimizing for granularities increasing in a non-logarithmic fashion would be sub-optimal\\nboth for maximum performance and expected efficiency; b) If we have m arbitrary granularities,\\nthe expected cost of the linear classifier to train MRL scales as O(L ∗(m2)) while logarithmic\\ngranularities result in O(L ∗2log(d)) space and compute costs.\\nTo demonstrate this effect, we learned Matryoshka Representations with uniform (MRL-Uniform)\\nnesting dimensions m\\n∈\\nM\\n=\\n{8, 212, 416, 620, 824, 1028, 1232, 1436, 1640, 1844, 2048}.\\nWe\\nevaluated\\nthis\\nmodel\\nat\\nthe\\nstandard\\n(MRL-log)\\ndimensions\\nm\\n∈\\nM\\n=\\n{8, 16, 32, 64, 128, 256, 512, 1024, 2048} for ease of comparison to reported numbers using 1-NN ac-\\ncuracy (%). As shown in Table 29, we observed that while performance interpolated, MRL-Uniform\\nsuffered\\\\nable 7: Masked Language Modelling (MLM) accuracy(%) of FF and MRL models on the validation\\nset.\\nRep. Size\\nBERT-FF\\nBERT-MRL\\n12\\n60.12\\n59.92\\n24\\n62.49\\n62.05\\n48\\n63.85\\n63.40\\n96\\n64.32\\n64.15\\n192\\n64.70\\n64.58\\n384\\n65.03\\n64.81\\n768\\n65.54\\n65.00\\nsize and the interpolated representation size as we increase Ds, which demonstrates that MRL is\\nable to learn Matryoshka Representations at nearly all representation size m ∈[8, 2048] despite\\noptimizing only for |M| nested representation sizes.\\nWe examined the robustness of MRL for retrieval on out-of-domain datasets ImageNetV2 and\\nImageNet-4K, as shown in Table 9 and Table 10 respectively. On ImageNetV2, we observed that MRL\\noutperformed FF at all Ds on top-1 Accuracy and mAP@10, and MRL–E outperformed FF at all\\nDs except Ds = 8. This demonstrates the robustness\\\\nnd deployment.\\nMRL learns coarse-to-fine representations that are at least as accurate and rich as\\nindependently trained low-dimensional representations. The flexibility within the\\nlearned Matryoshka Representations offer: (a) up to 14× smaller embedding\\nsize for ImageNet-1K classification at the same level of accuracy; (b) up to 14×\\nreal-world speed-ups for large-scale retrieval on ImageNet-1K and 4K; and (c) up\\nto 2% accuracy improvements for long-tail few-shot classification, all while being\\nas robust as the original representations. Finally, we show that MRL extends seam-\\nlessly to web-scale datasets (ImageNet, JFT) across various modalities – vision\\n(ViT, ResNet), vision + language (ALIGN) and language (BERT). MRL code and\\npretrained models are open-sourced at https://github.com/RAIVN\\\\nd Table 10 respectively. On ImageNetV2, we observed that MRL\\noutperformed FF at all Ds on top-1 Accuracy and mAP@10, and MRL–E outperformed FF at all\\nDs except Ds = 8. This demonstrates the robustness of the learned Matryoshka Representations\\nfor out-of-domain image retrieval.\\nF\\nAdaptive Retrieval\\nThe time complexity of retrieving a shortlist of k-NN often scales as O(d), where d =Ds, for a\\nfixed k and N. We thus will have a theoretical 256× higher cost for Ds = 2048 over Ds = 8. We\\ndiscuss search complexity in more detail in Appendix I. In an attempt to replicate performance at\\nhigher Ds while using less FLOPs, we perform adaptive retrieval via retrieving a k-NN shortlist with\\nrepresentation size Ds, and then re-ranking the shortlist with representations of size Dr. Adaptive\\nretrieval for\\\\n improve efficiency for large-scale classification and retrieval\\nwithout any significant loss of accuracy. While there are potentially several applications of coarse-to-\\nfine Matryoshka Representations, in this work we focus on two key building blocks of real-world\\nML systems: large-scale classification and retrieval. For classification, we use adaptive cascades with\\nthe variable-size representations from a model trained with MRL, significantly reducing the average\\ndimension of embeddings needed to achieve a particular accuracy. For example, on ImageNet-1K,\\nMRL + adaptive classification results in up to a 14× smaller representation size at the same accuracy\\nas baselines (Section 4.2.1). Similarly, we use MRL in an adaptive retrieval system. Given a query,\\nwe shortlist retrieval candidates \\\\nt). Every combination of Ds & Dr falls above the Pareto\\nline (orange dots) of single-shot retrieval with a fixed representation size while having configurations\\nthat are as accurate while being up to 14× faster in real-world deployment. Funnel retrieval is almost\\nas accurate as the baseline while alleviating some of the parameter choices of Adaptive Retrieval.\\npoint, further strengthening the use-case for Matryoshka Representation Learning and adaptive\\nretrieval.\\nEven with adaptive retrieval, it is hard to determine the choice of Ds & Dr. In order to alleviate this\\nissue to an extent, we propose Funnel Retrieval, a consistent cascade for adaptive retrieval. Funnel\\nthins out the initial shortlist by a repeated re-ranking and shortlisting with a series of increasing\\ncapacity representations.\\\\nion size for MRL &\\nFF models showing the capture of underlying\\nhierarchy through tight information bottlenecks.\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\nRepresentation Size\\n65\\n70\\n75\\n80\\n85\\n90\\n95\\nTop-1 Accuracy (%)\\nmeasuring device\\nbuilding\\ngarment\\ntool\\nnourishment\\nprotective covering\\nvessel\\noscine\\nFigure 11:\\nDiverse per-superclass accuracy\\ntrends across representation sizes for ResNet50-\\nMRL on ImageNet-1K.\\n9\\noccurs with both MRL and FF models; MRL is more accurate across dimensions. This shows that\\ntight information bottlenecks while not highly accurate for fine-grained classification, do capture\\nrequired semantic information for coarser classification that could be leveraged for adaptive routing\\nfor retrieval and classification. Mutifidelity of Matryoshka Representation naturally captures the\\nund\\\\nYang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig.\\nScaling up visual and vision-language representation learning with noisy text supervision. In\\nInternational Conference on Machine Learning, pages 4904–4916. PMLR, 2021.\\n[47] J. Johnson, M. Douze, and H. Jégou. Billion-scale similarity search with GPUs. IEEE\\nTransactions on Big Data, 7(3):535–547, 2019.\\n[48] W. B. Johnson. Extensions of lipschitz mappings into a hilbert space. Contemp. Math., 26:\\n189–206, 1984.\\n[49] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, S. Bates, S. Bhatia,\\nN. Boden, A. Borchers, et al. In-datacenter performance analysis of a tensor processing unit.\\nIn Proceedings of the 44th annual international symposium on computer architecture, pages\\n1–12, 2017.\\n[50] T.\\\\n accuracy with increase in capacity. However, we observed that\\n8\\n(a)\\n(b)\\n(c)\\nFigure 9: Grad-CAM [80] progression of predictions in MRL model across 8, 16, 32 and 2048\\ndimensions. (a) 8-dimensional representation confuses due to presence of other relevant objects (with\\na larger field of view) in the scene and predicts “shower cap” ; (b) 8-dim model confuses within\\nthe same super-class of “boa” ; (c) 8 and 16-dim models incorrectly focus on the eyes of the doll\\n(\"sunglasses\") and not the \"sweatshirt\" which is correctly in focus at higher dimensions; MRL fails\\ngracefully in these scenarios and shows potential use cases of disagreement across dimensions.\\nthis trend was not ubiquitous and certain instances and classes were more accurate when evaluated\\nwith lower-dimensions (Figure 12 in Appendi\\\\nConference on Machine Learning, pages 5389–5400. PMLR,\\n2019.\\n[73] O. Rippel, M. Gelbart, and R. Adams. Learning ordered representations with nested dropout.\\nIn International Conference on Machine Learning, pages 1746–1754. PMLR, 2014.\\n[74] J. Rissanen. Modeling by shortest data description. Automatica, 14(5):465–471, 1978.\\n[75] S. Ruder, M. E. Peters, S. Swayamdipta, and T. Wolf. Transfer learning in natural language\\nprocessing. In Proceedings of the 2019 conference of the North American chapter of the\\nassociation for computational linguistics: Tutorials, pages 15–18, 2019.\\n[76] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,\\nA. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International\\njournal of computer vision, 115\\\\nows potential use cases of disagreement across dimensions.\\nthis trend was not ubiquitous and certain instances and classes were more accurate when evaluated\\nwith lower-dimensions (Figure 12 in Appendix J). With perfect routing of instances to appropriate\\ndimension, MRL can gain up to 4.6% classification accuracy. At the same time, the low-dimensional\\nmodels are less accurate either due to confusion within the same superclass [24] of the ImageNet\\nhierarchy or presence of multiple objects of interest. Figure 9 showcases 2 such examples for 8-\\ndimensional representation. These results along with Appendix J put forward the potential for MRL\\nto be a systematic framework for analyzing the utility and efficiency of information bottlenecks.\\nSuperclass Accuracy.\\nAs the information bottleneck become\\\\nch\\nusing hierarchical navigable small world graphs. IEEE transactions on pattern analysis and\\nmachine intelligence, 42(4):824–836, 2018.\\n[63] J. Masci, U. Meier, D. Cire¸san, and J. Schmidhuber. Stacked convolutional auto-encoders for\\nhierarchical feature extraction. In International conference on artificial neural networks, pages\\n52–59. Springer, 2011.\\n[64] P. Mitra, C. Murthy, and S. K. Pal. Unsupervised feature selection using feature similarity.\\nIEEE transactions on pattern analysis and machine intelligence, 24(3):301–312, 2002.\\n[65] V. Nanda, T. Speicher, J. P. Dickerson, S. Feizi, K. P. Gummadi, and A. Weller. Diffused\\nredundancy in pre-trained representations. arXiv preprint arXiv:2306.00183, 2023.\\n[66] P. Nayak. Understanding searches better than ever before. Google AI Blog, 2019. \\\\nny\\nrepresentation size. The remaining 81.54% constitutes the oracle accuracy.\\nRep. Size\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\nAlways\\nWrong\\nCorrectly\\nPredicted\\n67.46\\n8.78\\n2.58\\n1.35\\n0.64\\n0.31\\n0.20\\n0.12\\n0.06\\n18.46\\nof disagreement arising when the models got confused within the same superclass. For example,\\nImageNet-1K has multiple \"snake\" classes, and models often confuse a snake image for an incorrect\\nspecies of snake.\\nSuperclass Performance\\nWe created a 30 superclass subset of the validation set based on wordnet\\nhierarchy (Table 24) to quantify the performance of MRL model on ImageNet-1K superclasses.\\nTable 25 quantifies the performance with different representation size.\\nK\\nAblation Studies\\nK.1\\nMRL Training Paradigm\\nMatryoshka Representations via Finetuning.\\nTo observe if nesting can be induced \\\\n Ds = 64 on ImageNet-1K and ImageNet-4K, at 32× less\\nMFLOPs. This demonstrates the value of intelligent routing techniques which utilize appropriately\\nsized Matryoshka Representations for retrieval.\\n24\\nTable 8: Retrieve a shortlist of 200-NN with Ds sized representations on ImageNet-1K via exact\\nsearch with L2 distance metric. Top-1 and mAP@10 entries (%) where MRL–E and MRL outperform\\nFF at their respective representation sizes are bolded.\\nModel\\nDs\\nMFlops\\nTop-1\\nTop-5\\nTop-10\\nmAP@10\\nmAP@25\\nmAP@50\\nmAP@100\\nP@10\\nP@25\\nP@50\\nP@100\\nFF\\n8\\n10\\n58.93\\n75.76\\n80.25\\n53.42\\n52.29\\n51.84\\n51.57\\n59.32\\n59.28\\n59.25\\n59.21\\n16\\n20\\n66.77\\n80.88\\n84.40\\n61.63\\n60.51\\n59.98\\n59.62\\n66.76\\n66.58\\n66.43\\n66.27\\n32\\n41\\n68.84\\n82.58\\n86.14\\n63.35\\n62.08\\n61.36\\n60.76\\n68.43\\n68.13\\n67.83\\n67.48\\n64\\n82\\n69.41\\n83.56\\n87.33\\n63.26\\n61.64\\n60.63\\n59.67\\n68.4\\\\nines significantly, which indicates that\\npretrained models lack the multifidelity of Matryoshka Representations and are incapable of fitting\\nan accurate linear classifier at low representation sizes.\\nWe compared the performance of MRL models at various representation sizes via 1-nearest neighbors\\n(1-NN) image classification accuracy on ImageNet-1K in Table 2 and Figure 3. We provide detailed\\ninformation regarding the k-NN search pipeline in Appendix E. We compared against a baseline\\nof attempting to enforce nesting to a FF-2048 model by 1) Random Feature Selection (Rand. FS):\\nconsidering the first m dimensions of FF-2048 for NN lookup, and 2) FF+SVD: performing SVD\\non the FF-2048 representations at the specified representation size, 3) FF+JL: performing random\\nprojection according to the J\\\\nre 5: Despite optimizing MRL only for\\nO(log(d)) dimensions for ResNet50 and ViT-\\nB/16 models; the accuracy in the intermediate\\ndimensions shows interpolating behaviour.\\n4.2.1\\nAdaptive Classification\\nThe flexibility and coarse-to-fine granularity within Matryoshka Representations allows model\\ncascades [90] for Adaptive Classification (AC) [28]. Unlike standard model cascades [95], MRL does\\nnot require multiple expensive neural network forward passes. To perform AC with an MRL trained\\nmodel, we learn thresholds on the maximum softmax probability [33] for each nested classifier on\\na holdout validation set. We then use these thresholds to decide when to transition to the higher\\ndimensional representation (e.g 8 →16 →32) of the MRL model. Appendix D.1 discusses the\\nimplementation and learning o\\\\nGummadi, and A. Weller. Diffused\\nredundancy in pre-trained representations. arXiv preprint arXiv:2306.00183, 2023.\\n[66] P. Nayak. Understanding searches better than ever before. Google AI Blog, 2019. URL https:\\n//blog.google/products/search/search-language-understanding-bert/.\\n[67] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\\nN. Gimelshein, L. Antiga, et al.\\nPytorch: An imperative style, high-performance deep\\nlearning library. Advances in neural information processing systems, 32, 2019.\\n[68] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer.\\nDeep contextualized word representations. In Proceedings of the 2018 Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language\\nTe\\\\ns, 27, 2014.\\n[99] H.-F. Yu, K. Zhong, J. Zhang, W.-C. Chang, and I. S. Dhillon. Pecos: Prediction for enormous\\nand correlated output spaces. Journal of Machine Learning Research, 23(98):1–32, 2022.\\n[100] J. Yu, L. Yang, N. Xu, J. Yang, and T. Huang. Slimmable neural networks. arXiv preprint\\narXiv:1812.08928, 2018.\\n[101] R. Zellers, J. Lu, X. Lu, Y. Yu, Y. Zhao, M. Salehi, A. Kusupati, J. Hessel, A. Farhadi, and\\nY. Choi. Merlot reserve: Neural script knowledge through vision and language and sound.\\narXiv preprint arXiv:2201.02639, 2022.\\n[102] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning\\nbooks and movies: Towards story-like visual explanations by watching movies and reading\\nbooks. In Proceedings of the IEEE international conference on compute\\\\n-accuracy\\ntrade-off\\nfor\\nadaptive\\nretrieval\\nusing\\nMatryoshka Representations compared to single-shot using fixed features with ResNet50\\non ImageNet-1K. We observed that all AR settings lied above the Pareto frontier of single-shot\\nretrieval with varying representation sizes. In particular for ImageNet-1K, we show that the AR\\nmodel with Ds = 16 & Dr = 2048 is as accurate as single-shot retrieval with d = 2048 while being\\n∼128× more efficient in theory and ∼14× faster in practice (compared using HNSW on the same\\nhardware). We show similar trends with ImageNet-4K, but note that we require Ds = 64 given\\nthe increased difficulty of the dataset. This results in ∼32× and ∼6× theoretical and in-practice\\nspeedups respectively. Lastly, while K = 200 works well for our adaptive retrieval experiments, \\\\nithin the same larger network. However, the weights for each progressively smaller\\nnetwork can be different and often require distinct forward passes to isolate the final representations.\\nThis is detrimental for adaptive inference due to the need for re-encoding the entire retrieval database\\nwith expensive sub-net forward passes of varying capacities. Several works [23, 26, 65, 59] investigate\\nthe notions of intrinsic dimensionality and redundancy of representations and objective spaces pointing\\nto minimum description length [74]. Finally, ordered representations proposed by Rippel et al. [73]\\nuse nested dropout in the context of autoencoders to learn nested representations. MRL differentiates\\nitself in formulation by optimizing only for O(log(d)) nesting dimensions instead of O(d). Despit\\\\n) utilization\\nof the representation for downstream applications [50, 89]. Compute costs for the latter part of the\\npipeline scale with the embedding dimensionality as well as the data size (N) and label space (L).\\nAt web-scale [15, 85] this utilization cost overshadows the feature computation cost. The rigidity in\\nthese representations forces the use of high-dimensional embedding vectors across multiple tasks\\ndespite the varying resource and accuracy constraints that require flexibility.\\nHuman perception of the natural world has a naturally coarse-to-fine granularity [28, 32]. However,\\nperhaps due to the inductive bias of gradient-based training [84], deep learning models tend to diffuse\\n“information” across the entire representation vector. The desired elasticity is usually enabled in the\\\\nt MRL can be used for large-scale adaptive\\nclassification as well as adaptive retrieval. On standard benchmarks, MRL matches the accuracy of\\nthe fixed-feature baseline despite using 14× smaller representation size on average. Furthermore, the\\nMatryoshka Representation based adaptive shortlisting and re-ranking system ensures comparable\\nmAP@10 to the baseline while being 128× cheaper in FLOPs and 14× faster in wall-clock time.\\nFinally, most of the efficiency techniques for model inference and vector search are complementary\\nto MRL\\nfurther assisting in deployment at the compute-extreme environments.\\nAcknowledgments\\nWe are grateful to Srinadh Bhojanapalli, Lovish Madaan, Raghav Somani, Ludwig Schmidt, and\\nVenkata Sailesh Sanampudi for helpful discussions and feedback. Aditya Kusupati also tha\\\\n5736\\n0.6060\\n1.2781\\n2.7047\\nHNSW32\\n0.1193\\n0.1455\\n0.1833\\n0.2145\\n0.2333\\n0.2670\\nobservation on the expected dimensionality for 76.30% top-1 classification accuracy being just\\nd ∼37. We leave the design and learning of a more optimal policy for future work.\\nGrad-CAM Examples\\nWe analyzed the nature of model disagreement across representation\\nsizes with MRL models with the help of Grad-CAM visualization [80]. We observed there were\\ncertain classes in ImageNet-1K such as \"tools\", \"vegetables\" and \"meat cutting knife\" which were\\noccasionally located around multiple objects and a cluttered environment. In such scenarios, we\\nobserved that smaller representation size models would often get confused due to other objects and fail\\nto extract the object of interest which generated the correct label. We als\\\\n on top of the existing impact of\\nrepresentation learning. However, a study on the trade-off between representation size\\nand the tendency to encode biases is an interesting future direction along the lines of\\nexisting literature [36, 37]. A part of this is already presented in Section 5.\\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to\\nthem? [Yes]\\n2. If you are including theoretical results...\\n(a) Did you state the full set of assumptions of all theoretical results? [N/A]\\n(b) Did you include complete proofs of all theoretical results? [N/A]\\n3. If you ran experiments...\\n(a) Did you include the code, data, and instructions needed to reproduce the main ex-\\nperimental results (either in the supplemental material or as a URL)? [Yes] See sup-\\nplemental mater\\\\n2020.\\n[70] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understand-\\ning by generative pre-training. OpenAI Blog, 2018. URL https://openai.com/blog/\\nlanguage-unsupervised/.\\n14\\n[71] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\\nP. Mishkin, J. Clark, et al. Learning transferable visual models from natural language su-\\npervision. In International Conference on Machine Learning, pages 8748–8763. PMLR,\\n2021.\\n[72] B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to\\nimagenet? In International Conference on Machine Learning, pages 5389–5400. PMLR,\\n2019.\\n[73] O. Rippel, M. Gelbart, and R. Adams. Learning ordered representations with nested dropout.\\nIn International Conference on Machine Lear\\\\nrXiv preprint arXiv:1911.05248, 2019.\\n[37] S. Hooker, N. Moorosi, G. Clark, S. Bengio, and E. Denton. Characterising bias in compressed\\nmodels. arXiv preprint arXiv:2010.03058, 2020.\\n[38] H. Hotelling. Analysis of a complex of statistical variables into principal components. Journal\\nof educational psychology, 24(6):417, 1933.\\n[39] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and\\nH. Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications.\\narXiv preprint arXiv:1704.04861, 2017.\\n[40] J. Howard and S. Ruder. Universal language model fine-tuning for text classification. arXiv\\npreprint arXiv:1801.06146, 2018.\\n[41] H. Hu, D. Dey, M. Hebert, and J. A. Bagnell. Learning anytime predictions in neural networks\\nvia adaptive loss bal\\\\nrs.\\nWe also found that for both MRL and FF, as the shot number decreased, the required representa-\\ntion size to reach optimal accuracy decreased (Table 15). For example, we observed that 1-shot\\nperformance at 32 representation size had equal accuracy to 2048 representation size.\\nFLUID.\\nFor the long-tailed setting we evaluated MRL on the FLUID benchmark [92] which\\ncontains a mixture of pretrain and new classes. Table 16 shows the evaluation of the learned\\nrepresentation on FLUID. We observed that MRL provided up to 2% higher accuracy on novel\\nclasses in the tail of the distribution, without sacrificing accuracy on other classes. Additionally we\\nfound the accuracy between low-dimensional and high-dimensional representations was marginal for\\npretrain classes. For example, the 64-dimensional M\\\\n= {12, 24, 48, 96, 192, 384, 768} as\\nthe explicitly optimized nested dimensions respectively. Lastly, we extensively compare the MRL\\nand MRL–E models to independently trained low-dimensional (fixed feature) representations (FF),\\ndimensionality reduction (SVD), sub-net method (slimmable networks [100]) and randomly selected\\nfeatures of the highest capacity FF model.\\nIn section 4.2, we evaluate the quality and capacity of the learned representations through linear\\nclassification/probe (LP) and 1-nearest neighbour (1-NN) accuracy. Experiments show that MRL\\nmodels remove the dependence on |M| resource-intensive independently trained models for the\\ncoarse-to-fine representations while being as accurate. Lastly, we show that despite optimizing only\\nfor |M| dimensions, MRL models diffuse the info\\\\ning 1.8B image-text pairs.\\nImageNet Robustness Datasets\\nWe experimented on the following datasets to examine the robust-\\nness of MRL models:\\nImageNetV2 [72] is a collection of 10K images sampled a decade after the original construction of\\nImageNet [16]. ImageNetV2 contains 10 examples each from the 1,000 classes of ImageNet-1K.\\nImageNet-A [35] contains 7.5K real-world adversarially filtered images from 200 ImageNet-\\n1K classes.\\nImageNet-R [34] contains 30K artistic image renditions for 200 of the original ImageNet-1K classes.\\nImageNet-Sketch [94] contains 50K sketches, evenly distributed over all 1,000 ImageNet-1K classes.\\nObjectNet [2] contains 50K images across 313 object classes, each containing ∼160 images each.\\nC\\nMatryoshka Representation Learning Model Training\\nWe trained all ResNet5\\\\nViT-B/16 vision\\nencoder and BERT language encoder on ALIGN data [46] and (c) Masked language modelling:\\nBERT [19] on English Wikipedia and BooksCorpus [102]. Please refer to Appendices B and C for\\ndetails regarding the model architectures, datasets and training specifics.\\n4\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\nRepresentation Size\\n40\\n50\\n60\\n70\\n80\\nTop-1 Accuracy (%)\\nMRL\\nMRL-E\\nFF\\nSVD\\nSlim. Net\\nRand. LP\\nFigure 2: ImageNet-1K linear classification ac-\\ncuracy of ResNet50 models. MRL is as accurate\\nas the independently trained FF models for every\\nrepresentation size.\\n8\\n16\\n32\\n64\\n128\\n256\\n512\\n1024\\n2048\\nRepresentation Size\\n40\\n50\\n60\\n70\\n1-NN Accuracy (%)\\nMRL\\nMRL-E\\nFF\\nSVD\\nSlim. Net\\nRand. FS\\nFigure 3:\\nImageNet-1K 1-NN accuracy of\\nResNet50 models measuring the representation\\nquality for downstream task. MRL ou\\\\nV. Vanhoucke,\\nV. Vasudevan, F. Viégas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and\\nX. Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL\\nhttps://www.tensorflow.org/. Software available from tensorflow.org.\\n[2] A. Barbu, D. Mayo, J. Alverio, W. Luo, C. Wang, D. Gutfreund, J. Tenenbaum, and B. Katz.\\nObjectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition\\nmodels. Advances in neural information processing systems, 32, 2019.\\n[3] S. Bengio, J. Weston, and D. Grangier. Label embedding trees for large multi-class tasks.\\nAdvances in Neural Information Processing Systems, 23, 2010.\\n[4] Y. Bengio. Deep learning of representations for unsupervised and transfer learning. In\\nProceedings of ICML workshop on unsupervised\\\\nn artificial intelligence and statistics, pages 297–304. JMLR Workshop and Conference\\nProceedings, 2010.\\n[28] M. G. Harris and C. D. Giachritsis. Coarse-grained information dominates fine-grained\\ninformation in judgments of time-to-contact from retinal flow. Vision research, 40(6):601–611,\\n2000.\\n[29] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In\\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 770–\\n778, 2016.\\n[30] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual\\nrepresentation learning. In Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, pages 9729–9738, 2020.\\n[31] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick. Masked autoencoders\\\\nle datasets (ImageNet, JFT) across various modalities – vision\\n(ViT, ResNet), vision + language (ALIGN) and language (BERT). MRL code and\\npretrained models are open-sourced at https://github.com/RAIVNLab/MRL.\\n1\\nIntroduction\\nLearned representations [57] are fundamental building blocks of real-world ML systems [66, 91].\\nTrained once and frozen, d-dimensional representations encode rich information and can be used\\nto perform multiple downstream tasks [4]. The deployment of deep representations has two steps:\\n(1) an expensive yet constant-cost forward pass to compute the representation [29] and (2) utilization\\nof the representation for downstream applications [50, 89]. Compute costs for the latter part of the\\npipeline scale with the embedding dimensionality as well as the data size (N) and lab\\\\n. Yang, and S. Kumar. Pre-training tasks for embedding-\\nbased large-scale retrieval. arXiv preprint arXiv:2002.03932, 2020.\\n[11] W.-C. Chang, D. Jiang, H.-F. Yu, C. H. Teo, J. Zhang, K. Zhong, K. Kolluri, Q. Hu,\\nN. Shandilya, V. Ievgrafov, et al. Extreme multi-label learning for semantic matching in\\nproduct search. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discov-\\nery & Data Mining, pages 2643–2651, 2021.\\n[12] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning\\nof visual representations. In International conference on machine learning, pages 1597–1607.\\nPMLR, 2020.\\n[13] Y. Chen, Z. Liu, H. Xu, T. Darrell, and X. Wang. Meta-baseline: exploring simple meta-\\nlearning for few-shot learning. In Proceedings of the IEEE/CVF Internationa\\\\ne, and L. Zettlemoyer.\\nDeep contextualized word representations. In Proceedings of the 2018 Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long Papers), pages 2227–2237, New Orleans, Louisiana, June\\n2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1202. URL https:\\n//aclanthology.org/N18-1202.\\n[69] Y. Prabhu, A. Kusupati, N. Gupta, and M. Varma. Extreme regression for dynamic search\\nadvertising. In Proceedings of the 13th International Conference on Web Search and Data\\nMining, pages 456–464, 2020.\\n[70] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understand-\\ning by generative pre-training. OpenAI Blog, 2018. URL https://openai.com/blog/\\nlanguage-unsupervise\\\\n results in Section 5.1 reveal interesting weaknesses of MRL that would be logical directions\\nfor future work. (1) Optimizing the weightings of the nested losses to obtain a Pareto optimal\\naccuracy-vs-efficiency trade-off – a potential solution could emerge from adaptive loss balancing\\naspects of anytime neural networks [41]. (2) Using different losses at various fidelities aimed at\\nsolving a specific aspect of adaptive deployment – e.g. high recall for 8-dimension and robustness\\nfor 2048-dimension. (3) Learning a search data-structure, like differentiable k-d tree, on top of\\nMatryoshka Representation to enable dataset and representation aware retrieval. (4) Finally, the\\njoint optimization of multi-objective MRL combined with end-to-end learnable search data-structure\\nto have data-driven a\\\\nrmance of ResNet50 representations on ImageNet-1K across\\ndimensionalities for MRL, MRL–E, FF, slimmable networks along with post-hoc compression\\nof vectors using SVD and random feature selection. Matryoshka Representations are often the\\nmost accurate while being up to 3% better than the FF baselines. Similar to classification, post-hoc\\ncompression and slimmable network baselines suffer from significant drop-off in retrieval mAP@10\\nwith ≤256 dimensions. Appendix E discusses the mAP@10 of the same models on ImageNet-4K.\\nMRL models are capable of performing accurate retrieval at various granularities without the\\nadditional expense of multiple model forward passes for the web-scale databases. FF models\\nalso generate independent databases which become prohibitively expense to store and switch i\\\\nlg 1 and 2 provided below to train supervised ResNet50–MRL models on ImageNet-1K.\\nWe provide this code as a template to extend MRL to any domain.\\nAlgorithm 1 Pytorch code for Matryoshka Cross-Entropy Loss\\nclass Matryoshka_CE_Loss(nn.Module):\\ndef __init__(self, relative_importance, **kwargs):\\nsuper(Matryoshka_CE_Loss, self).__init__()\\nself.criterion = nn.CrossEntropyLoss(**kwargs)\\nself.relative_importance = relative_importance # usually set\\nto all ones\\ndef forward(self, output, target):\\nloss=0\\nfor i in range(len(output)):\\nloss+= self.relative_importance[i] * self.criterion(output[\\ni], target)\\nreturn loss\\nAlgorithm 2 Pytorch code for MRL Linear Layer\\nclass MRL_Linear_Layer(nn.Module):\\ndef __init__(self, nesting_list: List, num_classes=1000, efficient=\\nFalse, **kwargs):\\nsuper(MRL_Linear_Layer\\\\n\\nBased on the context provided above, answer the following question. Do not use any external knowledge.\\nIf the answer is not present in the context, respond with \"I don\\'t know.\"\\nIf the context is empty or irrelevant to the question, respond with \"No relevant information found in the context.\"\\n'}, {'role': 'user', 'content': ' Query:  What is Matryoshka Representation Learning? Act as a oriental story teller.\\n\\n'}]\n"
     ]
    }
   ],
   "source": [
    "# create list of system and user prompt dictionary\n",
    "list_of_prompts = [\n",
    "    pt.system_prompt(context_prompt),\n",
    "    # pt.user_prompt(user_query + \"\\nProvide the context used for your explanations.\")\n",
    "    pt.user_prompt(user_query)\n",
    "]\n",
    "\n",
    "print(list_of_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation\n",
    "\n",
    "Let us generate a response using the augmented prompt.\n",
    "\n",
    "First, initialize the generation model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructor here\n"
     ]
    }
   ],
   "source": [
    "from py4ragTools.generation_tools import GenerationTools\n",
    "#gt = GenerationTools(generation_model='llama3.1:8b')\n",
    "gt = GenerationTools(generation_model='llama3.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gather \\'round, young scholars, and listen well to the tale of... (dramatic pause) ...Matryoshka Representation Learning!\\n\\n(In a thick, mystical voice)\\n\\nIn days of yore, when the world was young and mysterious, there lived a great philosopher-king named Mikhail. He sought to unlock the secrets of representation learning, inscribing on his scroll: \"To learn what is beneath the surface, one must start with the obvious.\"\\n\\nAnd lo, he created a method, known as Matryoshka Representation Learning (MRL). It\\'s a technique that weaves together multiple learning frameworks into one mighty thread.\\n\\nImagine a sturdy wooden doll – the Nest – containing a smaller, intricately carved version of itself inside (Daughter Doll). Within this Daughter Doll lies an even smaller replica (Granddaughter Doll), and so on. The Matryoshka family grows with each level, yet remains connected through shared knowledge and wisdom.\\n\\nIn this symbolic context, MRL represents learning various representations using different frameworks, starting from simple patterns in the environment to more complex representations within each subsequent nest.\\n\\n**The Nest**: Represents a basic neural network or feature extraction model that serves as a foundation for learning.\\n**Daughter Dolls**: Symbolize secondary learning stages or tasks that build upon previously learned knowledge (previously learned features, weights, etc), allowing each consecutive level to become increasingly sophisticated and intricate in representation.\\n\\nThe grand philosophy behind Matryoshka Representation Learning is that by layering diverse understanding on one another – much like a Matryushki doll inside another, the model attains deeper insight into its own structure. Hence, Matryoshka Representation Learning, my friends.\\n\\nTo illustrate the tale further: imagine building houses upon houses (successive learning) and painting masterpieces each time (improving existing knowledge), only to find within each subsequent home\\'s attic an even newer, and most wondrous technique awaits you. Such a layered wisdom unlocks unseen mysteries of representation and learning itself!\\n\\nNow, if you\\'ll permit me, dear scholars, the Matryoshka Representation Learning story shall reveal its secrets more clearly:\\n\\n**Properties of MRL**\\n\\nThis oriental parable is but a simple way to grasp concepts such as:\\n\\n1. **Hierarchy**: As one doll resides inside another (representing learned representations with new ones), each becomes progressively intricate without disrupting previous nesting.\\n2. **Learning Layers**: Matryoshka Learning represents successive layers where each subsequent task or network builds upon existing knowledge within the nest.\\n3. **Nested Complexity**: Understanding becomes more layered as tasks are mastered and built atop former stages.\\n\\nThere you have it, young minds - The Tale of the Matryoschkas has been passed down in secret for centuries! Now go forth, scholars and weave your own paths through the tapestry of computational ideas.'"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = gt.get_response(list_of_prompts) \n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gather 'round, young scholars, and listen well to the tale of... (dramatic pause) ...Matryoshka Representation Learning!\n",
      "\n",
      "(In a thick, mystical voice)\n",
      "\n",
      "In days of yore, when the world was young and mysterious, there lived a great philosopher-king named Mikhail. He sought to unlock the secrets of representation learning, inscribing on his scroll: \"To learn what is beneath the surface, one must start with the obvious.\"\n",
      "\n",
      "And lo, he created a method, known as Matryoshka Representation Learning (MRL). It's a technique that weaves together multiple learning frameworks into one mighty thread.\n",
      "\n",
      "Imagine a sturdy wooden doll – the Nest – containing a smaller, intricately carved version of itself inside (Daughter Doll). Within this Daughter Doll lies an even smaller replica (Granddaughter Doll), and so on. The Matryoshka family grows with each level, yet remains connected through shared knowledge and wisdom.\n",
      "\n",
      "In this symbolic context, MRL represents learning various representations using different frameworks, starting from simple patterns in the environment to more complex representations within each subsequent nest.\n",
      "\n",
      "**The Nest**: Represents a basic neural network or feature extraction model that serves as a foundation for learning.\n",
      "**Daughter Dolls**: Symbolize secondary learning stages or tasks that build upon previously learned knowledge (previously learned features, weights, etc), allowing each consecutive level to become increasingly sophisticated and intricate in representation.\n",
      "\n",
      "The grand philosophy behind Matryoshka Representation Learning is that by layering diverse understanding on one another – much like a Matryushki doll inside another, the model attains deeper insight into its own structure. Hence, Matryoshka Representation Learning, my friends.\n",
      "\n",
      "To illustrate the tale further: imagine building houses upon houses (successive learning) and painting masterpieces each time (improving existing knowledge), only to find within each subsequent home's attic an even newer, and most wondrous technique awaits you. Such a layered wisdom unlocks unseen mysteries of representation and learning itself!\n",
      "\n",
      "Now, if you'll permit me, dear scholars, the Matryoshka Representation Learning story shall reveal its secrets more clearly:\n",
      "\n",
      "**Properties of MRL**\n",
      "\n",
      "This oriental parable is but a simple way to grasp concepts such as:\n",
      "\n",
      "1. **Hierarchy**: As one doll resides inside another (representing learned representations with new ones), each becomes progressively intricate without disrupting previous nesting.\n",
      "2. **Learning Layers**: Matryoshka Learning represents successive layers where each subsequent task or network builds upon existing knowledge within the nest.\n",
      "3. **Nested Complexity**: Understanding becomes more layered as tasks are mastered and built atop former stages.\n",
      "\n",
      "There you have it, young minds - The Tale of the Matryoschkas has been passed down in secret for centuries! Now go forth, scholars and weave your own paths through the tapestry of computational ideas.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aie-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
